{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune import CLIReporter, ExperimentAnalysis\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air import session\n",
    "from ray.train import Checkpoint\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r'D:\\School\\ADMU\\4Y\\SEM 1\\MATH 199.11\\Final\\DAILY\\LUZ_Daily_Complete.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_columns(df, value, substrings_ffill, substrings_interpolate):\n",
    "    # Replace -999 with NaN\n",
    "    df = df.replace(value, np.nan)\n",
    "    \n",
    "    # Forward fill for specified substrings\n",
    "    ffill_cols = df.loc[:, df.columns.str.contains('|'.join(substrings_ffill), case=False)]\n",
    "    ffill_cols = ffill_cols.ffill()\n",
    "    \n",
    "    # Interpolate for specified substrings\n",
    "    interpolate_cols = df.loc[:, df.columns.str.contains('|'.join(substrings_interpolate), case=False)]\n",
    "    interpolate_cols = interpolate_cols.interpolate(method='linear')\n",
    "    \n",
    "    return ffill_cols, interpolate_cols\n",
    "\n",
    "columns_with_minus_999 = data.loc[:, (data == -999).any(axis=0)]\n",
    "\n",
    "# Process columns\n",
    "rainfall_cols, temp_cols_interpolated = process_columns(\n",
    "    columns_with_minus_999, \n",
    "    -999, \n",
    "    substrings_ffill=['rainfall'], \n",
    "    substrings_interpolate=['tmax', 'tmin']\n",
    ")\n",
    "X = data.copy()\n",
    "X[rainfall_cols.columns] = rainfall_cols\n",
    "X[temp_cols_interpolated.columns] = temp_cols_interpolated\n",
    "y = data[['GWAP','LWAP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.6 * len(X))  # 60% for training\n",
    "val_size = int(0.20 * len(X))   # 20% for validation\n",
    "test_size = len(X) - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "train_data = X[:train_size]\n",
    "train_labels = y[:train_size]\n",
    "\n",
    "val_data = X[train_size:train_size + val_size]\n",
    "val_labels = y[train_size:train_size + val_size]\n",
    "\n",
    "test_data = X[train_size + val_size:]\n",
    "test_labels = y[train_size + val_size:]\n",
    "seq_len=7\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_cols = []\n",
    "boxcox_cols = []\n",
    "yeojohnson_cols = []\n",
    "\n",
    "# Classify columns into MinMax, BoxCox, or YeoJohnson families\n",
    "def classify_features(data):\n",
    "    for column in data.columns:\n",
    "        col_data = data[column]\n",
    "        skewness = col_data.skew()\n",
    "        kurt = col_data.kurtosis()\n",
    "        is_positive = np.all(col_data > 0)\n",
    "\n",
    "        if -1 <= skewness <= 1 and -1 <= kurt <= 1:\n",
    "            minmax_cols.append(column)  # MinMax family\n",
    "        elif is_positive:\n",
    "            boxcox_cols.append(column)  # BoxCox family\n",
    "        else:\n",
    "            yeojohnson_cols.append(column)  # YeoJohnson family\n",
    "\n",
    "classify_features(X)\n",
    "\n",
    "minmax_colsy = []\n",
    "boxcox_colsy = []\n",
    "yeojohnson_colsy = []\n",
    "\n",
    "def classify_features(data):\n",
    "    for column in data.columns:\n",
    "        col_data = data[column]\n",
    "        skewness = col_data.skew()\n",
    "        kurt = col_data.kurtosis()\n",
    "        is_positive = np.all(col_data > 0)\n",
    "\n",
    "        if -1 <= skewness <= 1 and -1 <= kurt <= 1:\n",
    "            minmax_colsy.append(column)  # MinMax family\n",
    "        elif is_positive:\n",
    "            boxcox_colsy.append(column)  # BoxCox family\n",
    "        else:\n",
    "            yeojohnson_colsy.append(column)  # YeoJohnson family\n",
    "\n",
    "classify_features(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.DataFrame(train_data)  # Replace `data` with your actual data\n",
    "val_data_df = pd.DataFrame(val_data)  # Replace `data` with your actual data\n",
    "test_data_df = pd.DataFrame(test_data)  # Replace `data` with your actual data\n",
    "# Test MinMaxScaler independently\n",
    "minmax_test = MinMaxScaler(feature_range=(0, 1))\n",
    "minmaxfit = minmax_test.fit(train_data_df[minmax_cols])\n",
    "train_data_minmax = minmaxfit.transform(train_data_df[minmax_cols])\n",
    "val_data_minmax = minmaxfit.transform(val_data_df[minmax_cols])\n",
    "test_data_minmax = minmaxfit.transform(test_data_df[minmax_cols])\n",
    "\n",
    "# Test Box-Cox + MinMaxScaler independently\n",
    "boxcox_pipeline = Pipeline([\n",
    "    ('boxcox', PowerTransformer(method='box-cox', standardize=False)),\n",
    "    ('minmax', MinMaxScaler(feature_range=(0, 1)))\n",
    "])\n",
    "bc = boxcox_pipeline.fit(train_data_df[boxcox_cols])\n",
    "train_data_bc = bc.transform(train_data_df[boxcox_cols])\n",
    "val_data_bc = bc.transform(val_data_df[boxcox_cols])\n",
    "test_data_bc = bc.transform(test_data_df[boxcox_cols])\n",
    "\n",
    "# Test Yeo-Johnson + MinMaxScaler independently\n",
    "yeojohnson_pipeline = Pipeline([\n",
    "    ('yeojohnson', PowerTransformer(method='yeo-johnson', standardize=False)),\n",
    "    ('minmax', MinMaxScaler(feature_range=(0, 1)))\n",
    "])\n",
    "yj = yeojohnson_pipeline.fit(train_data_df[yeojohnson_cols])\n",
    "train_data_yj = yj.transform(train_data_df[yeojohnson_cols])\n",
    "val_data_yj = yj.transform(val_data_df[yeojohnson_cols])\n",
    "test_data_yj = yj.transform(test_data_df[yeojohnson_cols])\n",
    "\n",
    "train_data_transformed = np.hstack([train_data_minmax, train_data_bc, train_data_yj])\n",
    "val_data_transformed = np.hstack([val_data_minmax, val_data_bc, val_data_yj])\n",
    "test_data_transformed = np.hstack([test_data_minmax, test_data_bc, test_data_yj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = pd.DataFrame(train_labels)  # Replace `data` with your actual data\n",
    "val_labels_df = pd.DataFrame(val_labels)  # Replace `data` with your actual data\n",
    "test_labels_df = pd.DataFrame(test_labels)  # Replace `data` with your actual data\n",
    "# Test MinMaxScaler independently\n",
    "\n",
    "\n",
    "# Test Box-Cox + MinMaxScaler independently\n",
    "boxcox_pipeline = Pipeline([\n",
    "    ('boxcox', PowerTransformer(method='box-cox', standardize=False)),\n",
    "    ('minmax', MinMaxScaler(feature_range=(0, 1)))\n",
    "])\n",
    "bcy = boxcox_pipeline.fit(train_data_df[boxcox_colsy])\n",
    "train_labels_bc = bcy.transform(train_data_df[boxcox_colsy])\n",
    "val_labels_bc = bcy.transform(val_data_df[boxcox_colsy])\n",
    "test_labels_bc = bcy.transform(test_data_df[boxcox_colsy])\n",
    "\n",
    "# Test Yeo-Johnson + MinMaxScaler independently\n",
    "train_labels_transformed = np.hstack([train_labels_bc])\n",
    "val_labels_transformed = np.hstack([ val_labels_bc])\n",
    "test_labels_transformed = np.hstack([ test_labels_bc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for each column in the combined validation data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLOW_LUZ</th>\n",
       "      <th>Hourly Demand</th>\n",
       "      <th>TMAX_Cubi Point</th>\n",
       "      <th>TMAX_NAIA</th>\n",
       "      <th>TMIN_NAIA</th>\n",
       "      <th>TMAX_Science Garden</th>\n",
       "      <th>TMAX_San Jose</th>\n",
       "      <th>TMIN_San Jose</th>\n",
       "      <th>TMAX_Tayabas</th>\n",
       "      <th>TMIN_Tayabas</th>\n",
       "      <th>TMAX_CLSU</th>\n",
       "      <th>TMIN_CLSU</th>\n",
       "      <th>TMAX_Tanay</th>\n",
       "      <th>TMIN_Tanay</th>\n",
       "      <th>TMAX_Ambulong</th>\n",
       "      <th>TMAX_Casiguran</th>\n",
       "      <th>TMIN_Casiguran</th>\n",
       "      <th>TMAX_Clark</th>\n",
       "      <th>TMIN_Clark</th>\n",
       "      <th>TMAX_Calapan</th>\n",
       "      <th>TMIN_Calapan</th>\n",
       "      <th>GWAP</th>\n",
       "      <th>LWAP</th>\n",
       "      <th>TMIN_Cubi Point</th>\n",
       "      <th>TMIN_Science Garden</th>\n",
       "      <th>TMIN_Ambulong</th>\n",
       "      <th>RESERVE_GWAP_Fr</th>\n",
       "      <th>RESERVE_GWAP_Ru</th>\n",
       "      <th>RESERVE_GWAP_Rd</th>\n",
       "      <th>RESERVE_GWAP_Dr</th>\n",
       "      <th>RAINFALL_Cubi Point</th>\n",
       "      <th>RAINFALL_NAIA</th>\n",
       "      <th>RAINFALL_Science Garden</th>\n",
       "      <th>RAINFALL_San Jose</th>\n",
       "      <th>RAINFALL_Tayabas</th>\n",
       "      <th>RAINFALL_CLSU</th>\n",
       "      <th>RAINFALL_Tanay</th>\n",
       "      <th>RAINFALL_Ambulong</th>\n",
       "      <th>RAINFALL_Casiguran</th>\n",
       "      <th>RAINFALL_Clark</th>\n",
       "      <th>RAINFALL_Calapan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.468299</td>\n",
       "      <td>0.648252</td>\n",
       "      <td>0.546937</td>\n",
       "      <td>0.590728</td>\n",
       "      <td>0.606996</td>\n",
       "      <td>0.565195</td>\n",
       "      <td>0.580500</td>\n",
       "      <td>0.536403</td>\n",
       "      <td>0.561562</td>\n",
       "      <td>0.552756</td>\n",
       "      <td>0.543888</td>\n",
       "      <td>0.598566</td>\n",
       "      <td>0.554953</td>\n",
       "      <td>0.666924</td>\n",
       "      <td>0.538093</td>\n",
       "      <td>0.555466</td>\n",
       "      <td>0.620166</td>\n",
       "      <td>0.609437</td>\n",
       "      <td>0.581394</td>\n",
       "      <td>0.611216</td>\n",
       "      <td>0.518960</td>\n",
       "      <td>0.606551</td>\n",
       "      <td>0.601150</td>\n",
       "      <td>0.492411</td>\n",
       "      <td>0.414022</td>\n",
       "      <td>0.446758</td>\n",
       "      <td>0.465425</td>\n",
       "      <td>0.516107</td>\n",
       "      <td>0.414435</td>\n",
       "      <td>0.377133</td>\n",
       "      <td>0.579450</td>\n",
       "      <td>0.456032</td>\n",
       "      <td>0.436821</td>\n",
       "      <td>0.494787</td>\n",
       "      <td>0.379439</td>\n",
       "      <td>0.568961</td>\n",
       "      <td>0.469711</td>\n",
       "      <td>0.527558</td>\n",
       "      <td>0.586225</td>\n",
       "      <td>0.491808</td>\n",
       "      <td>0.612988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.203223</td>\n",
       "      <td>0.173484</td>\n",
       "      <td>0.169088</td>\n",
       "      <td>0.173105</td>\n",
       "      <td>0.166796</td>\n",
       "      <td>0.183004</td>\n",
       "      <td>0.169354</td>\n",
       "      <td>0.213272</td>\n",
       "      <td>0.191139</td>\n",
       "      <td>0.205733</td>\n",
       "      <td>0.167631</td>\n",
       "      <td>0.207601</td>\n",
       "      <td>0.205654</td>\n",
       "      <td>0.178767</td>\n",
       "      <td>0.186873</td>\n",
       "      <td>0.216838</td>\n",
       "      <td>0.163944</td>\n",
       "      <td>0.155433</td>\n",
       "      <td>0.180081</td>\n",
       "      <td>0.173074</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>0.170671</td>\n",
       "      <td>0.170680</td>\n",
       "      <td>0.182261</td>\n",
       "      <td>0.176314</td>\n",
       "      <td>0.153151</td>\n",
       "      <td>0.228774</td>\n",
       "      <td>0.175465</td>\n",
       "      <td>0.104273</td>\n",
       "      <td>0.243130</td>\n",
       "      <td>0.206565</td>\n",
       "      <td>0.236578</td>\n",
       "      <td>0.269305</td>\n",
       "      <td>0.236016</td>\n",
       "      <td>0.244571</td>\n",
       "      <td>0.206949</td>\n",
       "      <td>0.241283</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>0.215219</td>\n",
       "      <td>0.234339</td>\n",
       "      <td>0.189873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.324398</td>\n",
       "      <td>0.544614</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.477477</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.434343</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.423554</td>\n",
       "      <td>0.472656</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.549296</td>\n",
       "      <td>0.414414</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.503704</td>\n",
       "      <td>0.460843</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.489789</td>\n",
       "      <td>0.486298</td>\n",
       "      <td>0.388664</td>\n",
       "      <td>0.306930</td>\n",
       "      <td>0.353685</td>\n",
       "      <td>0.304974</td>\n",
       "      <td>0.409046</td>\n",
       "      <td>0.357732</td>\n",
       "      <td>0.198878</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.386552</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.212722</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.335037</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.662646</td>\n",
       "      <td>0.549550</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.553719</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.718310</td>\n",
       "      <td>0.558559</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.614815</td>\n",
       "      <td>0.608434</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.579811</td>\n",
       "      <td>0.495312</td>\n",
       "      <td>0.430661</td>\n",
       "      <td>0.454780</td>\n",
       "      <td>0.476836</td>\n",
       "      <td>0.498505</td>\n",
       "      <td>0.424483</td>\n",
       "      <td>0.405656</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.386552</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.368485</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.344686</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.547165</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.582796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.619960</td>\n",
       "      <td>0.777584</td>\n",
       "      <td>0.639640</td>\n",
       "      <td>0.700450</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.709459</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.718519</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.747126</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.746433</td>\n",
       "      <td>0.738104</td>\n",
       "      <td>0.618592</td>\n",
       "      <td>0.534411</td>\n",
       "      <td>0.539245</td>\n",
       "      <td>0.620082</td>\n",
       "      <td>0.640535</td>\n",
       "      <td>0.467362</td>\n",
       "      <td>0.568597</td>\n",
       "      <td>0.722833</td>\n",
       "      <td>0.619823</td>\n",
       "      <td>0.649551</td>\n",
       "      <td>0.637988</td>\n",
       "      <td>0.564978</td>\n",
       "      <td>0.691765</td>\n",
       "      <td>0.676540</td>\n",
       "      <td>0.697769</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.621451</td>\n",
       "      <td>0.786879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FLOW_LUZ  Hourly Demand  TMAX_Cubi Point   TMAX_NAIA   TMIN_NAIA  \\\n",
       "count  438.000000     438.000000       438.000000  438.000000  438.000000   \n",
       "mean     0.468299       0.648252         0.546937    0.590728    0.606996   \n",
       "std      0.203223       0.173484         0.169088    0.173105    0.166796   \n",
       "min      0.000000       0.000000         0.000000    0.000000    0.000000   \n",
       "25%      0.324398       0.544614         0.441441    0.477477    0.493827   \n",
       "50%      0.470000       0.662646         0.549550    0.594595    0.604938   \n",
       "75%      0.619960       0.777584         0.639640    0.700450    0.716049   \n",
       "max      1.000000       1.000000         1.000000    1.000000    1.000000   \n",
       "\n",
       "       TMAX_Science Garden  TMAX_San Jose  TMIN_San Jose  TMAX_Tayabas  \\\n",
       "count           438.000000     438.000000     438.000000    438.000000   \n",
       "mean              0.565195       0.580500       0.536403      0.561562   \n",
       "std               0.183004       0.169354       0.213272      0.191139   \n",
       "min               0.000000       0.000000       0.000000      0.000000   \n",
       "25%               0.434343       0.482143       0.361111      0.432432   \n",
       "50%               0.575758       0.580357       0.472222      0.576577   \n",
       "75%               0.696970       0.714286       0.750000      0.709459   \n",
       "max               1.000000       1.000000       1.000000      1.000000   \n",
       "\n",
       "       TMIN_Tayabas   TMAX_CLSU   TMIN_CLSU  TMAX_Tanay  TMIN_Tanay  \\\n",
       "count    438.000000  438.000000  438.000000  438.000000  438.000000   \n",
       "mean       0.552756    0.543888    0.598566    0.554953    0.666924   \n",
       "std        0.205733    0.167631    0.207601    0.205654    0.178767   \n",
       "min        0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%        0.428571    0.423554    0.472656    0.400000    0.549296   \n",
       "50%        0.607143    0.553719    0.625000    0.552381    0.718310   \n",
       "75%        0.696429    0.652893    0.765625    0.714286    0.788732   \n",
       "max        1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       TMAX_Ambulong  TMAX_Casiguran  TMIN_Casiguran  TMAX_Clark  TMIN_Clark  \\\n",
       "count     438.000000      438.000000      438.000000  438.000000  438.000000   \n",
       "mean        0.538093        0.555466        0.620166    0.609437    0.581394   \n",
       "std         0.186873        0.216838        0.163944    0.155433    0.180081   \n",
       "min         0.000000        0.000000        0.000000    0.000000    0.000000   \n",
       "25%         0.414414        0.392157        0.510204    0.503704    0.460843   \n",
       "50%         0.558559        0.588235        0.642857    0.614815    0.608434   \n",
       "75%         0.666667        0.725490        0.755102    0.718519    0.710843   \n",
       "max         1.000000        1.000000        1.000000    1.000000    1.000000   \n",
       "\n",
       "       TMAX_Calapan  TMIN_Calapan        GWAP        LWAP  TMIN_Cubi Point  \\\n",
       "count    438.000000    438.000000  438.000000  438.000000       438.000000   \n",
       "mean       0.611216      0.518960    0.606551    0.601150         0.492411   \n",
       "std        0.173074      0.178700    0.170671    0.170680         0.182261   \n",
       "min        0.000000      0.000000    0.000000    0.000000         0.000000   \n",
       "25%        0.517241      0.391304    0.489789    0.486298         0.388664   \n",
       "50%        0.632184      0.521739    0.584372    0.579811         0.495312   \n",
       "75%        0.747126      0.630435    0.746433    0.738104         0.618592   \n",
       "max        1.000000      1.000000    1.000000    1.000000         1.000000   \n",
       "\n",
       "       TMIN_Science Garden  TMIN_Ambulong  RESERVE_GWAP_Fr  RESERVE_GWAP_Ru  \\\n",
       "count           438.000000     438.000000       438.000000       438.000000   \n",
       "mean              0.414022       0.446758         0.465425         0.516107   \n",
       "std               0.176314       0.153151         0.228774         0.175465   \n",
       "min               0.000000       0.000000         0.000000         0.000000   \n",
       "25%               0.306930       0.353685         0.304974         0.409046   \n",
       "50%               0.430661       0.454780         0.476836         0.498505   \n",
       "75%               0.534411       0.539245         0.620082         0.640535   \n",
       "max               1.000000       1.000000         1.000000         1.000000   \n",
       "\n",
       "       RESERVE_GWAP_Rd  RESERVE_GWAP_Dr  RAINFALL_Cubi Point  RAINFALL_NAIA  \\\n",
       "count       438.000000       438.000000           438.000000     438.000000   \n",
       "mean          0.414435         0.377133             0.579450       0.456032   \n",
       "std           0.104273         0.243130             0.206565       0.236578   \n",
       "min           0.000000         0.000000             0.000000       0.000000   \n",
       "25%           0.357732         0.198878             0.493149       0.386552   \n",
       "50%           0.424483         0.405656             0.493149       0.386552   \n",
       "75%           0.467362         0.568597             0.722833       0.619823   \n",
       "max           1.000000         1.000000             1.000000       1.000000   \n",
       "\n",
       "       RAINFALL_Science Garden  RAINFALL_San Jose  RAINFALL_Tayabas  \\\n",
       "count               438.000000         438.000000        438.000000   \n",
       "mean                  0.436821           0.494787          0.379439   \n",
       "std                   0.269305           0.236016          0.244571   \n",
       "min                   0.000000           0.000000          0.000000   \n",
       "25%                   0.347275           0.417791          0.212722   \n",
       "50%                   0.347275           0.417791          0.368485   \n",
       "75%                   0.649551           0.637988          0.564978   \n",
       "max                   1.000000           1.000000          1.000000   \n",
       "\n",
       "       RAINFALL_CLSU  RAINFALL_Tanay  RAINFALL_Ambulong  RAINFALL_Casiguran  \\\n",
       "count     438.000000      438.000000         438.000000          438.000000   \n",
       "mean        0.568961        0.469711           0.527558            0.586225   \n",
       "std         0.206949        0.241283           0.214956            0.215219   \n",
       "min         0.000000        0.000000           0.000000            0.000000   \n",
       "25%         0.493466        0.335037           0.413668            0.383035   \n",
       "50%         0.493466        0.344686           0.413668            0.547165   \n",
       "75%         0.691765        0.676540           0.697769            0.789462   \n",
       "max         1.000000        1.000000           1.000000            1.000000   \n",
       "\n",
       "       RAINFALL_Clark  RAINFALL_Calapan  \n",
       "count      438.000000        438.000000  \n",
       "mean         0.491808          0.612988  \n",
       "std          0.234339          0.189873  \n",
       "min          0.000000          0.000000  \n",
       "25%          0.422884          0.435920  \n",
       "50%          0.422884          0.582796  \n",
       "75%          0.621451          0.786879  \n",
       "max          1.000000          1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLOW_LUZ</th>\n",
       "      <th>Hourly Demand</th>\n",
       "      <th>TMAX_Cubi Point</th>\n",
       "      <th>TMAX_NAIA</th>\n",
       "      <th>TMIN_NAIA</th>\n",
       "      <th>TMAX_Science Garden</th>\n",
       "      <th>TMAX_San Jose</th>\n",
       "      <th>TMIN_San Jose</th>\n",
       "      <th>TMAX_Tayabas</th>\n",
       "      <th>TMIN_Tayabas</th>\n",
       "      <th>TMAX_CLSU</th>\n",
       "      <th>TMIN_CLSU</th>\n",
       "      <th>TMAX_Tanay</th>\n",
       "      <th>TMIN_Tanay</th>\n",
       "      <th>TMAX_Ambulong</th>\n",
       "      <th>TMAX_Casiguran</th>\n",
       "      <th>TMIN_Casiguran</th>\n",
       "      <th>TMAX_Clark</th>\n",
       "      <th>TMIN_Clark</th>\n",
       "      <th>TMAX_Calapan</th>\n",
       "      <th>TMIN_Calapan</th>\n",
       "      <th>GWAP</th>\n",
       "      <th>LWAP</th>\n",
       "      <th>TMIN_Cubi Point</th>\n",
       "      <th>TMIN_Science Garden</th>\n",
       "      <th>TMIN_Ambulong</th>\n",
       "      <th>RESERVE_GWAP_Fr</th>\n",
       "      <th>RESERVE_GWAP_Ru</th>\n",
       "      <th>RESERVE_GWAP_Rd</th>\n",
       "      <th>RESERVE_GWAP_Dr</th>\n",
       "      <th>RAINFALL_Cubi Point</th>\n",
       "      <th>RAINFALL_NAIA</th>\n",
       "      <th>RAINFALL_Science Garden</th>\n",
       "      <th>RAINFALL_San Jose</th>\n",
       "      <th>RAINFALL_Tayabas</th>\n",
       "      <th>RAINFALL_CLSU</th>\n",
       "      <th>RAINFALL_Tanay</th>\n",
       "      <th>RAINFALL_Ambulong</th>\n",
       "      <th>RAINFALL_Casiguran</th>\n",
       "      <th>RAINFALL_Clark</th>\n",
       "      <th>RAINFALL_Calapan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.462319</td>\n",
       "      <td>0.841316</td>\n",
       "      <td>0.600518</td>\n",
       "      <td>0.698815</td>\n",
       "      <td>0.725351</td>\n",
       "      <td>0.679120</td>\n",
       "      <td>0.575893</td>\n",
       "      <td>0.827182</td>\n",
       "      <td>0.735222</td>\n",
       "      <td>0.783268</td>\n",
       "      <td>0.683573</td>\n",
       "      <td>0.758455</td>\n",
       "      <td>0.709361</td>\n",
       "      <td>0.782124</td>\n",
       "      <td>0.683019</td>\n",
       "      <td>0.755909</td>\n",
       "      <td>0.784806</td>\n",
       "      <td>0.715373</td>\n",
       "      <td>0.706057</td>\n",
       "      <td>0.773658</td>\n",
       "      <td>0.866885</td>\n",
       "      <td>0.547668</td>\n",
       "      <td>0.547750</td>\n",
       "      <td>0.653095</td>\n",
       "      <td>0.564982</td>\n",
       "      <td>0.580869</td>\n",
       "      <td>0.306686</td>\n",
       "      <td>0.396148</td>\n",
       "      <td>-0.019063</td>\n",
       "      <td>0.158629</td>\n",
       "      <td>0.626125</td>\n",
       "      <td>0.501174</td>\n",
       "      <td>0.510468</td>\n",
       "      <td>0.534797</td>\n",
       "      <td>0.324867</td>\n",
       "      <td>0.629076</td>\n",
       "      <td>0.484759</td>\n",
       "      <td>0.549820</td>\n",
       "      <td>0.509254</td>\n",
       "      <td>0.542504</td>\n",
       "      <td>0.551526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.185224</td>\n",
       "      <td>0.164575</td>\n",
       "      <td>0.191032</td>\n",
       "      <td>0.169632</td>\n",
       "      <td>0.165854</td>\n",
       "      <td>0.186608</td>\n",
       "      <td>0.180428</td>\n",
       "      <td>0.112834</td>\n",
       "      <td>0.136298</td>\n",
       "      <td>0.158137</td>\n",
       "      <td>0.165557</td>\n",
       "      <td>0.179922</td>\n",
       "      <td>0.183633</td>\n",
       "      <td>0.132843</td>\n",
       "      <td>0.167520</td>\n",
       "      <td>0.141552</td>\n",
       "      <td>0.115302</td>\n",
       "      <td>0.176049</td>\n",
       "      <td>0.148321</td>\n",
       "      <td>0.140960</td>\n",
       "      <td>0.188844</td>\n",
       "      <td>0.170823</td>\n",
       "      <td>0.174764</td>\n",
       "      <td>0.224561</td>\n",
       "      <td>0.221538</td>\n",
       "      <td>0.240512</td>\n",
       "      <td>0.238990</td>\n",
       "      <td>0.201831</td>\n",
       "      <td>0.138439</td>\n",
       "      <td>0.207967</td>\n",
       "      <td>0.236831</td>\n",
       "      <td>0.264479</td>\n",
       "      <td>0.271226</td>\n",
       "      <td>0.222858</td>\n",
       "      <td>0.190269</td>\n",
       "      <td>0.218323</td>\n",
       "      <td>0.245374</td>\n",
       "      <td>0.220948</td>\n",
       "      <td>0.205980</td>\n",
       "      <td>0.268478</td>\n",
       "      <td>0.199414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.075273</td>\n",
       "      <td>0.286574</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>0.144144</td>\n",
       "      <td>0.320988</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.198198</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.144144</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.204948</td>\n",
       "      <td>0.193783</td>\n",
       "      <td>0.168933</td>\n",
       "      <td>0.090310</td>\n",
       "      <td>0.018486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>-0.062443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.347785</td>\n",
       "      <td>0.749094</td>\n",
       "      <td>0.477477</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.608025</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.718310</td>\n",
       "      <td>0.623874</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.617470</td>\n",
       "      <td>0.701149</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.420445</td>\n",
       "      <td>0.423292</td>\n",
       "      <td>0.533064</td>\n",
       "      <td>0.430661</td>\n",
       "      <td>0.470994</td>\n",
       "      <td>0.087748</td>\n",
       "      <td>0.238427</td>\n",
       "      <td>-0.062443</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.386552</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.212722</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.335037</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.434647</td>\n",
       "      <td>0.869941</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.711712</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.584821</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.719008</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.795775</td>\n",
       "      <td>0.707207</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.533827</td>\n",
       "      <td>0.542242</td>\n",
       "      <td>0.637676</td>\n",
       "      <td>0.534411</td>\n",
       "      <td>0.557184</td>\n",
       "      <td>0.301316</td>\n",
       "      <td>0.394328</td>\n",
       "      <td>-0.062443</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.386552</td>\n",
       "      <td>0.385298</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.270214</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.335037</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.602658</td>\n",
       "      <td>0.974261</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.835586</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.785354</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.879630</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.855952</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.808559</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660125</td>\n",
       "      <td>0.660893</td>\n",
       "      <td>0.760517</td>\n",
       "      <td>0.691822</td>\n",
       "      <td>0.667565</td>\n",
       "      <td>0.483357</td>\n",
       "      <td>0.527860</td>\n",
       "      <td>-0.062424</td>\n",
       "      <td>0.322743</td>\n",
       "      <td>0.866118</td>\n",
       "      <td>0.732528</td>\n",
       "      <td>0.760878</td>\n",
       "      <td>0.687006</td>\n",
       "      <td>0.440897</td>\n",
       "      <td>0.836375</td>\n",
       "      <td>0.726893</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.684436</td>\n",
       "      <td>0.797450</td>\n",
       "      <td>0.725080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.005873</td>\n",
       "      <td>1.095787</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>1.045045</td>\n",
       "      <td>1.098765</td>\n",
       "      <td>1.020202</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>1.092593</td>\n",
       "      <td>1.027027</td>\n",
       "      <td>1.321429</td>\n",
       "      <td>0.983471</td>\n",
       "      <td>1.109375</td>\n",
       "      <td>1.019048</td>\n",
       "      <td>1.056338</td>\n",
       "      <td>1.018018</td>\n",
       "      <td>1.127451</td>\n",
       "      <td>1.204082</td>\n",
       "      <td>1.007407</td>\n",
       "      <td>1.072289</td>\n",
       "      <td>1.045977</td>\n",
       "      <td>1.217391</td>\n",
       "      <td>0.872305</td>\n",
       "      <td>0.889567</td>\n",
       "      <td>1.874994</td>\n",
       "      <td>1.276220</td>\n",
       "      <td>1.493443</td>\n",
       "      <td>0.859863</td>\n",
       "      <td>0.796214</td>\n",
       "      <td>0.591384</td>\n",
       "      <td>0.732025</td>\n",
       "      <td>1.001455</td>\n",
       "      <td>0.958972</td>\n",
       "      <td>1.004991</td>\n",
       "      <td>0.994911</td>\n",
       "      <td>0.807591</td>\n",
       "      <td>0.991809</td>\n",
       "      <td>0.947618</td>\n",
       "      <td>0.920229</td>\n",
       "      <td>0.950460</td>\n",
       "      <td>0.993812</td>\n",
       "      <td>0.978044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FLOW_LUZ  Hourly Demand  TMAX_Cubi Point   TMAX_NAIA   TMIN_NAIA  \\\n",
       "count  146.000000     146.000000       146.000000  146.000000  146.000000   \n",
       "mean     0.462319       0.841316         0.600518    0.698815    0.725351   \n",
       "std      0.185224       0.164575         0.191032    0.169632    0.165854   \n",
       "min      0.075273       0.286574         0.126126    0.144144    0.320988   \n",
       "25%      0.347785       0.749094         0.477477    0.594595    0.608025   \n",
       "50%      0.434647       0.869941         0.594595    0.711712    0.740741   \n",
       "75%      0.602658       0.974261         0.736486    0.835586    0.851852   \n",
       "max      1.005873       1.095787         0.981982    1.045045    1.098765   \n",
       "\n",
       "       TMAX_Science Garden  TMAX_San Jose  TMIN_San Jose  TMAX_Tayabas  \\\n",
       "count           146.000000     146.000000     146.000000    146.000000   \n",
       "mean              0.679120       0.575893       0.827182      0.735222   \n",
       "std               0.186608       0.180428       0.112834      0.136298   \n",
       "min               0.040404       0.062500       0.444444      0.198198   \n",
       "25%               0.595960       0.464286       0.768519      0.648649   \n",
       "50%               0.696970       0.584821       0.819444      0.756757   \n",
       "75%               0.785354       0.687500       0.879630      0.828829   \n",
       "max               1.020202       0.982143       1.092593      1.027027   \n",
       "\n",
       "       TMIN_Tayabas   TMAX_CLSU   TMIN_CLSU  TMAX_Tanay  TMIN_Tanay  \\\n",
       "count    146.000000  146.000000  146.000000  146.000000  146.000000   \n",
       "mean       0.783268    0.683573    0.758455    0.709361    0.782124   \n",
       "std        0.158137    0.165557    0.179922    0.183633    0.132843   \n",
       "min        0.339286    0.198347    0.062500    0.238095    0.422535   \n",
       "25%        0.696429    0.595041    0.687500    0.619048    0.718310   \n",
       "50%        0.785714    0.719008    0.765625    0.742857    0.795775   \n",
       "75%        0.875000    0.793388    0.859375    0.855952    0.873239   \n",
       "max        1.321429    0.983471    1.109375    1.019048    1.056338   \n",
       "\n",
       "       TMAX_Ambulong  TMAX_Casiguran  TMIN_Casiguran  TMAX_Clark  TMIN_Clark  \\\n",
       "count     146.000000      146.000000      146.000000  146.000000  146.000000   \n",
       "mean        0.683019        0.755909        0.784806    0.715373    0.706057   \n",
       "std         0.167520        0.141552        0.115302    0.176049    0.148321   \n",
       "min         0.144144        0.431373        0.306122    0.133333    0.277108   \n",
       "25%         0.623874        0.666667        0.714286    0.666667    0.617470   \n",
       "50%         0.707207        0.784314        0.795918    0.766667    0.710843   \n",
       "75%         0.808559        0.882353        0.852041    0.837037    0.807229   \n",
       "max         1.018018        1.127451        1.204082    1.007407    1.072289   \n",
       "\n",
       "       TMAX_Calapan  TMIN_Calapan        GWAP        LWAP  TMIN_Cubi Point  \\\n",
       "count    146.000000    146.000000  146.000000  146.000000       146.000000   \n",
       "mean       0.773658      0.866885    0.547668    0.547750         0.653095   \n",
       "std        0.140960      0.188844    0.170823    0.174764         0.224561   \n",
       "min        0.195402      0.173913    0.204948    0.193783         0.168933   \n",
       "25%        0.701149      0.782609    0.420445    0.423292         0.533064   \n",
       "50%        0.804598      0.891304    0.533827    0.542242         0.637676   \n",
       "75%        0.862069      1.000000    0.660125    0.660893         0.760517   \n",
       "max        1.045977      1.217391    0.872305    0.889567         1.874994   \n",
       "\n",
       "       TMIN_Science Garden  TMIN_Ambulong  RESERVE_GWAP_Fr  RESERVE_GWAP_Ru  \\\n",
       "count           146.000000     146.000000       146.000000       146.000000   \n",
       "mean              0.564982       0.580869         0.306686         0.396148   \n",
       "std               0.221538       0.240512         0.238990         0.201831   \n",
       "min               0.090310       0.018486         0.000000         0.014423   \n",
       "25%               0.430661       0.470994         0.087748         0.238427   \n",
       "50%               0.534411       0.557184         0.301316         0.394328   \n",
       "75%               0.691822       0.667565         0.483357         0.527860   \n",
       "max               1.276220       1.493443         0.859863         0.796214   \n",
       "\n",
       "       RESERVE_GWAP_Rd  RESERVE_GWAP_Dr  RAINFALL_Cubi Point  RAINFALL_NAIA  \\\n",
       "count       146.000000       146.000000           146.000000     146.000000   \n",
       "mean         -0.019063         0.158629             0.626125       0.501174   \n",
       "std           0.138439         0.207967             0.236831       0.264479   \n",
       "min          -0.062443         0.000000             0.000000       0.000000   \n",
       "25%          -0.062443         0.000022             0.493149       0.386552   \n",
       "50%          -0.062443         0.000030             0.493149       0.386552   \n",
       "75%          -0.062424         0.322743             0.866118       0.732528   \n",
       "max           0.591384         0.732025             1.001455       0.958972   \n",
       "\n",
       "       RAINFALL_Science Garden  RAINFALL_San Jose  RAINFALL_Tayabas  \\\n",
       "count               146.000000         146.000000        146.000000   \n",
       "mean                  0.510468           0.534797          0.324867   \n",
       "std                   0.271226           0.222858          0.190269   \n",
       "min                   0.000000           0.000000          0.000000   \n",
       "25%                   0.347275           0.417791          0.212722   \n",
       "50%                   0.385298           0.417791          0.270214   \n",
       "75%                   0.760878           0.687006          0.440897   \n",
       "max                   1.004991           0.994911          0.807591   \n",
       "\n",
       "       RAINFALL_CLSU  RAINFALL_Tanay  RAINFALL_Ambulong  RAINFALL_Casiguran  \\\n",
       "count     146.000000      146.000000         146.000000          146.000000   \n",
       "mean        0.629076        0.484759           0.549820            0.509254   \n",
       "std         0.218323        0.245374           0.220948            0.205980   \n",
       "min         0.000000        0.000000           0.000000            0.000000   \n",
       "25%         0.493466        0.335037           0.413668            0.383035   \n",
       "50%         0.493466        0.335037           0.413668            0.383035   \n",
       "75%         0.836375        0.726893           0.768000            0.684436   \n",
       "max         0.991809        0.947618           0.920229            0.950460   \n",
       "\n",
       "       RAINFALL_Clark  RAINFALL_Calapan  \n",
       "count      146.000000        146.000000  \n",
       "mean         0.542504          0.551526  \n",
       "std          0.268478          0.199414  \n",
       "min          0.000000          0.000000  \n",
       "25%          0.422884          0.435920  \n",
       "50%          0.422884          0.435920  \n",
       "75%          0.797450          0.725080  \n",
       "max          0.993812          0.978044  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLOW_LUZ</th>\n",
       "      <th>Hourly Demand</th>\n",
       "      <th>TMAX_Cubi Point</th>\n",
       "      <th>TMAX_NAIA</th>\n",
       "      <th>TMIN_NAIA</th>\n",
       "      <th>TMAX_Science Garden</th>\n",
       "      <th>TMAX_San Jose</th>\n",
       "      <th>TMIN_San Jose</th>\n",
       "      <th>TMAX_Tayabas</th>\n",
       "      <th>TMIN_Tayabas</th>\n",
       "      <th>TMAX_CLSU</th>\n",
       "      <th>TMIN_CLSU</th>\n",
       "      <th>TMAX_Tanay</th>\n",
       "      <th>TMIN_Tanay</th>\n",
       "      <th>TMAX_Ambulong</th>\n",
       "      <th>TMAX_Casiguran</th>\n",
       "      <th>TMIN_Casiguran</th>\n",
       "      <th>TMAX_Clark</th>\n",
       "      <th>TMIN_Clark</th>\n",
       "      <th>TMAX_Calapan</th>\n",
       "      <th>TMIN_Calapan</th>\n",
       "      <th>GWAP</th>\n",
       "      <th>LWAP</th>\n",
       "      <th>TMIN_Cubi Point</th>\n",
       "      <th>TMIN_Science Garden</th>\n",
       "      <th>TMIN_Ambulong</th>\n",
       "      <th>RESERVE_GWAP_Fr</th>\n",
       "      <th>RESERVE_GWAP_Ru</th>\n",
       "      <th>RESERVE_GWAP_Rd</th>\n",
       "      <th>RESERVE_GWAP_Dr</th>\n",
       "      <th>RAINFALL_Cubi Point</th>\n",
       "      <th>RAINFALL_NAIA</th>\n",
       "      <th>RAINFALL_Science Garden</th>\n",
       "      <th>RAINFALL_San Jose</th>\n",
       "      <th>RAINFALL_Tayabas</th>\n",
       "      <th>RAINFALL_CLSU</th>\n",
       "      <th>RAINFALL_Tanay</th>\n",
       "      <th>RAINFALL_Ambulong</th>\n",
       "      <th>RAINFALL_Casiguran</th>\n",
       "      <th>RAINFALL_Clark</th>\n",
       "      <th>RAINFALL_Calapan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.557478</td>\n",
       "      <td>0.790429</td>\n",
       "      <td>0.538072</td>\n",
       "      <td>0.593546</td>\n",
       "      <td>0.685016</td>\n",
       "      <td>0.543171</td>\n",
       "      <td>0.560176</td>\n",
       "      <td>0.810185</td>\n",
       "      <td>0.658213</td>\n",
       "      <td>0.711840</td>\n",
       "      <td>0.639251</td>\n",
       "      <td>0.755886</td>\n",
       "      <td>0.590085</td>\n",
       "      <td>0.722458</td>\n",
       "      <td>0.563618</td>\n",
       "      <td>0.671501</td>\n",
       "      <td>0.736791</td>\n",
       "      <td>0.604718</td>\n",
       "      <td>0.642680</td>\n",
       "      <td>0.694221</td>\n",
       "      <td>0.822111</td>\n",
       "      <td>0.425797</td>\n",
       "      <td>0.413879</td>\n",
       "      <td>0.605789</td>\n",
       "      <td>0.490719</td>\n",
       "      <td>0.572939</td>\n",
       "      <td>0.248807</td>\n",
       "      <td>0.509619</td>\n",
       "      <td>0.020389</td>\n",
       "      <td>0.126388</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.489098</td>\n",
       "      <td>0.484212</td>\n",
       "      <td>0.512489</td>\n",
       "      <td>0.409320</td>\n",
       "      <td>0.574505</td>\n",
       "      <td>0.525290</td>\n",
       "      <td>0.569485</td>\n",
       "      <td>0.595334</td>\n",
       "      <td>0.482560</td>\n",
       "      <td>0.599255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.181921</td>\n",
       "      <td>0.150292</td>\n",
       "      <td>0.142749</td>\n",
       "      <td>0.129288</td>\n",
       "      <td>0.125611</td>\n",
       "      <td>0.164609</td>\n",
       "      <td>0.171825</td>\n",
       "      <td>0.092766</td>\n",
       "      <td>0.184081</td>\n",
       "      <td>0.135763</td>\n",
       "      <td>0.136037</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.176289</td>\n",
       "      <td>0.113633</td>\n",
       "      <td>0.149362</td>\n",
       "      <td>0.188913</td>\n",
       "      <td>0.120253</td>\n",
       "      <td>0.104543</td>\n",
       "      <td>0.098211</td>\n",
       "      <td>0.172207</td>\n",
       "      <td>0.145781</td>\n",
       "      <td>0.129339</td>\n",
       "      <td>0.128884</td>\n",
       "      <td>0.141149</td>\n",
       "      <td>0.172058</td>\n",
       "      <td>0.159695</td>\n",
       "      <td>0.155736</td>\n",
       "      <td>0.155523</td>\n",
       "      <td>0.065825</td>\n",
       "      <td>0.162759</td>\n",
       "      <td>0.238283</td>\n",
       "      <td>0.249329</td>\n",
       "      <td>0.289652</td>\n",
       "      <td>0.260219</td>\n",
       "      <td>0.247500</td>\n",
       "      <td>0.192706</td>\n",
       "      <td>0.222899</td>\n",
       "      <td>0.170946</td>\n",
       "      <td>0.221186</td>\n",
       "      <td>0.259388</td>\n",
       "      <td>0.180647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.030434</td>\n",
       "      <td>0.274650</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.283951</td>\n",
       "      <td>-0.010101</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.198198</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.274074</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.126463</td>\n",
       "      <td>0.117123</td>\n",
       "      <td>0.284784</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>0.248106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.062443</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.438704</td>\n",
       "      <td>0.702188</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.466518</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.531532</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.450450</td>\n",
       "      <td>0.553922</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.548148</td>\n",
       "      <td>0.581325</td>\n",
       "      <td>0.563218</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.331399</td>\n",
       "      <td>0.318027</td>\n",
       "      <td>0.495312</td>\n",
       "      <td>0.377928</td>\n",
       "      <td>0.470994</td>\n",
       "      <td>0.140552</td>\n",
       "      <td>0.449355</td>\n",
       "      <td>-0.051848</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.386552</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.212722</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.335037</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.558182</td>\n",
       "      <td>0.824704</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.679012</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.718310</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.413897</td>\n",
       "      <td>0.399998</td>\n",
       "      <td>0.609240</td>\n",
       "      <td>0.473266</td>\n",
       "      <td>0.557184</td>\n",
       "      <td>0.248140</td>\n",
       "      <td>0.554680</td>\n",
       "      <td>0.021794</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.473708</td>\n",
       "      <td>0.465016</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.403740</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.512369</td>\n",
       "      <td>0.503364</td>\n",
       "      <td>0.575704</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.514458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.693362</td>\n",
       "      <td>0.898801</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.787037</td>\n",
       "      <td>0.664141</td>\n",
       "      <td>0.707589</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.721429</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.682432</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.506575</td>\n",
       "      <td>0.495505</td>\n",
       "      <td>0.717940</td>\n",
       "      <td>0.583711</td>\n",
       "      <td>0.652418</td>\n",
       "      <td>0.357595</td>\n",
       "      <td>0.605170</td>\n",
       "      <td>0.065220</td>\n",
       "      <td>0.286576</td>\n",
       "      <td>0.760694</td>\n",
       "      <td>0.662005</td>\n",
       "      <td>0.710276</td>\n",
       "      <td>0.729482</td>\n",
       "      <td>0.632636</td>\n",
       "      <td>0.735342</td>\n",
       "      <td>0.717194</td>\n",
       "      <td>0.719330</td>\n",
       "      <td>0.812450</td>\n",
       "      <td>0.643084</td>\n",
       "      <td>0.769946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997687</td>\n",
       "      <td>1.025971</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.855856</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.074074</td>\n",
       "      <td>1.117117</td>\n",
       "      <td>1.107143</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.253521</td>\n",
       "      <td>0.909910</td>\n",
       "      <td>1.029412</td>\n",
       "      <td>1.061224</td>\n",
       "      <td>0.785185</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>1.034483</td>\n",
       "      <td>1.173913</td>\n",
       "      <td>0.871075</td>\n",
       "      <td>0.863151</td>\n",
       "      <td>0.973942</td>\n",
       "      <td>1.051195</td>\n",
       "      <td>1.198596</td>\n",
       "      <td>0.698703</td>\n",
       "      <td>0.853544</td>\n",
       "      <td>0.178230</td>\n",
       "      <td>0.614714</td>\n",
       "      <td>0.993971</td>\n",
       "      <td>0.937120</td>\n",
       "      <td>1.019252</td>\n",
       "      <td>0.988346</td>\n",
       "      <td>0.884560</td>\n",
       "      <td>0.953031</td>\n",
       "      <td>0.920012</td>\n",
       "      <td>0.926266</td>\n",
       "      <td>0.986848</td>\n",
       "      <td>0.987053</td>\n",
       "      <td>1.000699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FLOW_LUZ  Hourly Demand  TMAX_Cubi Point   TMAX_NAIA   TMIN_NAIA  \\\n",
       "count  146.000000     146.000000       146.000000  146.000000  146.000000   \n",
       "mean     0.557478       0.790429         0.538072    0.593546    0.685016   \n",
       "std      0.181921       0.150292         0.142749    0.129288    0.125611   \n",
       "min      0.030434       0.274650         0.099099    0.243243    0.283951   \n",
       "25%      0.438704       0.702188         0.452703    0.513514    0.592593   \n",
       "50%      0.558182       0.824704         0.576577    0.594595    0.679012   \n",
       "75%      0.693362       0.898801         0.648649    0.693694    0.787037   \n",
       "max      0.997687       1.025971         0.765766    0.855856    0.975309   \n",
       "\n",
       "       TMAX_Science Garden  TMAX_San Jose  TMIN_San Jose  TMAX_Tayabas  \\\n",
       "count           146.000000     146.000000     146.000000    146.000000   \n",
       "mean              0.543171       0.560176       0.810185      0.658213   \n",
       "std               0.164609       0.171825       0.092766      0.184081   \n",
       "min              -0.010101      -0.071429       0.583333      0.198198   \n",
       "25%               0.444444       0.466518       0.743056      0.531532   \n",
       "50%               0.575758       0.553571       0.796296      0.666667   \n",
       "75%               0.664141       0.707589       0.861111      0.790541   \n",
       "max               0.888889       0.875000       1.074074      1.117117   \n",
       "\n",
       "       TMIN_Tayabas   TMAX_CLSU   TMIN_CLSU  TMAX_Tanay  TMIN_Tanay  \\\n",
       "count    146.000000  146.000000  146.000000  146.000000  146.000000   \n",
       "mean       0.711840    0.639251    0.755886    0.590085    0.722458   \n",
       "std        0.135763    0.136037    0.111262    0.176289    0.113633   \n",
       "min        0.339286    0.190083    0.390625    0.190476    0.436620   \n",
       "25%        0.642857    0.595041    0.687500    0.469048    0.647887   \n",
       "50%        0.696429    0.661157    0.765625    0.600000    0.718310   \n",
       "75%        0.785714    0.727273    0.843750    0.721429    0.788732   \n",
       "max        1.107143    0.884298    1.000000    0.952381    1.253521   \n",
       "\n",
       "       TMAX_Ambulong  TMAX_Casiguran  TMIN_Casiguran  TMAX_Clark  TMIN_Clark  \\\n",
       "count     146.000000      146.000000      146.000000  146.000000  146.000000   \n",
       "mean        0.563618        0.671501        0.736791    0.604718    0.642680   \n",
       "std         0.149362        0.188913        0.120253    0.104543    0.098211   \n",
       "min         0.180180        0.127451        0.459184    0.274074    0.349398   \n",
       "25%         0.450450        0.553922        0.673469    0.548148    0.581325   \n",
       "50%         0.576577        0.686275        0.734694    0.622222    0.650602   \n",
       "75%         0.682432        0.803922        0.816327    0.674074    0.710843   \n",
       "max         0.909910        1.029412        1.061224    0.785185    0.903614   \n",
       "\n",
       "       TMAX_Calapan  TMIN_Calapan        GWAP        LWAP  TMIN_Cubi Point  \\\n",
       "count    146.000000    146.000000  146.000000  146.000000       146.000000   \n",
       "mean       0.694221      0.822111    0.425797    0.413879         0.605789   \n",
       "std        0.172207      0.145781    0.129339    0.128884         0.141149   \n",
       "min        0.080460      0.391304    0.126463    0.117123         0.284784   \n",
       "25%        0.563218      0.717391    0.331399    0.318027         0.495312   \n",
       "50%        0.689655      0.826087    0.413897    0.399998         0.609240   \n",
       "75%        0.833333      0.913043    0.506575    0.495505         0.717940   \n",
       "max        1.034483      1.173913    0.871075    0.863151         0.973942   \n",
       "\n",
       "       TMIN_Science Garden  TMIN_Ambulong  RESERVE_GWAP_Fr  RESERVE_GWAP_Ru  \\\n",
       "count           146.000000     146.000000       146.000000       146.000000   \n",
       "mean              0.490719       0.572939         0.248807         0.509619   \n",
       "std               0.172058       0.159695         0.155736         0.155523   \n",
       "min               0.143431       0.248106         0.000000         0.000000   \n",
       "25%               0.377928       0.470994         0.140552         0.449355   \n",
       "50%               0.473266       0.557184         0.248140         0.554680   \n",
       "75%               0.583711       0.652418         0.357595         0.605170   \n",
       "max               1.051195       1.198596         0.698703         0.853544   \n",
       "\n",
       "       RESERVE_GWAP_Rd  RESERVE_GWAP_Dr  RAINFALL_Cubi Point  RAINFALL_NAIA  \\\n",
       "count       146.000000       146.000000           146.000000     146.000000   \n",
       "mean          0.020389         0.126388             0.584337       0.489098   \n",
       "std           0.065825         0.162759             0.238283       0.249329   \n",
       "min          -0.062443         0.000008             0.000000       0.000000   \n",
       "25%          -0.051848         0.000022             0.493149       0.386552   \n",
       "50%           0.021794         0.000058             0.493149       0.473708   \n",
       "75%           0.065220         0.286576             0.760694       0.662005   \n",
       "max           0.178230         0.614714             0.993971       0.937120   \n",
       "\n",
       "       RAINFALL_Science Garden  RAINFALL_San Jose  RAINFALL_Tayabas  \\\n",
       "count               146.000000         146.000000        146.000000   \n",
       "mean                  0.484212           0.512489          0.409320   \n",
       "std                   0.289652           0.260219          0.247500   \n",
       "min                   0.000000           0.000000          0.000000   \n",
       "25%                   0.347275           0.417791          0.212722   \n",
       "50%                   0.465016           0.417791          0.403740   \n",
       "75%                   0.710276           0.729482          0.632636   \n",
       "max                   1.019252           0.988346          0.884560   \n",
       "\n",
       "       RAINFALL_CLSU  RAINFALL_Tanay  RAINFALL_Ambulong  RAINFALL_Casiguran  \\\n",
       "count     146.000000      146.000000         146.000000          146.000000   \n",
       "mean        0.574505        0.525290           0.569485            0.595334   \n",
       "std         0.192706        0.222899           0.170946            0.221186   \n",
       "min         0.000000        0.000000           0.413668            0.000000   \n",
       "25%         0.493466        0.335037           0.413668            0.383035   \n",
       "50%         0.493466        0.512369           0.503364            0.575704   \n",
       "75%         0.735342        0.717194           0.719330            0.812450   \n",
       "max         0.953031        0.920012           0.926266            0.986848   \n",
       "\n",
       "       RAINFALL_Clark  RAINFALL_Calapan  \n",
       "count      146.000000        146.000000  \n",
       "mean         0.482560          0.599255  \n",
       "std          0.259388          0.180647  \n",
       "min          0.000000          0.435920  \n",
       "25%          0.422884          0.435920  \n",
       "50%          0.422884          0.514458  \n",
       "75%          0.643084          0.769946  \n",
       "max          0.987053          1.000699  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_combined = np.hstack([train_data_minmax, train_data_bc, train_data_yj])\n",
    "val_data_combined = np.hstack([val_data_minmax, val_data_bc, val_data_yj])\n",
    "test_data_combined = np.hstack([test_data_minmax, test_data_bc, test_data_yj])\n",
    "\n",
    "# Create DataFrames for each combined dataset\n",
    "train_data_combined_df = pd.DataFrame(train_data_combined, columns=minmax_cols + boxcox_cols + yeojohnson_cols)\n",
    "val_data_combined_df = pd.DataFrame(val_data_combined, columns=minmax_cols + boxcox_cols + yeojohnson_cols)\n",
    "test_data_combined_df = pd.DataFrame(test_data_combined, columns=minmax_cols + boxcox_cols + yeojohnson_cols)\n",
    "\n",
    "# Ensure all columns are displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the summary statistics for each column in the combined validation data\n",
    "print(\"Summary statistics for each column in the train,val,test data:\")\n",
    "display(train_data_combined_df.describe(), val_data_combined_df.describe(), test_data_combined_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx:idx+self.seq_len], self.y[idx+self.seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(train_data_transformed, train_labels_transformed, seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TimeSeriesDataset(val_data_transformed, val_labels_transformed, seq_len)    \n",
    "val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False) \n",
    "\n",
    "test_dataset = TimeSeriesDataset(test_data_transformed, test_labels_transformed, seq_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCustomCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, activation_fn):\n",
    "        super(LSTMCustomCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "        # Combine all gate matrices into one large matrix for efficiency\n",
    "        self.W_ih = nn.Linear(input_size, 4 * hidden_size, bias=False)\n",
    "        self.W_hh = nn.Linear(hidden_size, 4 * hidden_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(4 * hidden_size))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        \n",
    "        # Optimized matrix multiplication and bias addition\n",
    "        gates = self.W_ih(x) + self.W_hh(h) + self.bias\n",
    "        \n",
    "        # Split into 4 gate vectors\n",
    "        i_gate, f_gate, o_gate, g_gate = torch.chunk(gates, 4, dim=1)\n",
    "        \n",
    "        # Sigmoid activations for gates\n",
    "        i_gate = torch.sigmoid(i_gate)\n",
    "        f_gate = torch.sigmoid(f_gate)\n",
    "        o_gate = torch.sigmoid(o_gate)\n",
    "        \n",
    "        # Apply the custom activation function for the cell gate\n",
    "        g_gate = self.activation_fn(g_gate)\n",
    "        \n",
    "        # Compute the new cell state\n",
    "        c_next = f_gate * c + i_gate * g_gate\n",
    "        \n",
    "        # Compute the new hidden state using the custom activation function\n",
    "        h_next = o_gate * self.activation_fn(c_next)\n",
    "        \n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCustom(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, activation_fn=torch.tanh, batch_first=False):\n",
    "        super(LSTMCustom, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.activation_fn = activation_fn\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        # Create a list of LSTM cells\n",
    "        self.cells = nn.ModuleList([LSTMCustomCell(input_size if i == 0 else hidden_size, hidden_size, activation_fn) for i in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Determine the correct input shape\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_len, _ = x.size()\n",
    "            x = x.transpose(0, 1)  # Convert to (seq_len, batch_size, input_size) for processing\n",
    "        else:\n",
    "            seq_len, batch_size, _ = x.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            # Initialize hidden and cell states with zeros\n",
    "            h = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
    "        else:\n",
    "            h, c = hidden\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        # Iterate over each time step\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t, :, :]  # Input at time step t\n",
    "            for i, cell in enumerate(self.cells):\n",
    "\n",
    "                h[i], c[i] = cell(x_t, (h[i], c[i]))\n",
    "                x_t = h[i]  # Pass hidden state to the next layer\n",
    "\n",
    "            outputs.append(h[-1].unsqueeze(0))  # Collect output from the last layer\n",
    "        \n",
    "        # Stack the outputs across time steps\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        \n",
    "        # Convert output back to (batch_size, seq_len, hidden_size) if batch_first is True\n",
    "        if self.batch_first:\n",
    "            outputs = outputs.transpose(0, 1)\n",
    "        \n",
    "        # Return outputs and the last hidden and cell states\n",
    "        return outputs, (torch.stack(h), torch.stack(c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers,activation_fn):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = LSTMCustom(input_size, hidden_size, num_layers, activation_fn, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = train_data.shape[1]  # Number of features\n",
    "hidden_size = 128\n",
    "output_size = train_labels.shape[1]  # Number of output features\n",
    "num_layers = 2\n",
    "activation_fn = torch.relu\n",
    "model = LSTMModel(input_size, hidden_size,output_size, num_layers,activation_fn).to(device)\n",
    "epoch=100\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Use MSE loss for regression tasks\n",
    "optimizer = optim.AdamW(list(model.parameters()), lr=1e-4, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "def train(model, dataloader, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0  # Initialize total loss to 0\n",
    "\n",
    "    \n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Check for NaN values in inputs\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).to(device)\n",
    "        \n",
    "        # Check for NaN values in outputs\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Return the average loss over all batches\n",
    "    return total_loss / len(train_dataloader.dataset)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0  # Initialize total loss\n",
    "    \n",
    "\n",
    "    for i, (inputs, target) in enumerate(dataloader):  # Use `test_dataloader`\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        predictions = outputs.to(device)  \n",
    "    \n",
    "        loss = criterion(predictions, target)\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Return the average loss over all batches\n",
    "    \n",
    "    return total_loss/len(dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(config,epoch):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = LSTMModel(input_size, config[\"hidden_size\"], output_size, config[\"num_layers\"],activation_fn).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(list(model.parameters()), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    \n",
    "    \n",
    "\n",
    "    train_dataloader, val_dataloader = load_data()\n",
    "    for e in range(epoch):  # Replace with your actual number of epochs\n",
    "        train_loss = train(model, train_dataloader, device, optimizer, criterion)\n",
    "        val_loss = evaluate(model, val_dataloader, device, criterion)\n",
    "        \n",
    "\n",
    "        # Report the results\n",
    "        session.report(  # Highlighted change\n",
    "            {\"loss\": val_loss} # Highlighted change\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"hidden_size\": tune.choice([50, 100, 200]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-3)\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"loss\", \"training_iteration\"]\n",
    ")\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    return f\"trial_{trial.trial_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 00:10:24,902\tINFO worker.py:1807 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45961d80e4aa42d7b9ef195c1b767770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.9.20</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.0.0.dev0</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.20', ray_version='3.0.0.dev0', ray_commit='e1c4e58016f79c9c1134da45cea23e3345f5354b')"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()  # Shutdown any existing Ray instances\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 00:12:47,527\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-10-26 00:12:47 (running for 00:00:00.19)\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.09205416791650245 | Iter 1.000: -0.2659536552943772\n",
      "Logical resource usage: 10.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "+------------------------+----------+-------+---------------+-------------+--------------+----------------+\n",
      "| Trial name             | status   | loc   |   hidden_size |          lr |   num_layers |   weight_decay |\n",
      "|------------------------+----------+-------+---------------+-------------+--------------+----------------|\n",
      "| train_lstm_f9a96_00000 | PENDING  |       |           100 | 0.00363249  |            2 |    4.36647e-05 |\n",
      "| train_lstm_f9a96_00001 | PENDING  |       |           200 | 0.0211543   |            2 |    0.000658411 |\n",
      "| train_lstm_f9a96_00002 | PENDING  |       |           100 | 0.0582938   |            3 |    1.8428e-06  |\n",
      "| train_lstm_f9a96_00003 | PENDING  |       |            50 | 0.000946218 |            2 |    1.46566e-05 |\n",
      "| train_lstm_f9a96_00004 | PENDING  |       |            50 | 0.0786622   |            2 |    6.62373e-05 |\n",
      "| train_lstm_f9a96_00005 | PENDING  |       |            50 | 0.000313185 |            3 |    1.11406e-06 |\n",
      "| train_lstm_f9a96_00006 | PENDING  |       |            50 | 0.0279502   |            3 |    0.000131999 |\n",
      "| train_lstm_f9a96_00007 | PENDING  |       |            50 | 0.000166776 |            3 |    1.18959e-05 |\n",
      "| train_lstm_f9a96_00008 | PENDING  |       |           200 | 0.0074113   |            1 |    9.83318e-06 |\n",
      "| train_lstm_f9a96_00009 | PENDING  |       |           200 | 0.000945257 |            2 |    0.000154461 |\n",
      "+------------------------+----------+-------+---------------+-------------+--------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-26 00:12:52 (running for 00:00:05.28)\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.09205416791650245 | Iter 1.000: -0.2659536552943772\n",
      "Logical resource usage: 10.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "+------------------------+----------+-------+---------------+-------------+--------------+----------------+\n",
      "| Trial name             | status   | loc   |   hidden_size |          lr |   num_layers |   weight_decay |\n",
      "|------------------------+----------+-------+---------------+-------------+--------------+----------------|\n",
      "| train_lstm_f9a96_00000 | PENDING  |       |           100 | 0.00363249  |            2 |    4.36647e-05 |\n",
      "| train_lstm_f9a96_00001 | PENDING  |       |           200 | 0.0211543   |            2 |    0.000658411 |\n",
      "| train_lstm_f9a96_00002 | PENDING  |       |           100 | 0.0582938   |            3 |    1.8428e-06  |\n",
      "| train_lstm_f9a96_00003 | PENDING  |       |            50 | 0.000946218 |            2 |    1.46566e-05 |\n",
      "| train_lstm_f9a96_00004 | PENDING  |       |            50 | 0.0786622   |            2 |    6.62373e-05 |\n",
      "| train_lstm_f9a96_00005 | PENDING  |       |            50 | 0.000313185 |            3 |    1.11406e-06 |\n",
      "| train_lstm_f9a96_00006 | PENDING  |       |            50 | 0.0279502   |            3 |    0.000131999 |\n",
      "| train_lstm_f9a96_00007 | PENDING  |       |            50 | 0.000166776 |            3 |    1.18959e-05 |\n",
      "| train_lstm_f9a96_00008 | PENDING  |       |           200 | 0.0074113   |            1 |    9.83318e-06 |\n",
      "| train_lstm_f9a96_00009 | PENDING  |       |           200 | 0.000945257 |            2 |    0.000154461 |\n",
      "+------------------------+----------+-------+---------------+-------------+--------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-26 00:12:57 (running for 00:00:10.38)\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.09205416791650245 | Iter 1.000: -0.2659536552943772\n",
      "Logical resource usage: 10.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "+------------------------+----------+-------+---------------+-------------+--------------+----------------+\n",
      "| Trial name             | status   | loc   |   hidden_size |          lr |   num_layers |   weight_decay |\n",
      "|------------------------+----------+-------+---------------+-------------+--------------+----------------|\n",
      "| train_lstm_f9a96_00000 | PENDING  |       |           100 | 0.00363249  |            2 |    4.36647e-05 |\n",
      "| train_lstm_f9a96_00001 | PENDING  |       |           200 | 0.0211543   |            2 |    0.000658411 |\n",
      "| train_lstm_f9a96_00002 | PENDING  |       |           100 | 0.0582938   |            3 |    1.8428e-06  |\n",
      "| train_lstm_f9a96_00003 | PENDING  |       |            50 | 0.000946218 |            2 |    1.46566e-05 |\n",
      "| train_lstm_f9a96_00004 | PENDING  |       |            50 | 0.0786622   |            2 |    6.62373e-05 |\n",
      "| train_lstm_f9a96_00005 | PENDING  |       |            50 | 0.000313185 |            3 |    1.11406e-06 |\n",
      "| train_lstm_f9a96_00006 | PENDING  |       |            50 | 0.0279502   |            3 |    0.000131999 |\n",
      "| train_lstm_f9a96_00007 | PENDING  |       |            50 | 0.000166776 |            3 |    1.18959e-05 |\n",
      "| train_lstm_f9a96_00008 | PENDING  |       |           200 | 0.0074113   |            1 |    9.83318e-06 |\n",
      "| train_lstm_f9a96_00009 | PENDING  |       |           200 | 0.000945257 |            2 |    0.000154461 |\n",
      "+------------------------+----------+-------+---------------+-------------+--------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-26 00:13:03 (running for 00:00:15.46)\n",
      "Using AsyncHyperBand: num_stopped=11\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.12475153117728748 | Iter 1.000: -0.2646365842694859\n",
      "Logical resource usage: 10.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+----------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |          lr |   num_layers |   weight_decay |     loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+-------------+--------------+----------------+----------+----------------------|\n",
      "| train_lstm_f9a96_00001 | RUNNING    | 127.0.0.1:21576 |           200 | 0.0211543   |            2 |    0.000658411 |          |                      |\n",
      "| train_lstm_f9a96_00002 | PENDING    |                 |           100 | 0.0582938   |            3 |    1.8428e-06  |          |                      |\n",
      "| train_lstm_f9a96_00003 | PENDING    |                 |            50 | 0.000946218 |            2 |    1.46566e-05 |          |                      |\n",
      "| train_lstm_f9a96_00004 | PENDING    |                 |            50 | 0.0786622   |            2 |    6.62373e-05 |          |                      |\n",
      "| train_lstm_f9a96_00005 | PENDING    |                 |            50 | 0.000313185 |            3 |    1.11406e-06 |          |                      |\n",
      "| train_lstm_f9a96_00006 | PENDING    |                 |            50 | 0.0279502   |            3 |    0.000131999 |          |                      |\n",
      "| train_lstm_f9a96_00007 | PENDING    |                 |            50 | 0.000166776 |            3 |    1.18959e-05 |          |                      |\n",
      "| train_lstm_f9a96_00008 | PENDING    |                 |           200 | 0.0074113   |            1 |    9.83318e-06 |          |                      |\n",
      "| train_lstm_f9a96_00009 | PENDING    |                 |           200 | 0.000945257 |            2 |    0.000154461 |          |                      |\n",
      "| train_lstm_f9a96_00000 | TERMINATED | 127.0.0.1:17460 |           100 | 0.00363249  |            2 |    4.36647e-05 | 0.211365 |                    2 |\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-26 00:13:08 (running for 00:00:20.53)\n",
      "Using AsyncHyperBand: num_stopped=13\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.12475153117728748 | Iter 1.000: -0.26727072631926846\n",
      "Logical resource usage: 0/20 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (7 PENDING, 3 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |          lr |   num_layers |   weight_decay |      loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------|\n",
      "| train_lstm_f9a96_00003 | PENDING    |                 |            50 | 0.000946218 |            2 |    1.46566e-05 |           |                      |\n",
      "| train_lstm_f9a96_00004 | PENDING    |                 |            50 | 0.0786622   |            2 |    6.62373e-05 |           |                      |\n",
      "| train_lstm_f9a96_00005 | PENDING    |                 |            50 | 0.000313185 |            3 |    1.11406e-06 |           |                      |\n",
      "| train_lstm_f9a96_00006 | PENDING    |                 |            50 | 0.0279502   |            3 |    0.000131999 |           |                      |\n",
      "| train_lstm_f9a96_00007 | PENDING    |                 |            50 | 0.000166776 |            3 |    1.18959e-05 |           |                      |\n",
      "| train_lstm_f9a96_00008 | PENDING    |                 |           200 | 0.0074113   |            1 |    9.83318e-06 |           |                      |\n",
      "| train_lstm_f9a96_00009 | PENDING    |                 |           200 | 0.000945257 |            2 |    0.000154461 |           |                      |\n",
      "| train_lstm_f9a96_00000 | TERMINATED | 127.0.0.1:17460 |           100 | 0.00363249  |            2 |    4.36647e-05 |  0.211365 |                    2 |\n",
      "| train_lstm_f9a96_00001 | TERMINATED | 127.0.0.1:21576 |           200 | 0.0211543   |            2 |    0.000658411 | 11.5876   |                    1 |\n",
      "| train_lstm_f9a96_00002 | TERMINATED | 127.0.0.1:6960  |           100 | 0.0582938   |            3 |    1.8428e-06  | 85.0858   |                    1 |\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-26 00:13:13 (running for 00:00:25.62)\n",
      "Using AsyncHyperBand: num_stopped=14\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.12475153117728748 | Iter 1.000: -0.26895512651196485\n",
      "Logical resource usage: 10.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (6 PENDING, 4 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |          lr |   num_layers |   weight_decay |      loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------|\n",
      "| train_lstm_f9a96_00004 | PENDING    |                 |            50 | 0.0786622   |            2 |    6.62373e-05 |           |                      |\n",
      "| train_lstm_f9a96_00005 | PENDING    |                 |            50 | 0.000313185 |            3 |    1.11406e-06 |           |                      |\n",
      "| train_lstm_f9a96_00006 | PENDING    |                 |            50 | 0.0279502   |            3 |    0.000131999 |           |                      |\n",
      "| train_lstm_f9a96_00007 | PENDING    |                 |            50 | 0.000166776 |            3 |    1.18959e-05 |           |                      |\n",
      "| train_lstm_f9a96_00008 | PENDING    |                 |           200 | 0.0074113   |            1 |    9.83318e-06 |           |                      |\n",
      "| train_lstm_f9a96_00009 | PENDING    |                 |           200 | 0.000945257 |            2 |    0.000154461 |           |                      |\n",
      "| train_lstm_f9a96_00000 | TERMINATED | 127.0.0.1:17460 |           100 | 0.00363249  |            2 |    4.36647e-05 |  0.211365 |                    2 |\n",
      "| train_lstm_f9a96_00001 | TERMINATED | 127.0.0.1:21576 |           200 | 0.0211543   |            2 |    0.000658411 | 11.5876   |                    1 |\n",
      "| train_lstm_f9a96_00002 | TERMINATED | 127.0.0.1:6960  |           100 | 0.0582938   |            3 |    1.8428e-06  | 85.0858   |                    1 |\n",
      "| train_lstm_f9a96_00003 | TERMINATED | 127.0.0.1:20152 |            50 | 0.000946218 |            2 |    1.46566e-05 |  0.27064  |                    1 |\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-26 00:13:18 (running for 00:00:30.70)\n",
      "Using AsyncHyperBand: num_stopped=15\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.12475153117728748 | Iter 1.000: -0.2706395267046613\n",
      "Logical resource usage: 10.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (5 PENDING, 5 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |          lr |   num_layers |   weight_decay |      loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------|\n",
      "| train_lstm_f9a96_00005 | PENDING    |                 |            50 | 0.000313185 |            3 |    1.11406e-06 |           |                      |\n",
      "| train_lstm_f9a96_00006 | PENDING    |                 |            50 | 0.0279502   |            3 |    0.000131999 |           |                      |\n",
      "| train_lstm_f9a96_00007 | PENDING    |                 |            50 | 0.000166776 |            3 |    1.18959e-05 |           |                      |\n",
      "| train_lstm_f9a96_00008 | PENDING    |                 |           200 | 0.0074113   |            1 |    9.83318e-06 |           |                      |\n",
      "| train_lstm_f9a96_00009 | PENDING    |                 |           200 | 0.000945257 |            2 |    0.000154461 |           |                      |\n",
      "| train_lstm_f9a96_00000 | TERMINATED | 127.0.0.1:17460 |           100 | 0.00363249  |            2 |    4.36647e-05 |  0.211365 |                    2 |\n",
      "| train_lstm_f9a96_00001 | TERMINATED | 127.0.0.1:21576 |           200 | 0.0211543   |            2 |    0.000658411 | 11.5876   |                    1 |\n",
      "| train_lstm_f9a96_00002 | TERMINATED | 127.0.0.1:6960  |           100 | 0.0582938   |            3 |    1.8428e-06  | 85.0858   |                    1 |\n",
      "| train_lstm_f9a96_00003 | TERMINATED | 127.0.0.1:20152 |            50 | 0.000946218 |            2 |    1.46566e-05 |  0.27064  |                    1 |\n",
      "| train_lstm_f9a96_00004 | TERMINATED | 127.0.0.1:29076 |            50 | 0.0786622   |            2 |    6.62373e-05 |  1.0253   |                    1 |\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-26 00:13:23 (running for 00:00:35.71)\n",
      "Using AsyncHyperBand: num_stopped=16\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.12475153117728748 | Iter 1.000: -0.2833358576293472\n",
      "Logical resource usage: 10.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |          lr |   num_layers |   weight_decay |      loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------|\n",
      "| train_lstm_f9a96_00006 | RUNNING    | 127.0.0.1:21088 |            50 | 0.0279502   |            3 |    0.000131999 |           |                      |\n",
      "| train_lstm_f9a96_00007 | PENDING    |                 |            50 | 0.000166776 |            3 |    1.18959e-05 |           |                      |\n",
      "| train_lstm_f9a96_00008 | PENDING    |                 |           200 | 0.0074113   |            1 |    9.83318e-06 |           |                      |\n",
      "| train_lstm_f9a96_00009 | PENDING    |                 |           200 | 0.000945257 |            2 |    0.000154461 |           |                      |\n",
      "| train_lstm_f9a96_00000 | TERMINATED | 127.0.0.1:17460 |           100 | 0.00363249  |            2 |    4.36647e-05 |  0.211365 |                    2 |\n",
      "| train_lstm_f9a96_00001 | TERMINATED | 127.0.0.1:21576 |           200 | 0.0211543   |            2 |    0.000658411 | 11.5876   |                    1 |\n",
      "| train_lstm_f9a96_00002 | TERMINATED | 127.0.0.1:6960  |           100 | 0.0582938   |            3 |    1.8428e-06  | 85.0858   |                    1 |\n",
      "| train_lstm_f9a96_00003 | TERMINATED | 127.0.0.1:20152 |            50 | 0.000946218 |            2 |    1.46566e-05 |  0.27064  |                    1 |\n",
      "| train_lstm_f9a96_00004 | TERMINATED | 127.0.0.1:29076 |            50 | 0.0786622   |            2 |    6.62373e-05 |  1.0253   |                    1 |\n",
      "| train_lstm_f9a96_00005 | TERMINATED | 127.0.0.1:13936 |            50 | 0.000313185 |            3 |    1.11406e-06 |  0.302208 |                    1 |\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-26 00:13:28 (running for 00:00:40.77)\n",
      "Using AsyncHyperBand: num_stopped=18\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.16805805611631852 | Iter 1.000: -0.2833358576293472\n",
      "Logical resource usage: 0/20 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (2 PENDING, 8 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |          lr |   num_layers |   weight_decay |      loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------|\n",
      "| train_lstm_f9a96_00008 | PENDING    |                 |           200 | 0.0074113   |            1 |    9.83318e-06 |           |                      |\n",
      "| train_lstm_f9a96_00009 | PENDING    |                 |           200 | 0.000945257 |            2 |    0.000154461 |           |                      |\n",
      "| train_lstm_f9a96_00000 | TERMINATED | 127.0.0.1:17460 |           100 | 0.00363249  |            2 |    4.36647e-05 |  0.211365 |                    2 |\n",
      "| train_lstm_f9a96_00001 | TERMINATED | 127.0.0.1:21576 |           200 | 0.0211543   |            2 |    0.000658411 | 11.5876   |                    1 |\n",
      "| train_lstm_f9a96_00002 | TERMINATED | 127.0.0.1:6960  |           100 | 0.0582938   |            3 |    1.8428e-06  | 85.0858   |                    1 |\n",
      "| train_lstm_f9a96_00003 | TERMINATED | 127.0.0.1:20152 |            50 | 0.000946218 |            2 |    1.46566e-05 |  0.27064  |                    1 |\n",
      "| train_lstm_f9a96_00004 | TERMINATED | 127.0.0.1:29076 |            50 | 0.0786622   |            2 |    6.62373e-05 |  1.0253   |                    1 |\n",
      "| train_lstm_f9a96_00005 | TERMINATED | 127.0.0.1:13936 |            50 | 0.000313185 |            3 |    1.11406e-06 |  0.302208 |                    1 |\n",
      "| train_lstm_f9a96_00006 | TERMINATED | 127.0.0.1:21088 |            50 | 0.0279502   |            3 |    0.000131999 |  0.371243 |                    1 |\n",
      "| train_lstm_f9a96_00007 | TERMINATED | 127.0.0.1:43388 |            50 | 0.000166776 |            3 |    1.18959e-05 |  0.237919 |                    2 |\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-26 00:13:33 (running for 00:00:45.87)\n",
      "Using AsyncHyperBand: num_stopped=19\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.03608573013715607 | Iter 2.000: -0.17003673272167177 | Iter 1.000: -0.2706395267046613\n",
      "Logical resource usage: 10.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (1 PENDING, 9 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |          lr |   num_layers |   weight_decay |      loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------|\n",
      "| train_lstm_f9a96_00009 | PENDING    |                 |           200 | 0.000945257 |            2 |    0.000154461 |           |                      |\n",
      "| train_lstm_f9a96_00000 | TERMINATED | 127.0.0.1:17460 |           100 | 0.00363249  |            2 |    4.36647e-05 |  0.211365 |                    2 |\n",
      "| train_lstm_f9a96_00001 | TERMINATED | 127.0.0.1:21576 |           200 | 0.0211543   |            2 |    0.000658411 | 11.5876   |                    1 |\n",
      "| train_lstm_f9a96_00002 | TERMINATED | 127.0.0.1:6960  |           100 | 0.0582938   |            3 |    1.8428e-06  | 85.0858   |                    1 |\n",
      "| train_lstm_f9a96_00003 | TERMINATED | 127.0.0.1:20152 |            50 | 0.000946218 |            2 |    1.46566e-05 |  0.27064  |                    1 |\n",
      "| train_lstm_f9a96_00004 | TERMINATED | 127.0.0.1:29076 |            50 | 0.0786622   |            2 |    6.62373e-05 |  1.0253   |                    1 |\n",
      "| train_lstm_f9a96_00005 | TERMINATED | 127.0.0.1:13936 |            50 | 0.000313185 |            3 |    1.11406e-06 |  0.302208 |                    1 |\n",
      "| train_lstm_f9a96_00006 | TERMINATED | 127.0.0.1:21088 |            50 | 0.0279502   |            3 |    0.000131999 |  0.371243 |                    1 |\n",
      "| train_lstm_f9a96_00007 | TERMINATED | 127.0.0.1:43388 |            50 | 0.000166776 |            3 |    1.18959e-05 |  0.237919 |                    2 |\n",
      "| train_lstm_f9a96_00008 | TERMINATED | 127.0.0.1:41416 |           200 | 0.0074113   |            1 |    9.83318e-06 |  0.170037 |                    2 |\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 00:13:36,224\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Paulo John Mercado/ray_results/train_lstm_2024-10-26_00-12-47' in 0.0121s.\n",
      "2024-10-26 00:13:36,228\tINFO tune.py:1041 -- Total run time: 48.70 seconds (48.67 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-10-26 00:13:36 (running for 00:00:48.69)\n",
      "Using AsyncHyperBand: num_stopped=20\n",
      "Bracket: Iter 8.000: -0.032067313281109004 | Iter 4.000: -0.061799049886653745 | Iter 2.000: -0.14739413194947962 | Iter 1.000: -0.26895512651196485\n",
      "Logical resource usage: 10.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-26_00-10-16_502316_8952/artifacts/2024-10-26_00-12-47/train_lstm_2024-10-26_00-12-47/driver_artifacts\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |          lr |   num_layers |   weight_decay |      loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------|\n",
      "| train_lstm_f9a96_00000 | TERMINATED | 127.0.0.1:17460 |           100 | 0.00363249  |            2 |    4.36647e-05 |  0.211365 |                    2 |\n",
      "| train_lstm_f9a96_00001 | TERMINATED | 127.0.0.1:21576 |           200 | 0.0211543   |            2 |    0.000658411 | 11.5876   |                    1 |\n",
      "| train_lstm_f9a96_00002 | TERMINATED | 127.0.0.1:6960  |           100 | 0.0582938   |            3 |    1.8428e-06  | 85.0858   |                    1 |\n",
      "| train_lstm_f9a96_00003 | TERMINATED | 127.0.0.1:20152 |            50 | 0.000946218 |            2 |    1.46566e-05 |  0.27064  |                    1 |\n",
      "| train_lstm_f9a96_00004 | TERMINATED | 127.0.0.1:29076 |            50 | 0.0786622   |            2 |    6.62373e-05 |  1.0253   |                    1 |\n",
      "| train_lstm_f9a96_00005 | TERMINATED | 127.0.0.1:13936 |            50 | 0.000313185 |            3 |    1.11406e-06 |  0.302208 |                    1 |\n",
      "| train_lstm_f9a96_00006 | TERMINATED | 127.0.0.1:21088 |            50 | 0.0279502   |            3 |    0.000131999 |  0.371243 |                    1 |\n",
      "| train_lstm_f9a96_00007 | TERMINATED | 127.0.0.1:43388 |            50 | 0.000166776 |            3 |    1.18959e-05 |  0.237919 |                    2 |\n",
      "| train_lstm_f9a96_00008 | TERMINATED | 127.0.0.1:41416 |           200 | 0.0074113   |            1 |    9.83318e-06 |  0.170037 |                    2 |\n",
      "| train_lstm_f9a96_00009 | TERMINATED | 127.0.0.1:28268 |           200 | 0.000945257 |            2 |    0.000154461 |  0.114682 |                    4 |\n",
      "+------------------------+------------+-----------------+---------------+-------------+--------------+----------------+-----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x1f18e667970>"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\n",
    "    tune.with_parameters(train_lstm, epoch=epoch),\n",
    "    resources_per_trial={\"cpu\": 10, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    verbose=1,\n",
    "    trial_dirname_creator=trial_dirname_creator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "def run(model, train_dataloader, val_dataloader, test_dataloader, device, epoch,optimizer):\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        \n",
    "        train_loss = train(model, train_dataloader, device, optimizer, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss=evaluate(model,val_dataloader,device,criterion)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "        \n",
    "\n",
    "        if train_loss < 1e-3 :\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.4303893691010929, Val Loss: 0.35171569883823395\n",
      "Epoch 2, Training Loss: 0.41206902196401785, Val Loss: 0.3332543888216396\n",
      "Epoch 3, Training Loss: 0.3930401147933128, Val Loss: 0.31321168728440785\n",
      "Epoch 4, Training Loss: 0.3717699684980574, Val Loss: 0.2897042425737964\n",
      "Epoch 5, Training Loss: 0.346005033174413, Val Loss: 0.2597726595809134\n",
      "Epoch 6, Training Loss: 0.31275243075292236, Val Loss: 0.21953562974072188\n",
      "Epoch 7, Training Loss: 0.2678997321983501, Val Loss: 0.1641673918733065\n",
      "Epoch 8, Training Loss: 0.20540126712167345, Val Loss: 0.09149462363193575\n",
      "Epoch 9, Training Loss: 0.12020646205081187, Val Loss: 0.03463951865438935\n",
      "Epoch 10, Training Loss: 0.040617637880786116, Val Loss: 0.19386180602818084\n",
      "Epoch 11, Training Loss: 0.07423666572487825, Val Loss: 0.1369126840032262\n",
      "Epoch 12, Training Loss: 0.04671862447448507, Val Loss: 0.06311121250656869\n",
      "Epoch 13, Training Loss: 0.04280411270544313, Val Loss: 0.052750153105250365\n",
      "Epoch 14, Training Loss: 0.03988118266596042, Val Loss: 0.06375834319016917\n",
      "Epoch 15, Training Loss: 0.03661211966396485, Val Loss: 0.08246227423492954\n",
      "Epoch 16, Training Loss: 0.03689275156455914, Val Loss: 0.08769416423152676\n",
      "Epoch 17, Training Loss: 0.03655937186111707, Val Loss: 0.07903790988510462\n",
      "Epoch 18, Training Loss: 0.03499016106094119, Val Loss: 0.07158747363647969\n",
      "Epoch 19, Training Loss: 0.03351430201648006, Val Loss: 0.07029168066575373\n",
      "Epoch 20, Training Loss: 0.03227908742503888, Val Loss: 0.07206227491013438\n",
      "Epoch 21, Training Loss: 0.03131370106820026, Val Loss: 0.07253060752539325\n",
      "Epoch 22, Training Loss: 0.03038195289861575, Val Loss: 0.0702899188446484\n",
      "Epoch 23, Training Loss: 0.029320239182345553, Val Loss: 0.06701892233795399\n",
      "Epoch 24, Training Loss: 0.02823491081197826, Val Loss: 0.06440991994073923\n",
      "Epoch 25, Training Loss: 0.02724988390356648, Val Loss: 0.06222686176891807\n",
      "Epoch 26, Training Loss: 0.02640065911664625, Val Loss: 0.059665465204835795\n",
      "Epoch 27, Training Loss: 0.025616498880105615, Val Loss: 0.05696040659928493\n",
      "Epoch 28, Training Loss: 0.024872776259194946, Val Loss: 0.05468691601384458\n",
      "Epoch 29, Training Loss: 0.02421212132389197, Val Loss: 0.05280157912978165\n",
      "Epoch 30, Training Loss: 0.023620868749968413, Val Loss: 0.05092985847656675\n",
      "Epoch 31, Training Loss: 0.023073570413856385, Val Loss: 0.0490986053570569\n",
      "Epoch 32, Training Loss: 0.02256395933142671, Val Loss: 0.04734867720080794\n",
      "Epoch 33, Training Loss: 0.022093597166413084, Val Loss: 0.045735503272186936\n",
      "Epoch 34, Training Loss: 0.021662462018689533, Val Loss: 0.04422398742368753\n",
      "Epoch 35, Training Loss: 0.021261492538808255, Val Loss: 0.04276321817645066\n",
      "Epoch 36, Training Loss: 0.020890521561930046, Val Loss: 0.04141498244494843\n",
      "Epoch 37, Training Loss: 0.020548652455625566, Val Loss: 0.04013330130268344\n",
      "Epoch 38, Training Loss: 0.02023073520701543, Val Loss: 0.038955664409579135\n",
      "Epoch 39, Training Loss: 0.019932471129291297, Val Loss: 0.037868784003549344\n",
      "Epoch 40, Training Loss: 0.01964911667461531, Val Loss: 0.036849529247918573\n",
      "Epoch 41, Training Loss: 0.019374087005594353, Val Loss: 0.035857489158352504\n",
      "Epoch 42, Training Loss: 0.019112733264049633, Val Loss: 0.03489712100449226\n",
      "Epoch 43, Training Loss: 0.018862600392919128, Val Loss: 0.03397960741099694\n",
      "Epoch 44, Training Loss: 0.01862584698616836, Val Loss: 0.033165631855992105\n",
      "Epoch 45, Training Loss: 0.01839900853462396, Val Loss: 0.032435212096722\n",
      "Epoch 46, Training Loss: 0.018182500805749003, Val Loss: 0.03176389479165455\n",
      "Epoch 47, Training Loss: 0.017974406969139305, Val Loss: 0.0311777176831266\n",
      "Epoch 48, Training Loss: 0.017774057304465025, Val Loss: 0.030640176999911988\n",
      "Epoch 49, Training Loss: 0.01758088742592426, Val Loss: 0.03013987509573964\n",
      "Epoch 50, Training Loss: 0.01739224548432155, Val Loss: 0.029684833673050078\n",
      "Epoch 51, Training Loss: 0.017209766616314442, Val Loss: 0.02926523061536199\n",
      "Epoch 52, Training Loss: 0.01702920302506666, Val Loss: 0.028881041828891355\n",
      "Epoch 53, Training Loss: 0.016853440924614597, Val Loss: 0.028511857010906548\n",
      "Epoch 54, Training Loss: 0.016682094263383934, Val Loss: 0.02820384698055631\n",
      "Epoch 55, Training Loss: 0.01651585039849071, Val Loss: 0.027925940029483905\n",
      "Epoch 56, Training Loss: 0.016353946285032755, Val Loss: 0.02767185443382469\n",
      "Epoch 57, Training Loss: 0.016197397233162016, Val Loss: 0.027469970553898982\n",
      "Epoch 58, Training Loss: 0.016044368122524976, Val Loss: 0.027303643819453906\n",
      "Epoch 59, Training Loss: 0.015894844760716377, Val Loss: 0.027151619975301\n",
      "Epoch 60, Training Loss: 0.01574824864948017, Val Loss: 0.027025306021138062\n",
      "Epoch 61, Training Loss: 0.015604474938316855, Val Loss: 0.026900989706996534\n",
      "Epoch 62, Training Loss: 0.015465061299139828, Val Loss: 0.026774207465082623\n",
      "Epoch 63, Training Loss: 0.015326604826198654, Val Loss: 0.026664700225102814\n",
      "Epoch 64, Training Loss: 0.015187118835574375, Val Loss: 0.026560450280956226\n",
      "Epoch 65, Training Loss: 0.015050721709160544, Val Loss: 0.026463433992948465\n",
      "Epoch 66, Training Loss: 0.014917183708477324, Val Loss: 0.02642991559968578\n",
      "Epoch 67, Training Loss: 0.01478172635136652, Val Loss: 0.026364890121513135\n",
      "Epoch 68, Training Loss: 0.014652794164339655, Val Loss: 0.026356774938406703\n",
      "Epoch 69, Training Loss: 0.014523282666580446, Val Loss: 0.026307155090055878\n",
      "Epoch 70, Training Loss: 0.01439731465025941, Val Loss: 0.026296240150285283\n",
      "Epoch 71, Training Loss: 0.014274431312599343, Val Loss: 0.026249321405407335\n",
      "Epoch 72, Training Loss: 0.014154610619502113, Val Loss: 0.02625441567288886\n",
      "Epoch 73, Training Loss: 0.014033971626631345, Val Loss: 0.02622114604325603\n",
      "Epoch 74, Training Loss: 0.013914145012829116, Val Loss: 0.02623629720090962\n",
      "Epoch 75, Training Loss: 0.01379514345359014, Val Loss: 0.02621929082188675\n",
      "Epoch 76, Training Loss: 0.01367913011738084, Val Loss: 0.026241624098029926\n",
      "Epoch 77, Training Loss: 0.013561995612487646, Val Loss: 0.026232360346282988\n",
      "Epoch 78, Training Loss: 0.013446026344474603, Val Loss: 0.026219406815098344\n",
      "Epoch 79, Training Loss: 0.013329821317878026, Val Loss: 0.02626061959446763\n",
      "Epoch 80, Training Loss: 0.013211588631330054, Val Loss: 0.026238508468909228\n",
      "Epoch 81, Training Loss: 0.01309411585646446, Val Loss: 0.02624093564294225\n",
      "Epoch 82, Training Loss: 0.012972769313416252, Val Loss: 0.026219326573953355\n",
      "Epoch 83, Training Loss: 0.0128532369608133, Val Loss: 0.02624536219903891\n",
      "Epoch 84, Training Loss: 0.01273504911928401, Val Loss: 0.02625749276267539\n",
      "Epoch 85, Training Loss: 0.012619054649379579, Val Loss: 0.02629191203297471\n",
      "Epoch 86, Training Loss: 0.012502252639610571, Val Loss: 0.026353345741685345\n",
      "Epoch 87, Training Loss: 0.012383972873734073, Val Loss: 0.026387176174911663\n",
      "Epoch 88, Training Loss: 0.01226986187516675, Val Loss: 0.02650664382272487\n",
      "Epoch 89, Training Loss: 0.012150463184639984, Val Loss: 0.026466946110879776\n",
      "Epoch 90, Training Loss: 0.012036221758775811, Val Loss: 0.026575761143680956\n",
      "Epoch 91, Training Loss: 0.011920106587520954, Val Loss: 0.02669776633060236\n",
      "Epoch 92, Training Loss: 0.011804452197293463, Val Loss: 0.026875031884196852\n",
      "Epoch 93, Training Loss: 0.011690864428065354, Val Loss: 0.026839582504128382\n",
      "Epoch 94, Training Loss: 0.011582216605535448, Val Loss: 0.026988838645194073\n",
      "Epoch 95, Training Loss: 0.01147037159690413, Val Loss: 0.027108689756702175\n",
      "Epoch 96, Training Loss: 0.011357938323014367, Val Loss: 0.027191809237860947\n",
      "Epoch 97, Training Loss: 0.011258183437207251, Val Loss: 0.027484943701637735\n",
      "Epoch 98, Training Loss: 0.011140575596799084, Val Loss: 0.027517402182808882\n",
      "Epoch 99, Training Loss: 0.011034891234058238, Val Loss: 0.027567081635804486\n",
      "Epoch 100, Training Loss: 0.010928721820801636, Val Loss: 0.027863335695198114\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "run(model, train_dataloader,val_dataloader, test_dataloader, device, epoch,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Luzon GWAP: Training, Validation, and Test Losses Over Epochs')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
