{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import logging\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r'D:\\School\\ADMU\\4Y\\SEM 1\\MATH 199.11\\Final\\input_gwap_luz.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "data = data[['GWAP', 'FLOW_LUZ','GWAP_DR','GWAP_FR','GWAP_RD','GWAP_RU']]  # Select the relevant time series column\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data.values)\n",
    "\n",
    "columns = ['GWAP', 'FLOW_LUZ','GWAP_DR','GWAP_FR','GWAP_RD','GWAP_RU']\n",
    "        \n",
    "X  = data[columns].values\n",
    "y = data['GWAP'].values.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len + 1\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx:idx+self.seq_len], self.y[idx+self.seq_len-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(data))  # 70% for training\n",
    "val_size = int(0.15 * len(data))   # 15% for validation\n",
    "test_size = len(data) - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "train_data = X[:train_size]\n",
    "train_labels = y[:train_size]\n",
    "\n",
    "val_data = X[train_size:train_size + val_size]\n",
    "val_labels = y[train_size:train_size + val_size]\n",
    "\n",
    "test_data = X[train_size + val_size:]\n",
    "test_labels = y[train_size + val_size:]\n",
    "seq_len=1440\n",
    "batch_size=64\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, train_labels, seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TimeSeriesDataset(val_data, val_labels, seq_len)    \n",
    "val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False) \n",
    "\n",
    "test_dataset = TimeSeriesDataset(test_data, test_labels, seq_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "    \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Combined weights for all gates\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(4 * hidden_size, input_size))\n",
    "        self.weight_hh = nn.Parameter(torch.Tensor(4 * hidden_size, hidden_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float))\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        hx, cx = state\n",
    "        hx = self.dropout_layer(hx)  # Apply dropout to the hidden state\n",
    "        gates = F.linear(input, self.weight_ih, self.bias) + F.linear(hx, self.weight_hh)\n",
    "\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = F.relu(cellgate)  # Use ReLU here\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        hy = outgate * F.relu(cy)  # Use ReLU here as well\n",
    "\n",
    "        return hy, cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.cell_list = nn.ModuleList([CustomLSTMCell(input_size, hidden_size)])\n",
    "        self.cell_list.extend([CustomLSTMCell(hidden_size, hidden_size) for _ in range(1, num_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        hidden = [torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(self.num_layers)]\n",
    "        cell = [torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(self.num_layers)]\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            for l in range(self.num_layers):\n",
    "                if l == 0:\n",
    "                    hidden[l], cell[l] = self.cell_list[l](x[:, t, :], (hidden[l], cell[l]))\n",
    "                else:\n",
    "                    hidden[l], cell[l] = self.cell_list[l](hidden[l-1], (hidden[l], cell[l]))\n",
    "\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCustomCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, activation_fn):\n",
    "        super(LSTMCustomCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "        # Combine all gate matrices into one large matrix for efficiency\n",
    "        self.W_ih = nn.Linear(input_size, 4 * hidden_size, bias=False)\n",
    "        self.W_hh = nn.Linear(hidden_size, 4 * hidden_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(4 * hidden_size))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        \n",
    "        # Optimized matrix multiplication and bias addition\n",
    "        gates = self.W_ih(x) + self.W_hh(h) + self.bias\n",
    "        \n",
    "        # Split into 4 gate vectors\n",
    "        i_gate, f_gate, o_gate, g_gate = torch.chunk(gates, 4, dim=1)\n",
    "        \n",
    "        # Sigmoid activations for gates\n",
    "        i_gate = torch.sigmoid(i_gate)\n",
    "        f_gate = torch.sigmoid(f_gate)\n",
    "        o_gate = torch.sigmoid(o_gate)\n",
    "        \n",
    "        # Apply the custom activation function for the cell gate\n",
    "        g_gate = self.activation_fn(g_gate)\n",
    "        \n",
    "        # Compute the new cell state\n",
    "        c_next = f_gate * c + i_gate * g_gate\n",
    "        \n",
    "        # Compute the new hidden state using the custom activation function\n",
    "        h_next = o_gate * self.activation_fn(c_next)\n",
    "        \n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCustom(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, activation_fn=torch.tanh, batch_first=False):\n",
    "        super(LSTMCustom, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.activation_fn = activation_fn\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        # Create a list of LSTM cells\n",
    "        self.cells = nn.ModuleList([LSTMCustomCell(input_size if i == 0 else hidden_size, hidden_size, activation_fn) for i in range(num_layers)])\n",
    "        \n",
    "        # Add a fully connected layer to map the hidden size to the output size (1)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_len, _ = x.size()\n",
    "            x = x.transpose(0, 1)  # Convert to (seq_len, batch_size, input_size)\n",
    "        else:\n",
    "            seq_len, batch_size, _ = x.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            h = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
    "        else:\n",
    "            h, c = hidden\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t, :, :]  # Input at time step t\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                h[i], c[i] = cell(x_t, (h[i], c[i]))\n",
    "                x_t = h[i]  # Pass hidden state to the next layer\n",
    "            outputs.append(h[-1].unsqueeze(0))  # Collect output from the last layer\n",
    "        \n",
    "        # Stack the outputs across time steps\n",
    "        outputs = torch.cat(outputs, dim=0)  # Shape will be (seq_len, batch_size, hidden_size)\n",
    "        \n",
    "        # Apply the fully connected layer to the output of the last time step\n",
    "        outputs = self.fc(outputs[-1])  # Get the last output\n",
    "        outputs = outputs.view(-1, 1)  # Pass it through the fully connected layer\n",
    "\n",
    "        \n",
    "\n",
    "        # Return outputs and the last hidden and cell states\n",
    "        return outputs, (torch.stack(h), torch.stack(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = train_data.shape[1]  # Number of features\n",
    "hidden_size = 64\n",
    "output_size = train_labels.shape[1]  # Number of output features\n",
    "num_layers = 2\n",
    "#model = LSTMModel(input_size, hidden_size,output_size, num_layers).to(device)\n",
    "criterion=nn.MSELoss()\n",
    "activation_fn = torch.relu\n",
    "model = LSTMCustom(input_size, hidden_size, num_layers, activation_fn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.compile(model, backend=\"inductor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GradScaler\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "torch.cuda.synchronize()\n",
    "def train(model, train_dataloader, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0  # Initialize total loss to 0\n",
    "\n",
    "    start_data_time = time.time()\n",
    "    for i, (inputs, target) in enumerate(train_dataloader):\n",
    "        print(f\"Data loading time: {time.time() - start_data_time:.4f} seconds\")\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        print(inputs.shape)\n",
    "        print(target.shape)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs, _ = model(inputs)  # Unpack the output tuple\n",
    "            print(f\"Outputs shape after model: {outputs.shape}\")  # Debugging line\n",
    "            \n",
    "            # At this point, outputs should be of shape (batch_size, 1)\n",
    "            print(f\"Adjusted outputs shape for loss: {outputs.shape}\")  # Should be (128, 1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Return the average loss over all batches\n",
    "    return total_loss / len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, test_dataloader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0  # Initialize total loss\n",
    "\n",
    "\n",
    "    for i, (inputs, target) in enumerate(test_dataloader):  # Use `test_dataloader`\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Return the average loss over all batches\n",
    "    \n",
    "    return total_loss/len(test_dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, train_dataloader, test_dataloader, device, epoch):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2,weight_decay=1e-3)\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "         # Profiling the training phase\n",
    "        with torch.profiler.profile() as prof:\n",
    "            train_loss = train(model, train_dataloader, device, optimizer, criterion)\n",
    "        \n",
    "        # Print the profiling results\n",
    "        print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss}\")\n",
    "        loss = train(model, train_dataloader, device, optimizer,criterion)\n",
    "        \n",
    "        test_loss = evaluate(model, test_dataloader, device,criterion)\n",
    "        print(epoch, loss, test_loss)\n",
    "        if (epoch + 1) == 100 or (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch {epoch+1:04d} | loss: {loss:.4f} '\n",
    "                f'test_loss: {test_loss:.4f} ')\n",
    "    \n",
    "        if loss < 1e-3 and test_loss < 1e-3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading time: 0.0030 seconds\n",
      "torch.Size([64, 1440, 6])\n",
      "torch.Size([64, 1])\n",
      "Outputs shape after model: torch.Size([1440, 1])\n",
      "Adjusted outputs shape for loss: torch.Size([1440, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([1440, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1440) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[117], line 7\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model, train_dataloader, test_dataloader, device, epoch)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m      5\u001b[0m      \u001b[38;5;66;03m# Profiling the training phase\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile() \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[1;32m----> 7\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Print the profiling results\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prof\u001b[38;5;241m.\u001b[39mkey_averages()\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda_time_total\u001b[39m\u001b[38;5;124m\"\u001b[39m, row_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[1;32mIn[115], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, device, optimizer, criterion)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjusted outputs shape for loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should be (128, 1)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     28\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:3383\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1440) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "run(model, train_dataloader, test_dataloader, device, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([64, 10]) torch.Size([64, 1])\n",
      "torch.Size([40, 10]) torch.Size([40, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.data = torch.randn(size, 10)\n",
    "        self.labels = torch.randn(size, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "simple_dataset = SimpleDataset(1000)\n",
    "simple_dataloader = DataLoader(simple_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "for inputs, targets in simple_dataloader:\n",
    "    print(inputs.shape, targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sine_wave(seq_length, num_samples):\n",
    "    x = np.linspace(0, 100, num_samples)\n",
    "    y = np.sin(x)\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(y) - seq_length):\n",
    "        sequences.append(y[i:i + seq_length])\n",
    "        targets.append(y[i + seq_length])\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    return torch.tensor(sequences, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "seq_length = 10\n",
    "num_samples = 100\n",
    "X, y = generate_sine_wave(seq_length, num_samples)\n",
    "\n",
    "# Reshape to fit LSTM input (seq_len, batch_size, input_size)\n",
    "X = X.unsqueeze(-1)  # Add an extra dimension for input size (which is 1 for univariate time series)\n",
    "y = y.unsqueeze(-1)  # Add an extra dimension for target size\n",
    "\n",
    "# Split into train and test\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.03363672147194544\n",
      "Epoch 2/100, Loss: 0.03290561379657851\n",
      "Epoch 3/100, Loss: 0.032128037263949714\n",
      "Epoch 4/100, Loss: 0.031258573134740196\n",
      "Epoch 5/100, Loss: 0.030249755829572678\n",
      "Epoch 6/100, Loss: 0.0290426653292444\n",
      "Epoch 7/100, Loss: 0.02755696243709988\n",
      "Epoch 8/100, Loss: 0.025670189410448074\n",
      "Epoch 9/100, Loss: 0.023176424619224336\n",
      "Epoch 10/100, Loss: 0.0197226500345601\n",
      "Epoch 11/100, Loss: 0.014812210988667276\n",
      "Epoch 12/100, Loss: 0.008450245174268881\n",
      "Epoch 13/100, Loss: 0.0036447161498169103\n",
      "Epoch 14/100, Loss: 0.004164617094728682\n",
      "Epoch 15/100, Loss: 0.0038826227084630066\n",
      "Epoch 16/100, Loss: 0.002749183422161473\n",
      "Epoch 17/100, Loss: 0.0026872331897417703\n",
      "Epoch 18/100, Loss: 0.0025876302065120805\n",
      "Epoch 19/100, Loss: 0.0022334373659557765\n",
      "Epoch 20/100, Loss: 0.002003047020278043\n",
      "Epoch 21/100, Loss: 0.0018587648972041076\n",
      "Epoch 22/100, Loss: 0.0016525645429889362\n",
      "Epoch 23/100, Loss: 0.00146198986719052\n",
      "Epoch 24/100, Loss: 0.0012976009295218522\n",
      "Epoch 25/100, Loss: 0.0011220317230456406\n",
      "Epoch 26/100, Loss: 0.0009522494704773029\n",
      "Epoch 27/100, Loss: 0.0007958074824677573\n",
      "Epoch 28/100, Loss: 0.0006468675564974546\n",
      "Epoch 29/100, Loss: 0.0005129391405110558\n",
      "Epoch 30/100, Loss: 0.0003959742348848118\n",
      "Epoch 31/100, Loss: 0.0002959148647884528\n",
      "Epoch 32/100, Loss: 0.00021571365586068068\n",
      "Epoch 33/100, Loss: 0.00015521865700268082\n",
      "Epoch 34/100, Loss: 0.00011285957912655754\n",
      "Epoch 35/100, Loss: 8.541677460824657e-05\n",
      "Epoch 36/100, Loss: 6.862614019256498e-05\n",
      "Epoch 37/100, Loss: 5.861363266982759e-05\n",
      "Epoch 38/100, Loss: 5.217481723068179e-05\n",
      "Epoch 39/100, Loss: 4.724427789268601e-05\n",
      "Epoch 40/100, Loss: 4.274317163637736e-05\n",
      "Epoch 41/100, Loss: 3.842224800286608e-05\n",
      "Epoch 42/100, Loss: 3.439376915695094e-05\n",
      "Epoch 43/100, Loss: 3.0782773845001226e-05\n",
      "Epoch 44/100, Loss: 2.7612590404007482e-05\n",
      "Epoch 45/100, Loss: 2.4847342501743697e-05\n",
      "Epoch 46/100, Loss: 2.243578152653451e-05\n",
      "Epoch 47/100, Loss: 2.0316768617097598e-05\n",
      "Epoch 48/100, Loss: 1.8433872456727033e-05\n",
      "Epoch 49/100, Loss: 1.6750981154069046e-05\n",
      "Epoch 50/100, Loss: 1.5248015996702532e-05\n",
      "Epoch 51/100, Loss: 1.3908503913424081e-05\n",
      "Epoch 52/100, Loss: 1.2715134491574847e-05\n",
      "Epoch 53/100, Loss: 1.1652674402284902e-05\n",
      "Epoch 54/100, Loss: 1.070762755261967e-05\n",
      "Epoch 55/100, Loss: 9.866811625316688e-06\n",
      "Epoch 56/100, Loss: 9.117875682325879e-06\n",
      "Epoch 57/100, Loss: 8.450486246955632e-06\n",
      "Epoch 58/100, Loss: 7.85552583693061e-06\n",
      "Epoch 59/100, Loss: 7.324794751184527e-06\n",
      "Epoch 60/100, Loss: 6.850832505733706e-06\n",
      "Epoch 61/100, Loss: 6.427136011350538e-06\n",
      "Epoch 62/100, Loss: 6.0478679289291094e-06\n",
      "Epoch 63/100, Loss: 5.7077515723196684e-06\n",
      "Epoch 64/100, Loss: 5.402156602940522e-06\n",
      "Epoch 65/100, Loss: 5.126978976477403e-06\n",
      "Epoch 66/100, Loss: 4.878526043386147e-06\n",
      "Epoch 67/100, Loss: 4.653633924236702e-06\n",
      "Epoch 68/100, Loss: 4.449413457526437e-06\n",
      "Epoch 69/100, Loss: 4.263412493956922e-06\n",
      "Epoch 70/100, Loss: 4.09338417739491e-06\n",
      "Epoch 71/100, Loss: 3.937409423492176e-06\n",
      "Epoch 72/100, Loss: 3.793808092369646e-06\n",
      "Epoch 73/100, Loss: 3.6610692859539995e-06\n",
      "Epoch 74/100, Loss: 3.537923425432786e-06\n",
      "Epoch 75/100, Loss: 3.423206787071346e-06\n",
      "Epoch 76/100, Loss: 3.315987568283971e-06\n",
      "Epoch 77/100, Loss: 3.2153754242851087e-06\n",
      "Epoch 78/100, Loss: 3.1206244936280805e-06\n",
      "Epoch 79/100, Loss: 3.0311018437916773e-06\n",
      "Epoch 80/100, Loss: 2.946264127482815e-06\n",
      "Epoch 81/100, Loss: 2.865598137052277e-06\n",
      "Epoch 82/100, Loss: 2.7887069917495764e-06\n",
      "Epoch 83/100, Loss: 2.715218468236142e-06\n",
      "Epoch 84/100, Loss: 2.6448290100233862e-06\n",
      "Epoch 85/100, Loss: 2.57727611420301e-06\n",
      "Epoch 86/100, Loss: 2.512286856573256e-06\n",
      "Epoch 87/100, Loss: 2.449704702990453e-06\n",
      "Epoch 88/100, Loss: 2.3893279477407405e-06\n",
      "Epoch 89/100, Loss: 2.331016983387397e-06\n",
      "Epoch 90/100, Loss: 2.2746096040767347e-06\n",
      "Epoch 91/100, Loss: 2.220015112975994e-06\n",
      "Epoch 92/100, Loss: 2.1671271244688087e-06\n",
      "Epoch 93/100, Loss: 2.115854120650814e-06\n",
      "Epoch 94/100, Loss: 2.0661252998858495e-06\n",
      "Epoch 95/100, Loss: 2.017843952570628e-06\n",
      "Epoch 96/100, Loss: 1.9709590585787537e-06\n",
      "Epoch 97/100, Loss: 1.9254345160233142e-06\n",
      "Epoch 98/100, Loss: 1.881185388204661e-06\n",
      "Epoch 99/100, Loss: 1.838169989948963e-06\n",
      "Epoch 100/100, Loss: 1.7963822705900789e-06\n",
      "Test Loss: 3.7043497286504135e-05\n"
     ]
    }
   ],
   "source": [
    "# Define the custom model\n",
    "input_size = 1\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "\n",
    "model = LSTMCustom(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001,weight_decay=1e-4)\n",
    "model = torch.compile(model, backend=\"inductor\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Train in batches\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, _ = model(X_batch)\n",
    "        loss = criterion(outputs[:, -1, :], y_batch)  # Compare the last output with the target\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(X_train)}\")\n",
    "\n",
    "# Test the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs, _ = model(X_test)\n",
    "    test_loss = criterion(test_outputs[:, -1, :], y_test)\n",
    "    print(f\"Test Loss: {test_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
