{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune import CLIReporter, ExperimentAnalysis\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air import session\n",
    "from ray.train import Checkpoint\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r'D:\\School\\ADMU\\4Y\\SEM 1\\MATH 199.11\\Final\\DAILY\\LUZ_Daily_Complete.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "data = data.fillna(0)\n",
    "\n",
    "\n",
    "X = data\n",
    "\n",
    "y = data[['GWAP','LWAP']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.6 * len(X))  # 60% for training\n",
    "val_size = int(0.20 * len(X))   # 20% for validation\n",
    "test_size = len(X) - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "train_data = X[:train_size]\n",
    "train_labels = y[:train_size]\n",
    "\n",
    "val_data = X[train_size:train_size + val_size]\n",
    "val_labels = y[train_size:train_size + val_size]\n",
    "\n",
    "test_data = X[train_size + val_size:]\n",
    "test_labels = y[train_size + val_size:]\n",
    "seq_len=7\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_cols = []\n",
    "boxcox_cols = []\n",
    "yeojohnson_cols = []\n",
    "\n",
    "# Classify columns into MinMax, BoxCox, or YeoJohnson families\n",
    "def classify_features(data):\n",
    "    for column in data.columns:\n",
    "        col_data = data[column]\n",
    "        skewness = col_data.skew()\n",
    "        kurt = col_data.kurtosis()\n",
    "        is_positive = np.all(col_data > 0)\n",
    "\n",
    "        if -1 <= skewness <= 1 and -1 <= kurt <= 1:\n",
    "            minmax_cols.append(column)  # MinMax family\n",
    "        elif is_positive:\n",
    "            boxcox_cols.append(column)  # BoxCox family\n",
    "        else:\n",
    "            yeojohnson_cols.append(column)  # YeoJohnson family\n",
    "\n",
    "classify_features(X)\n",
    "\n",
    "minmax_colsy = []\n",
    "boxcox_colsy = []\n",
    "yeojohnson_colsy = []\n",
    "\n",
    "def classify_features(data):\n",
    "    for column in data.columns:\n",
    "        col_data = data[column]\n",
    "        skewness = col_data.skew()\n",
    "        kurt = col_data.kurtosis()\n",
    "        is_positive = np.all(col_data > 0)\n",
    "\n",
    "        if -1 <= skewness <= 1 and -1 <= kurt <= 1:\n",
    "            minmax_colsy.append(column)  # MinMax family\n",
    "        elif is_positive:\n",
    "            boxcox_colsy.append(column)  # BoxCox family\n",
    "        else:\n",
    "            yeojohnson_colsy.append(column)  # YeoJohnson family\n",
    "\n",
    "classify_features(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.DataFrame(train_data)  # Replace `data` with your actual data\n",
    "val_data_df = pd.DataFrame(val_data)  # Replace `data` with your actual data\n",
    "test_data_df = pd.DataFrame(test_data)  # Replace `data` with your actual data\n",
    "# Test MinMaxScaler independently\n",
    "minmax_test = MinMaxScaler(feature_range=(0, 1))\n",
    "minmaxfit = minmax_test.fit(train_data_df[minmax_cols])\n",
    "train_data_minmax = minmaxfit.transform(train_data_df[minmax_cols])\n",
    "val_data_minmax = minmaxfit.transform(val_data_df[minmax_cols])\n",
    "test_data_minmax = minmaxfit.transform(test_data_df[minmax_cols])\n",
    "\n",
    "# Test Box-Cox + MinMaxScaler independently\n",
    "boxcox_pipeline = Pipeline([\n",
    "    ('boxcox', PowerTransformer(method='box-cox', standardize=False)),\n",
    "    ('minmax', MinMaxScaler(feature_range=(0, 1)))\n",
    "])\n",
    "bc = boxcox_pipeline.fit(train_data_df[boxcox_cols])\n",
    "train_data_bc = bc.transform(train_data_df[boxcox_cols])\n",
    "val_data_bc = bc.transform(val_data_df[boxcox_cols])\n",
    "test_data_bc = bc.transform(test_data_df[boxcox_cols])\n",
    "\n",
    "# Test Yeo-Johnson + MinMaxScaler independently\n",
    "yeojohnson_pipeline = Pipeline([\n",
    "    ('yeojohnson', PowerTransformer(method='yeo-johnson', standardize=False)),\n",
    "    ('minmax', MinMaxScaler(feature_range=(0, 1)))\n",
    "])\n",
    "yj = yeojohnson_pipeline.fit(train_data_df[yeojohnson_cols])\n",
    "train_data_yj = yj.transform(train_data_df[yeojohnson_cols])\n",
    "val_data_yj = yj.transform(val_data_df[yeojohnson_cols])\n",
    "test_data_yj = yj.transform(test_data_df[yeojohnson_cols])\n",
    "\n",
    "train_data_transformed = np.hstack([train_data_minmax, train_data_bc, train_data_yj])\n",
    "val_data_transformed = np.hstack([val_data_minmax, val_data_bc, val_data_yj])\n",
    "test_data_transformed = np.hstack([test_data_minmax, test_data_bc, test_data_yj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed training data (Yeo-Johnson) for column: RAINFALL_Tanay\n",
      "   RAINFALL_Tanay\n",
      "0        0.335037\n",
      "1        0.000000\n",
      "2        0.335037\n",
      "3        0.335037\n",
      "4        0.335037\n",
      "Transformed validation data (Yeo-Johnson) for column: RAINFALL_Tanay\n",
      "   RAINFALL_Tanay\n",
      "0        0.335037\n",
      "1        0.335037\n",
      "2        0.000000\n",
      "3        0.335037\n",
      "4        0.335037\n",
      "Transformed test data (Yeo-Johnson) for column: RAINFALL_Tanay\n",
      "   RAINFALL_Tanay\n",
      "0        0.709829\n",
      "1        0.335037\n",
      "2        0.662765\n",
      "3        0.615077\n",
      "4        0.335037\n"
     ]
    }
   ],
   "source": [
    "column_name = 'RAINFALL_Tanay'\n",
    "\n",
    "yeojohnson_pipeline = Pipeline([\n",
    "    ('yeojohnson', PowerTransformer(method='yeo-johnson', standardize=False)),\n",
    "    ('minmax', MinMaxScaler(feature_range=(0, 1)))\n",
    "])\n",
    "\n",
    "yj = yeojohnson_pipeline.fit(train_data_df[[column_name]])\n",
    "train_tanay_yj = yj.transform(train_data_df[[column_name]])\n",
    "val_tanay_yj = yj.transform(val_data_df[[column_name]])\n",
    "test_tanay_yj = yj.transform(test_data_df[[column_name]])\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "train_tanay_yj_df = pd.DataFrame(train_tanay_yj, columns=[column_name])\n",
    "val_tanay_yj_df = pd.DataFrame(val_tanay_yj, columns=[column_name])\n",
    "test_tanay_yj_df = pd.DataFrame(test_tanay_yj, columns=[column_name])\n",
    "\n",
    "# Print the transformed data for verification\n",
    "print(\"Transformed training data (Yeo-Johnson) for column:\", column_name)\n",
    "print(train_tanay_yj_df.head())\n",
    "\n",
    "print(\"Transformed validation data (Yeo-Johnson) for column:\", column_name)\n",
    "print(val_tanay_yj_df.head())\n",
    "\n",
    "print(\"Transformed test data (Yeo-Johnson) for column:\", column_name)\n",
    "print(test_tanay_yj_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for Training Data:\n",
      " count    438.000000\n",
      "mean       8.080822\n",
      "std       21.176581\n",
      "min       -1.000000\n",
      "25%        0.000000\n",
      "50%        0.050000\n",
      "75%        6.750000\n",
      "max      217.600000\n",
      "Name: RAINFALL_Tanay, dtype: float64\n",
      "\n",
      "Summary Statistics for Validation Data:\n",
      " count    146.000000\n",
      "mean       2.410274\n",
      "std       85.453230\n",
      "min     -999.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%       10.500000\n",
      "max      105.200000\n",
      "Name: RAINFALL_Tanay, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_stats = train_data_df['RAINFALL_Tanay'].describe()\n",
    "val_stats = val_data_df['RAINFALL_Tanay'].describe()\n",
    "\n",
    "print(\"Summary Statistics for Training Data:\\n\", train_stats)\n",
    "print(\"\\nSummary Statistics for Validation Data:\\n\", val_stats)\n",
    "\n",
    "# Function to check for NaN or infinite values\n",
    "def check_for_nan_inf(df):\n",
    "    nan_count = df.isna().sum().sum()\n",
    "    inf_count = np.isinf(df).sum().sum()\n",
    "    return nan_count, inf_count\n",
    "\n",
    "# Check for NaN and Infinite values\n",
    "train_nan, train_inf = check_for_nan_inf(train_data_df)\n",
    "val_nan, val_inf = check_for_nan_inf(val_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for transformed training data (Yeo-Johnson) for column: RAINFALL_Tanay\n",
      "       RAINFALL_Tanay\n",
      "count      438.000000\n",
      "mean         0.469711\n",
      "std          0.241283\n",
      "min          0.000000\n",
      "25%          0.335037\n",
      "50%          0.344686\n",
      "75%          0.676540\n",
      "max          1.000000\n",
      "Summary statistics for transformed validation data (Yeo-Johnson) for column: RAINFALL_Tanay\n",
      "       RAINFALL_Tanay\n",
      "count      146.000000\n",
      "mean     -2639.430655\n",
      "std      31898.231800\n",
      "min    -385427.315508\n",
      "25%          0.335037\n",
      "50%          0.335037\n",
      "75%          0.726893\n",
      "max          0.947618\n",
      "Summary statistics for transformed test data (Yeo-Johnson) for column: RAINFALL_Tanay\n",
      "       RAINFALL_Tanay\n",
      "count      146.000000\n",
      "mean         0.525290\n",
      "std          0.222899\n",
      "min          0.000000\n",
      "25%          0.335037\n",
      "50%          0.512369\n",
      "75%          0.717194\n",
      "max          0.920012\n"
     ]
    }
   ],
   "source": [
    "# Get summary statistics for the transformed training data\n",
    "train_summary_statistics = train_tanay_yj_df.describe()\n",
    "print(\"Summary statistics for transformed training data (Yeo-Johnson) for column:\", column_name)\n",
    "print(train_summary_statistics)\n",
    "\n",
    "# Get summary statistics for the transformed validation data\n",
    "val_summary_statistics = val_tanay_yj_df.describe()\n",
    "print(\"Summary statistics for transformed validation data (Yeo-Johnson) for column:\", column_name)\n",
    "print(val_summary_statistics)\n",
    "\n",
    "# Get summary statistics for the transformed test data\n",
    "test_summary_statistics = test_tanay_yj_df.describe()\n",
    "print(\"Summary statistics for transformed test data (Yeo-Johnson) for column:\", column_name)\n",
    "print(test_summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = pd.DataFrame(train_labels)  # Replace `data` with your actual data\n",
    "val_labels_df = pd.DataFrame(val_labels)  # Replace `data` with your actual data\n",
    "test_labels_df = pd.DataFrame(test_labels)  # Replace `data` with your actual data\n",
    "# Test MinMaxScaler independently\n",
    "\n",
    "\n",
    "# Test Box-Cox + MinMaxScaler independently\n",
    "boxcox_pipeline = Pipeline([\n",
    "    ('boxcox', PowerTransformer(method='box-cox', standardize=False)),\n",
    "    ('minmax', MinMaxScaler(feature_range=(0, 1)))\n",
    "])\n",
    "bcy = boxcox_pipeline.fit(train_data_df[boxcox_colsy])\n",
    "train_labels_bc = bcy.transform(train_data_df[boxcox_colsy])\n",
    "val_labels_bc = bcy.transform(val_data_df[boxcox_colsy])\n",
    "test_labels_bc = bcy.transform(test_data_df[boxcox_colsy])\n",
    "\n",
    "# Test Yeo-Johnson + MinMaxScaler independently\n",
    "\n",
    "\n",
    "train_labels_transformed = np.hstack([train_labels_bc])\n",
    "val_labels_transformed = np.hstack([ val_labels_bc])\n",
    "test_labels_transformed = np.hstack([ test_labels_bc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for each column in the combined validation data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLOW_LUZ</th>\n",
       "      <th>Hourly Demand</th>\n",
       "      <th>TMAX_Cubi Point</th>\n",
       "      <th>TMAX_NAIA</th>\n",
       "      <th>TMIN_NAIA</th>\n",
       "      <th>TMAX_Science Garden</th>\n",
       "      <th>TMAX_San Jose</th>\n",
       "      <th>TMIN_San Jose</th>\n",
       "      <th>TMAX_Tayabas</th>\n",
       "      <th>TMIN_Tayabas</th>\n",
       "      <th>TMAX_CLSU</th>\n",
       "      <th>TMIN_CLSU</th>\n",
       "      <th>TMAX_Ambulong</th>\n",
       "      <th>TMAX_Casiguran</th>\n",
       "      <th>TMIN_Casiguran</th>\n",
       "      <th>TMAX_Clark</th>\n",
       "      <th>TMIN_Clark</th>\n",
       "      <th>TMAX_Calapan</th>\n",
       "      <th>TMIN_Calapan</th>\n",
       "      <th>GWAP</th>\n",
       "      <th>LWAP</th>\n",
       "      <th>TMIN_Cubi Point</th>\n",
       "      <th>TMIN_Science Garden</th>\n",
       "      <th>TMIN_Ambulong</th>\n",
       "      <th>RESERVE_GWAP_Fr</th>\n",
       "      <th>RESERVE_GWAP_Ru</th>\n",
       "      <th>RESERVE_GWAP_Rd</th>\n",
       "      <th>RESERVE_GWAP_Dr</th>\n",
       "      <th>RAINFALL_Cubi Point</th>\n",
       "      <th>RAINFALL_NAIA</th>\n",
       "      <th>RAINFALL_Science Garden</th>\n",
       "      <th>RAINFALL_San Jose</th>\n",
       "      <th>RAINFALL_Tayabas</th>\n",
       "      <th>RAINFALL_CLSU</th>\n",
       "      <th>RAINFALL_Tanay</th>\n",
       "      <th>TMAX_Tanay</th>\n",
       "      <th>TMIN_Tanay</th>\n",
       "      <th>RAINFALL_Ambulong</th>\n",
       "      <th>RAINFALL_Casiguran</th>\n",
       "      <th>RAINFALL_Clark</th>\n",
       "      <th>RAINFALL_Calapan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.468299</td>\n",
       "      <td>0.648252</td>\n",
       "      <td>0.546937</td>\n",
       "      <td>0.590728</td>\n",
       "      <td>0.606996</td>\n",
       "      <td>0.565195</td>\n",
       "      <td>0.580500</td>\n",
       "      <td>0.536403</td>\n",
       "      <td>0.561562</td>\n",
       "      <td>0.552756</td>\n",
       "      <td>0.543888</td>\n",
       "      <td>0.598566</td>\n",
       "      <td>0.538093</td>\n",
       "      <td>0.555466</td>\n",
       "      <td>0.620166</td>\n",
       "      <td>0.609437</td>\n",
       "      <td>0.581394</td>\n",
       "      <td>0.611216</td>\n",
       "      <td>0.518960</td>\n",
       "      <td>0.606551</td>\n",
       "      <td>0.601150</td>\n",
       "      <td>0.492411</td>\n",
       "      <td>0.414022</td>\n",
       "      <td>0.446758</td>\n",
       "      <td>0.465425</td>\n",
       "      <td>0.516107</td>\n",
       "      <td>0.414435</td>\n",
       "      <td>0.377133</td>\n",
       "      <td>0.579450</td>\n",
       "      <td>0.220784</td>\n",
       "      <td>0.436821</td>\n",
       "      <td>0.494787</td>\n",
       "      <td>0.379439</td>\n",
       "      <td>0.568961</td>\n",
       "      <td>0.469711</td>\n",
       "      <td>0.525837</td>\n",
       "      <td>0.536027</td>\n",
       "      <td>0.527558</td>\n",
       "      <td>0.586225</td>\n",
       "      <td>0.491808</td>\n",
       "      <td>0.612988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.203223</td>\n",
       "      <td>0.173484</td>\n",
       "      <td>0.169088</td>\n",
       "      <td>0.173105</td>\n",
       "      <td>0.166796</td>\n",
       "      <td>0.183004</td>\n",
       "      <td>0.169354</td>\n",
       "      <td>0.213272</td>\n",
       "      <td>0.191139</td>\n",
       "      <td>0.205733</td>\n",
       "      <td>0.167631</td>\n",
       "      <td>0.207601</td>\n",
       "      <td>0.186873</td>\n",
       "      <td>0.216838</td>\n",
       "      <td>0.163944</td>\n",
       "      <td>0.155433</td>\n",
       "      <td>0.180081</td>\n",
       "      <td>0.173074</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>0.170671</td>\n",
       "      <td>0.170680</td>\n",
       "      <td>0.182261</td>\n",
       "      <td>0.176314</td>\n",
       "      <td>0.153151</td>\n",
       "      <td>0.228774</td>\n",
       "      <td>0.175465</td>\n",
       "      <td>0.104273</td>\n",
       "      <td>0.243130</td>\n",
       "      <td>0.206565</td>\n",
       "      <td>0.059581</td>\n",
       "      <td>0.269305</td>\n",
       "      <td>0.236016</td>\n",
       "      <td>0.244571</td>\n",
       "      <td>0.206949</td>\n",
       "      <td>0.241283</td>\n",
       "      <td>0.208277</td>\n",
       "      <td>0.203472</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>0.215219</td>\n",
       "      <td>0.234339</td>\n",
       "      <td>0.189873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.324398</td>\n",
       "      <td>0.544614</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.477477</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.434343</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.423554</td>\n",
       "      <td>0.472656</td>\n",
       "      <td>0.414414</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.503704</td>\n",
       "      <td>0.460843</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.489789</td>\n",
       "      <td>0.486298</td>\n",
       "      <td>0.388664</td>\n",
       "      <td>0.306930</td>\n",
       "      <td>0.353685</td>\n",
       "      <td>0.304974</td>\n",
       "      <td>0.409046</td>\n",
       "      <td>0.357732</td>\n",
       "      <td>0.198878</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.215057</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.212722</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.335037</td>\n",
       "      <td>0.365696</td>\n",
       "      <td>0.383614</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.662646</td>\n",
       "      <td>0.549550</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.553719</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.558559</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.614815</td>\n",
       "      <td>0.608434</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.579811</td>\n",
       "      <td>0.495312</td>\n",
       "      <td>0.430661</td>\n",
       "      <td>0.454780</td>\n",
       "      <td>0.476836</td>\n",
       "      <td>0.498505</td>\n",
       "      <td>0.424483</td>\n",
       "      <td>0.405656</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.215057</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.368485</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.344686</td>\n",
       "      <td>0.517250</td>\n",
       "      <td>0.574931</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.547165</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.582796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.619960</td>\n",
       "      <td>0.777584</td>\n",
       "      <td>0.639640</td>\n",
       "      <td>0.700450</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.709459</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.718519</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.747126</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.746433</td>\n",
       "      <td>0.738104</td>\n",
       "      <td>0.618592</td>\n",
       "      <td>0.534411</td>\n",
       "      <td>0.539245</td>\n",
       "      <td>0.620082</td>\n",
       "      <td>0.640535</td>\n",
       "      <td>0.467362</td>\n",
       "      <td>0.568597</td>\n",
       "      <td>0.722833</td>\n",
       "      <td>0.218638</td>\n",
       "      <td>0.649551</td>\n",
       "      <td>0.637988</td>\n",
       "      <td>0.564978</td>\n",
       "      <td>0.691765</td>\n",
       "      <td>0.676540</td>\n",
       "      <td>0.685466</td>\n",
       "      <td>0.667813</td>\n",
       "      <td>0.697769</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.621451</td>\n",
       "      <td>0.786879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FLOW_LUZ  Hourly Demand  TMAX_Cubi Point   TMAX_NAIA   TMIN_NAIA  \\\n",
       "count  438.000000     438.000000       438.000000  438.000000  438.000000   \n",
       "mean     0.468299       0.648252         0.546937    0.590728    0.606996   \n",
       "std      0.203223       0.173484         0.169088    0.173105    0.166796   \n",
       "min      0.000000       0.000000         0.000000    0.000000    0.000000   \n",
       "25%      0.324398       0.544614         0.441441    0.477477    0.493827   \n",
       "50%      0.470000       0.662646         0.549550    0.594595    0.604938   \n",
       "75%      0.619960       0.777584         0.639640    0.700450    0.716049   \n",
       "max      1.000000       1.000000         1.000000    1.000000    1.000000   \n",
       "\n",
       "       TMAX_Science Garden  TMAX_San Jose  TMIN_San Jose  TMAX_Tayabas  \\\n",
       "count           438.000000     438.000000     438.000000    438.000000   \n",
       "mean              0.565195       0.580500       0.536403      0.561562   \n",
       "std               0.183004       0.169354       0.213272      0.191139   \n",
       "min               0.000000       0.000000       0.000000      0.000000   \n",
       "25%               0.434343       0.482143       0.361111      0.432432   \n",
       "50%               0.575758       0.580357       0.472222      0.576577   \n",
       "75%               0.696970       0.714286       0.750000      0.709459   \n",
       "max               1.000000       1.000000       1.000000      1.000000   \n",
       "\n",
       "       TMIN_Tayabas   TMAX_CLSU   TMIN_CLSU  TMAX_Ambulong  TMAX_Casiguran  \\\n",
       "count    438.000000  438.000000  438.000000     438.000000      438.000000   \n",
       "mean       0.552756    0.543888    0.598566       0.538093        0.555466   \n",
       "std        0.205733    0.167631    0.207601       0.186873        0.216838   \n",
       "min        0.000000    0.000000    0.000000       0.000000        0.000000   \n",
       "25%        0.428571    0.423554    0.472656       0.414414        0.392157   \n",
       "50%        0.607143    0.553719    0.625000       0.558559        0.588235   \n",
       "75%        0.696429    0.652893    0.765625       0.666667        0.725490   \n",
       "max        1.000000    1.000000    1.000000       1.000000        1.000000   \n",
       "\n",
       "       TMIN_Casiguran  TMAX_Clark  TMIN_Clark  TMAX_Calapan  TMIN_Calapan  \\\n",
       "count      438.000000  438.000000  438.000000    438.000000    438.000000   \n",
       "mean         0.620166    0.609437    0.581394      0.611216      0.518960   \n",
       "std          0.163944    0.155433    0.180081      0.173074      0.178700   \n",
       "min          0.000000    0.000000    0.000000      0.000000      0.000000   \n",
       "25%          0.510204    0.503704    0.460843      0.517241      0.391304   \n",
       "50%          0.642857    0.614815    0.608434      0.632184      0.521739   \n",
       "75%          0.755102    0.718519    0.710843      0.747126      0.630435   \n",
       "max          1.000000    1.000000    1.000000      1.000000      1.000000   \n",
       "\n",
       "             GWAP        LWAP  TMIN_Cubi Point  TMIN_Science Garden  \\\n",
       "count  438.000000  438.000000       438.000000           438.000000   \n",
       "mean     0.606551    0.601150         0.492411             0.414022   \n",
       "std      0.170671    0.170680         0.182261             0.176314   \n",
       "min      0.000000    0.000000         0.000000             0.000000   \n",
       "25%      0.489789    0.486298         0.388664             0.306930   \n",
       "50%      0.584372    0.579811         0.495312             0.430661   \n",
       "75%      0.746433    0.738104         0.618592             0.534411   \n",
       "max      1.000000    1.000000         1.000000             1.000000   \n",
       "\n",
       "       TMIN_Ambulong  RESERVE_GWAP_Fr  RESERVE_GWAP_Ru  RESERVE_GWAP_Rd  \\\n",
       "count     438.000000       438.000000       438.000000       438.000000   \n",
       "mean        0.446758         0.465425         0.516107         0.414435   \n",
       "std         0.153151         0.228774         0.175465         0.104273   \n",
       "min         0.000000         0.000000         0.000000         0.000000   \n",
       "25%         0.353685         0.304974         0.409046         0.357732   \n",
       "50%         0.454780         0.476836         0.498505         0.424483   \n",
       "75%         0.539245         0.620082         0.640535         0.467362   \n",
       "max         1.000000         1.000000         1.000000         1.000000   \n",
       "\n",
       "       RESERVE_GWAP_Dr  RAINFALL_Cubi Point  RAINFALL_NAIA  \\\n",
       "count       438.000000           438.000000     438.000000   \n",
       "mean          0.377133             0.579450       0.220784   \n",
       "std           0.243130             0.206565       0.059581   \n",
       "min           0.000000             0.000000       0.000000   \n",
       "25%           0.198878             0.493149       0.215057   \n",
       "50%           0.405656             0.493149       0.215057   \n",
       "75%           0.568597             0.722833       0.218638   \n",
       "max           1.000000             1.000000       1.000000   \n",
       "\n",
       "       RAINFALL_Science Garden  RAINFALL_San Jose  RAINFALL_Tayabas  \\\n",
       "count               438.000000         438.000000        438.000000   \n",
       "mean                  0.436821           0.494787          0.379439   \n",
       "std                   0.269305           0.236016          0.244571   \n",
       "min                   0.000000           0.000000          0.000000   \n",
       "25%                   0.347275           0.417791          0.212722   \n",
       "50%                   0.347275           0.417791          0.368485   \n",
       "75%                   0.649551           0.637988          0.564978   \n",
       "max                   1.000000           1.000000          1.000000   \n",
       "\n",
       "       RAINFALL_CLSU  RAINFALL_Tanay  TMAX_Tanay  TMIN_Tanay  \\\n",
       "count     438.000000      438.000000  438.000000  438.000000   \n",
       "mean        0.568961        0.469711    0.525837    0.536027   \n",
       "std         0.206949        0.241283    0.208277    0.203472   \n",
       "min         0.000000        0.000000    0.000000    0.000000   \n",
       "25%         0.493466        0.335037    0.365696    0.383614   \n",
       "50%         0.493466        0.344686    0.517250    0.574931   \n",
       "75%         0.691765        0.676540    0.685466    0.667813   \n",
       "max         1.000000        1.000000    1.000000    1.000000   \n",
       "\n",
       "       RAINFALL_Ambulong  RAINFALL_Casiguran  RAINFALL_Clark  RAINFALL_Calapan  \n",
       "count         438.000000          438.000000      438.000000        438.000000  \n",
       "mean            0.527558            0.586225        0.491808          0.612988  \n",
       "std             0.214956            0.215219        0.234339          0.189873  \n",
       "min             0.000000            0.000000        0.000000          0.000000  \n",
       "25%             0.413668            0.383035        0.422884          0.435920  \n",
       "50%             0.413668            0.547165        0.422884          0.582796  \n",
       "75%             0.697769            0.789462        0.621451          0.786879  \n",
       "max             1.000000            1.000000        1.000000          1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLOW_LUZ</th>\n",
       "      <th>Hourly Demand</th>\n",
       "      <th>TMAX_Cubi Point</th>\n",
       "      <th>TMAX_NAIA</th>\n",
       "      <th>TMIN_NAIA</th>\n",
       "      <th>TMAX_Science Garden</th>\n",
       "      <th>TMAX_San Jose</th>\n",
       "      <th>TMIN_San Jose</th>\n",
       "      <th>TMAX_Tayabas</th>\n",
       "      <th>TMIN_Tayabas</th>\n",
       "      <th>TMAX_CLSU</th>\n",
       "      <th>TMIN_CLSU</th>\n",
       "      <th>TMAX_Ambulong</th>\n",
       "      <th>TMAX_Casiguran</th>\n",
       "      <th>TMIN_Casiguran</th>\n",
       "      <th>TMAX_Clark</th>\n",
       "      <th>TMIN_Clark</th>\n",
       "      <th>TMAX_Calapan</th>\n",
       "      <th>TMIN_Calapan</th>\n",
       "      <th>GWAP</th>\n",
       "      <th>LWAP</th>\n",
       "      <th>TMIN_Cubi Point</th>\n",
       "      <th>TMIN_Science Garden</th>\n",
       "      <th>TMIN_Ambulong</th>\n",
       "      <th>RESERVE_GWAP_Fr</th>\n",
       "      <th>RESERVE_GWAP_Ru</th>\n",
       "      <th>RESERVE_GWAP_Rd</th>\n",
       "      <th>RESERVE_GWAP_Dr</th>\n",
       "      <th>RAINFALL_Cubi Point</th>\n",
       "      <th>RAINFALL_NAIA</th>\n",
       "      <th>RAINFALL_Science Garden</th>\n",
       "      <th>RAINFALL_San Jose</th>\n",
       "      <th>RAINFALL_Tayabas</th>\n",
       "      <th>RAINFALL_CLSU</th>\n",
       "      <th>RAINFALL_Tanay</th>\n",
       "      <th>TMAX_Tanay</th>\n",
       "      <th>TMIN_Tanay</th>\n",
       "      <th>RAINFALL_Ambulong</th>\n",
       "      <th>RAINFALL_Casiguran</th>\n",
       "      <th>RAINFALL_Clark</th>\n",
       "      <th>RAINFALL_Calapan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.462319</td>\n",
       "      <td>0.841316</td>\n",
       "      <td>0.600518</td>\n",
       "      <td>0.698815</td>\n",
       "      <td>0.725351</td>\n",
       "      <td>0.679120</td>\n",
       "      <td>0.575893</td>\n",
       "      <td>0.827182</td>\n",
       "      <td>0.735222</td>\n",
       "      <td>0.783268</td>\n",
       "      <td>0.683573</td>\n",
       "      <td>0.758455</td>\n",
       "      <td>0.683019</td>\n",
       "      <td>0.755909</td>\n",
       "      <td>0.784806</td>\n",
       "      <td>0.715373</td>\n",
       "      <td>0.706057</td>\n",
       "      <td>0.773658</td>\n",
       "      <td>0.866885</td>\n",
       "      <td>0.547668</td>\n",
       "      <td>0.547750</td>\n",
       "      <td>0.653095</td>\n",
       "      <td>0.564982</td>\n",
       "      <td>0.580869</td>\n",
       "      <td>0.306686</td>\n",
       "      <td>0.396148</td>\n",
       "      <td>-0.019063</td>\n",
       "      <td>0.158629</td>\n",
       "      <td>0.626125</td>\n",
       "      <td>0.235777</td>\n",
       "      <td>0.510468</td>\n",
       "      <td>0.534797</td>\n",
       "      <td>0.324867</td>\n",
       "      <td>0.629076</td>\n",
       "      <td>-2639.430655</td>\n",
       "      <td>0.670971</td>\n",
       "      <td>0.666716</td>\n",
       "      <td>0.549820</td>\n",
       "      <td>0.509254</td>\n",
       "      <td>0.542504</td>\n",
       "      <td>0.551526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.185224</td>\n",
       "      <td>0.164575</td>\n",
       "      <td>0.191032</td>\n",
       "      <td>0.169632</td>\n",
       "      <td>0.165854</td>\n",
       "      <td>0.186608</td>\n",
       "      <td>0.180428</td>\n",
       "      <td>0.112834</td>\n",
       "      <td>0.136298</td>\n",
       "      <td>0.158137</td>\n",
       "      <td>0.165557</td>\n",
       "      <td>0.179922</td>\n",
       "      <td>0.167520</td>\n",
       "      <td>0.141552</td>\n",
       "      <td>0.115302</td>\n",
       "      <td>0.176049</td>\n",
       "      <td>0.148321</td>\n",
       "      <td>0.140960</td>\n",
       "      <td>0.188844</td>\n",
       "      <td>0.170823</td>\n",
       "      <td>0.174764</td>\n",
       "      <td>0.224561</td>\n",
       "      <td>0.221538</td>\n",
       "      <td>0.240512</td>\n",
       "      <td>0.238990</td>\n",
       "      <td>0.201831</td>\n",
       "      <td>0.138439</td>\n",
       "      <td>0.207967</td>\n",
       "      <td>0.236831</td>\n",
       "      <td>0.049403</td>\n",
       "      <td>0.271226</td>\n",
       "      <td>0.222858</td>\n",
       "      <td>0.190269</td>\n",
       "      <td>0.218323</td>\n",
       "      <td>31898.231800</td>\n",
       "      <td>0.246484</td>\n",
       "      <td>0.189472</td>\n",
       "      <td>0.220948</td>\n",
       "      <td>0.205980</td>\n",
       "      <td>0.268478</td>\n",
       "      <td>0.199414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.075273</td>\n",
       "      <td>0.286574</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>0.144144</td>\n",
       "      <td>0.320988</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.198198</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.144144</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.204948</td>\n",
       "      <td>0.193783</td>\n",
       "      <td>0.168933</td>\n",
       "      <td>0.090310</td>\n",
       "      <td>0.018486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>-0.062443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-385427.315508</td>\n",
       "      <td>-1.201420</td>\n",
       "      <td>-0.213210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.347785</td>\n",
       "      <td>0.749094</td>\n",
       "      <td>0.477477</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.608025</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.623874</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.617470</td>\n",
       "      <td>0.701149</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.420445</td>\n",
       "      <td>0.423292</td>\n",
       "      <td>0.533064</td>\n",
       "      <td>0.430661</td>\n",
       "      <td>0.470994</td>\n",
       "      <td>0.087748</td>\n",
       "      <td>0.238427</td>\n",
       "      <td>-0.062443</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.215057</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.212722</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.335037</td>\n",
       "      <td>0.585625</td>\n",
       "      <td>0.574931</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.434647</td>\n",
       "      <td>0.869941</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.711712</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.584821</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.719008</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.707207</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.533827</td>\n",
       "      <td>0.542242</td>\n",
       "      <td>0.637676</td>\n",
       "      <td>0.534411</td>\n",
       "      <td>0.557184</td>\n",
       "      <td>0.301316</td>\n",
       "      <td>0.394328</td>\n",
       "      <td>-0.062443</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.215057</td>\n",
       "      <td>0.385298</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.270214</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.335037</td>\n",
       "      <td>0.715910</td>\n",
       "      <td>0.667813</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.602658</td>\n",
       "      <td>0.974261</td>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.835586</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.785354</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.879630</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.808559</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660125</td>\n",
       "      <td>0.660893</td>\n",
       "      <td>0.760517</td>\n",
       "      <td>0.691822</td>\n",
       "      <td>0.667565</td>\n",
       "      <td>0.483357</td>\n",
       "      <td>0.527860</td>\n",
       "      <td>-0.062424</td>\n",
       "      <td>0.322743</td>\n",
       "      <td>0.866118</td>\n",
       "      <td>0.226575</td>\n",
       "      <td>0.760878</td>\n",
       "      <td>0.687006</td>\n",
       "      <td>0.440897</td>\n",
       "      <td>0.836375</td>\n",
       "      <td>0.726893</td>\n",
       "      <td>0.837323</td>\n",
       "      <td>0.790590</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.684436</td>\n",
       "      <td>0.797450</td>\n",
       "      <td>0.725080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.005873</td>\n",
       "      <td>1.095787</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>1.045045</td>\n",
       "      <td>1.098765</td>\n",
       "      <td>1.020202</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>1.092593</td>\n",
       "      <td>1.027027</td>\n",
       "      <td>1.321429</td>\n",
       "      <td>0.983471</td>\n",
       "      <td>1.109375</td>\n",
       "      <td>1.018018</td>\n",
       "      <td>1.127451</td>\n",
       "      <td>1.204082</td>\n",
       "      <td>1.007407</td>\n",
       "      <td>1.072289</td>\n",
       "      <td>1.045977</td>\n",
       "      <td>1.217391</td>\n",
       "      <td>0.872305</td>\n",
       "      <td>0.889567</td>\n",
       "      <td>1.874994</td>\n",
       "      <td>1.276220</td>\n",
       "      <td>1.493443</td>\n",
       "      <td>0.859863</td>\n",
       "      <td>0.796214</td>\n",
       "      <td>0.591384</td>\n",
       "      <td>0.732025</td>\n",
       "      <td>1.001455</td>\n",
       "      <td>0.521779</td>\n",
       "      <td>1.004991</td>\n",
       "      <td>0.994911</td>\n",
       "      <td>0.807591</td>\n",
       "      <td>0.991809</td>\n",
       "      <td>0.947618</td>\n",
       "      <td>1.021759</td>\n",
       "      <td>1.103599</td>\n",
       "      <td>0.920229</td>\n",
       "      <td>0.950460</td>\n",
       "      <td>0.993812</td>\n",
       "      <td>0.978044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FLOW_LUZ  Hourly Demand  TMAX_Cubi Point   TMAX_NAIA   TMIN_NAIA  \\\n",
       "count  146.000000     146.000000       146.000000  146.000000  146.000000   \n",
       "mean     0.462319       0.841316         0.600518    0.698815    0.725351   \n",
       "std      0.185224       0.164575         0.191032    0.169632    0.165854   \n",
       "min      0.075273       0.286574         0.126126    0.144144    0.320988   \n",
       "25%      0.347785       0.749094         0.477477    0.594595    0.608025   \n",
       "50%      0.434647       0.869941         0.594595    0.711712    0.740741   \n",
       "75%      0.602658       0.974261         0.736486    0.835586    0.851852   \n",
       "max      1.005873       1.095787         0.981982    1.045045    1.098765   \n",
       "\n",
       "       TMAX_Science Garden  TMAX_San Jose  TMIN_San Jose  TMAX_Tayabas  \\\n",
       "count           146.000000     146.000000     146.000000    146.000000   \n",
       "mean              0.679120       0.575893       0.827182      0.735222   \n",
       "std               0.186608       0.180428       0.112834      0.136298   \n",
       "min               0.040404       0.062500       0.444444      0.198198   \n",
       "25%               0.595960       0.464286       0.768519      0.648649   \n",
       "50%               0.696970       0.584821       0.819444      0.756757   \n",
       "75%               0.785354       0.687500       0.879630      0.828829   \n",
       "max               1.020202       0.982143       1.092593      1.027027   \n",
       "\n",
       "       TMIN_Tayabas   TMAX_CLSU   TMIN_CLSU  TMAX_Ambulong  TMAX_Casiguran  \\\n",
       "count    146.000000  146.000000  146.000000     146.000000      146.000000   \n",
       "mean       0.783268    0.683573    0.758455       0.683019        0.755909   \n",
       "std        0.158137    0.165557    0.179922       0.167520        0.141552   \n",
       "min        0.339286    0.198347    0.062500       0.144144        0.431373   \n",
       "25%        0.696429    0.595041    0.687500       0.623874        0.666667   \n",
       "50%        0.785714    0.719008    0.765625       0.707207        0.784314   \n",
       "75%        0.875000    0.793388    0.859375       0.808559        0.882353   \n",
       "max        1.321429    0.983471    1.109375       1.018018        1.127451   \n",
       "\n",
       "       TMIN_Casiguran  TMAX_Clark  TMIN_Clark  TMAX_Calapan  TMIN_Calapan  \\\n",
       "count      146.000000  146.000000  146.000000    146.000000    146.000000   \n",
       "mean         0.784806    0.715373    0.706057      0.773658      0.866885   \n",
       "std          0.115302    0.176049    0.148321      0.140960      0.188844   \n",
       "min          0.306122    0.133333    0.277108      0.195402      0.173913   \n",
       "25%          0.714286    0.666667    0.617470      0.701149      0.782609   \n",
       "50%          0.795918    0.766667    0.710843      0.804598      0.891304   \n",
       "75%          0.852041    0.837037    0.807229      0.862069      1.000000   \n",
       "max          1.204082    1.007407    1.072289      1.045977      1.217391   \n",
       "\n",
       "             GWAP        LWAP  TMIN_Cubi Point  TMIN_Science Garden  \\\n",
       "count  146.000000  146.000000       146.000000           146.000000   \n",
       "mean     0.547668    0.547750         0.653095             0.564982   \n",
       "std      0.170823    0.174764         0.224561             0.221538   \n",
       "min      0.204948    0.193783         0.168933             0.090310   \n",
       "25%      0.420445    0.423292         0.533064             0.430661   \n",
       "50%      0.533827    0.542242         0.637676             0.534411   \n",
       "75%      0.660125    0.660893         0.760517             0.691822   \n",
       "max      0.872305    0.889567         1.874994             1.276220   \n",
       "\n",
       "       TMIN_Ambulong  RESERVE_GWAP_Fr  RESERVE_GWAP_Ru  RESERVE_GWAP_Rd  \\\n",
       "count     146.000000       146.000000       146.000000       146.000000   \n",
       "mean        0.580869         0.306686         0.396148        -0.019063   \n",
       "std         0.240512         0.238990         0.201831         0.138439   \n",
       "min         0.018486         0.000000         0.014423        -0.062443   \n",
       "25%         0.470994         0.087748         0.238427        -0.062443   \n",
       "50%         0.557184         0.301316         0.394328        -0.062443   \n",
       "75%         0.667565         0.483357         0.527860        -0.062424   \n",
       "max         1.493443         0.859863         0.796214         0.591384   \n",
       "\n",
       "       RESERVE_GWAP_Dr  RAINFALL_Cubi Point  RAINFALL_NAIA  \\\n",
       "count       146.000000           146.000000     146.000000   \n",
       "mean          0.158629             0.626125       0.235777   \n",
       "std           0.207967             0.236831       0.049403   \n",
       "min           0.000000             0.000000       0.213983   \n",
       "25%           0.000022             0.493149       0.215057   \n",
       "50%           0.000030             0.493149       0.215057   \n",
       "75%           0.322743             0.866118       0.226575   \n",
       "max           0.732025             1.001455       0.521779   \n",
       "\n",
       "       RAINFALL_Science Garden  RAINFALL_San Jose  RAINFALL_Tayabas  \\\n",
       "count               146.000000         146.000000        146.000000   \n",
       "mean                  0.510468           0.534797          0.324867   \n",
       "std                   0.271226           0.222858          0.190269   \n",
       "min                   0.000000           0.000000          0.000000   \n",
       "25%                   0.347275           0.417791          0.212722   \n",
       "50%                   0.385298           0.417791          0.270214   \n",
       "75%                   0.760878           0.687006          0.440897   \n",
       "max                   1.004991           0.994911          0.807591   \n",
       "\n",
       "       RAINFALL_CLSU  RAINFALL_Tanay  TMAX_Tanay  TMIN_Tanay  \\\n",
       "count     146.000000      146.000000  146.000000  146.000000   \n",
       "mean        0.629076    -2639.430655    0.670971    0.666716   \n",
       "std         0.218323    31898.231800    0.246484    0.189472   \n",
       "min         0.000000  -385427.315508   -1.201420   -0.213210   \n",
       "25%         0.493466        0.335037    0.585625    0.574931   \n",
       "50%         0.493466        0.335037    0.715910    0.667813   \n",
       "75%         0.836375        0.726893    0.837323    0.790590   \n",
       "max         0.991809        0.947618    1.021759    1.103599   \n",
       "\n",
       "       RAINFALL_Ambulong  RAINFALL_Casiguran  RAINFALL_Clark  RAINFALL_Calapan  \n",
       "count         146.000000          146.000000      146.000000        146.000000  \n",
       "mean            0.549820            0.509254        0.542504          0.551526  \n",
       "std             0.220948            0.205980        0.268478          0.199414  \n",
       "min             0.000000            0.000000        0.000000          0.000000  \n",
       "25%             0.413668            0.383035        0.422884          0.435920  \n",
       "50%             0.413668            0.383035        0.422884          0.435920  \n",
       "75%             0.768000            0.684436        0.797450          0.725080  \n",
       "max             0.920229            0.950460        0.993812          0.978044  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLOW_LUZ</th>\n",
       "      <th>Hourly Demand</th>\n",
       "      <th>TMAX_Cubi Point</th>\n",
       "      <th>TMAX_NAIA</th>\n",
       "      <th>TMIN_NAIA</th>\n",
       "      <th>TMAX_Science Garden</th>\n",
       "      <th>TMAX_San Jose</th>\n",
       "      <th>TMIN_San Jose</th>\n",
       "      <th>TMAX_Tayabas</th>\n",
       "      <th>TMIN_Tayabas</th>\n",
       "      <th>TMAX_CLSU</th>\n",
       "      <th>TMIN_CLSU</th>\n",
       "      <th>TMAX_Ambulong</th>\n",
       "      <th>TMAX_Casiguran</th>\n",
       "      <th>TMIN_Casiguran</th>\n",
       "      <th>TMAX_Clark</th>\n",
       "      <th>TMIN_Clark</th>\n",
       "      <th>TMAX_Calapan</th>\n",
       "      <th>TMIN_Calapan</th>\n",
       "      <th>GWAP</th>\n",
       "      <th>LWAP</th>\n",
       "      <th>TMIN_Cubi Point</th>\n",
       "      <th>TMIN_Science Garden</th>\n",
       "      <th>TMIN_Ambulong</th>\n",
       "      <th>RESERVE_GWAP_Fr</th>\n",
       "      <th>RESERVE_GWAP_Ru</th>\n",
       "      <th>RESERVE_GWAP_Rd</th>\n",
       "      <th>RESERVE_GWAP_Dr</th>\n",
       "      <th>RAINFALL_Cubi Point</th>\n",
       "      <th>RAINFALL_NAIA</th>\n",
       "      <th>RAINFALL_Science Garden</th>\n",
       "      <th>RAINFALL_San Jose</th>\n",
       "      <th>RAINFALL_Tayabas</th>\n",
       "      <th>RAINFALL_CLSU</th>\n",
       "      <th>RAINFALL_Tanay</th>\n",
       "      <th>TMAX_Tanay</th>\n",
       "      <th>TMIN_Tanay</th>\n",
       "      <th>RAINFALL_Ambulong</th>\n",
       "      <th>RAINFALL_Casiguran</th>\n",
       "      <th>RAINFALL_Clark</th>\n",
       "      <th>RAINFALL_Calapan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.557478</td>\n",
       "      <td>0.790429</td>\n",
       "      <td>0.538072</td>\n",
       "      <td>0.593546</td>\n",
       "      <td>0.685016</td>\n",
       "      <td>0.543171</td>\n",
       "      <td>0.560176</td>\n",
       "      <td>0.810185</td>\n",
       "      <td>0.658213</td>\n",
       "      <td>0.711840</td>\n",
       "      <td>0.639251</td>\n",
       "      <td>0.755886</td>\n",
       "      <td>0.563618</td>\n",
       "      <td>0.671501</td>\n",
       "      <td>0.736791</td>\n",
       "      <td>0.604718</td>\n",
       "      <td>0.642680</td>\n",
       "      <td>0.694221</td>\n",
       "      <td>0.822111</td>\n",
       "      <td>0.425797</td>\n",
       "      <td>0.413879</td>\n",
       "      <td>0.605789</td>\n",
       "      <td>0.490719</td>\n",
       "      <td>0.572939</td>\n",
       "      <td>0.248807</td>\n",
       "      <td>0.509619</td>\n",
       "      <td>0.020389</td>\n",
       "      <td>0.126388</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.227546</td>\n",
       "      <td>0.484212</td>\n",
       "      <td>0.512489</td>\n",
       "      <td>0.409320</td>\n",
       "      <td>0.574505</td>\n",
       "      <td>0.525290</td>\n",
       "      <td>0.560115</td>\n",
       "      <td>0.590763</td>\n",
       "      <td>0.569485</td>\n",
       "      <td>0.595334</td>\n",
       "      <td>0.482560</td>\n",
       "      <td>0.599255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.181921</td>\n",
       "      <td>0.150292</td>\n",
       "      <td>0.142749</td>\n",
       "      <td>0.129288</td>\n",
       "      <td>0.125611</td>\n",
       "      <td>0.164609</td>\n",
       "      <td>0.171825</td>\n",
       "      <td>0.092766</td>\n",
       "      <td>0.184081</td>\n",
       "      <td>0.135763</td>\n",
       "      <td>0.136037</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.149362</td>\n",
       "      <td>0.188913</td>\n",
       "      <td>0.120253</td>\n",
       "      <td>0.104543</td>\n",
       "      <td>0.098211</td>\n",
       "      <td>0.172207</td>\n",
       "      <td>0.145781</td>\n",
       "      <td>0.129339</td>\n",
       "      <td>0.128884</td>\n",
       "      <td>0.141149</td>\n",
       "      <td>0.172058</td>\n",
       "      <td>0.159695</td>\n",
       "      <td>0.155736</td>\n",
       "      <td>0.155523</td>\n",
       "      <td>0.065825</td>\n",
       "      <td>0.162759</td>\n",
       "      <td>0.238283</td>\n",
       "      <td>0.031679</td>\n",
       "      <td>0.289652</td>\n",
       "      <td>0.260219</td>\n",
       "      <td>0.247500</td>\n",
       "      <td>0.192706</td>\n",
       "      <td>0.222899</td>\n",
       "      <td>0.180478</td>\n",
       "      <td>0.153060</td>\n",
       "      <td>0.170946</td>\n",
       "      <td>0.221186</td>\n",
       "      <td>0.259388</td>\n",
       "      <td>0.180647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.030434</td>\n",
       "      <td>0.274650</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.283951</td>\n",
       "      <td>-0.010101</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.198198</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.274074</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.126463</td>\n",
       "      <td>0.117123</td>\n",
       "      <td>0.284784</td>\n",
       "      <td>0.143431</td>\n",
       "      <td>0.248106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.062443</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168248</td>\n",
       "      <td>0.278156</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.438704</td>\n",
       "      <td>0.702188</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.466518</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.531532</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.450450</td>\n",
       "      <td>0.553922</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.548148</td>\n",
       "      <td>0.581325</td>\n",
       "      <td>0.563218</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.331399</td>\n",
       "      <td>0.318027</td>\n",
       "      <td>0.495312</td>\n",
       "      <td>0.377928</td>\n",
       "      <td>0.470994</td>\n",
       "      <td>0.140552</td>\n",
       "      <td>0.449355</td>\n",
       "      <td>-0.051848</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.215057</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.212722</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.335037</td>\n",
       "      <td>0.433551</td>\n",
       "      <td>0.490038</td>\n",
       "      <td>0.413668</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.435920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.558182</td>\n",
       "      <td>0.824704</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.679012</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.413897</td>\n",
       "      <td>0.399998</td>\n",
       "      <td>0.609240</td>\n",
       "      <td>0.473266</td>\n",
       "      <td>0.557184</td>\n",
       "      <td>0.248140</td>\n",
       "      <td>0.554680</td>\n",
       "      <td>0.021794</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.465016</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.403740</td>\n",
       "      <td>0.493466</td>\n",
       "      <td>0.512369</td>\n",
       "      <td>0.565962</td>\n",
       "      <td>0.574931</td>\n",
       "      <td>0.503364</td>\n",
       "      <td>0.575704</td>\n",
       "      <td>0.422884</td>\n",
       "      <td>0.514458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.693362</td>\n",
       "      <td>0.898801</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.787037</td>\n",
       "      <td>0.664141</td>\n",
       "      <td>0.707589</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.682432</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.506575</td>\n",
       "      <td>0.495505</td>\n",
       "      <td>0.717940</td>\n",
       "      <td>0.583711</td>\n",
       "      <td>0.652418</td>\n",
       "      <td>0.357595</td>\n",
       "      <td>0.605170</td>\n",
       "      <td>0.065220</td>\n",
       "      <td>0.286576</td>\n",
       "      <td>0.760694</td>\n",
       "      <td>0.220537</td>\n",
       "      <td>0.710276</td>\n",
       "      <td>0.729482</td>\n",
       "      <td>0.632636</td>\n",
       "      <td>0.735342</td>\n",
       "      <td>0.717194</td>\n",
       "      <td>0.693058</td>\n",
       "      <td>0.667813</td>\n",
       "      <td>0.719330</td>\n",
       "      <td>0.812450</td>\n",
       "      <td>0.643084</td>\n",
       "      <td>0.769946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997687</td>\n",
       "      <td>1.025971</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.855856</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.074074</td>\n",
       "      <td>1.117117</td>\n",
       "      <td>1.107143</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909910</td>\n",
       "      <td>1.029412</td>\n",
       "      <td>1.061224</td>\n",
       "      <td>0.785185</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>1.034483</td>\n",
       "      <td>1.173913</td>\n",
       "      <td>0.871075</td>\n",
       "      <td>0.863151</td>\n",
       "      <td>0.973942</td>\n",
       "      <td>1.051195</td>\n",
       "      <td>1.198596</td>\n",
       "      <td>0.698703</td>\n",
       "      <td>0.853544</td>\n",
       "      <td>0.178230</td>\n",
       "      <td>0.614714</td>\n",
       "      <td>0.993971</td>\n",
       "      <td>0.414274</td>\n",
       "      <td>1.019252</td>\n",
       "      <td>0.988346</td>\n",
       "      <td>0.884560</td>\n",
       "      <td>0.953031</td>\n",
       "      <td>0.920012</td>\n",
       "      <td>0.946032</td>\n",
       "      <td>1.523148</td>\n",
       "      <td>0.926266</td>\n",
       "      <td>0.986848</td>\n",
       "      <td>0.987053</td>\n",
       "      <td>1.000699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FLOW_LUZ  Hourly Demand  TMAX_Cubi Point   TMAX_NAIA   TMIN_NAIA  \\\n",
       "count  146.000000     146.000000       146.000000  146.000000  146.000000   \n",
       "mean     0.557478       0.790429         0.538072    0.593546    0.685016   \n",
       "std      0.181921       0.150292         0.142749    0.129288    0.125611   \n",
       "min      0.030434       0.274650         0.099099    0.243243    0.283951   \n",
       "25%      0.438704       0.702188         0.452703    0.513514    0.592593   \n",
       "50%      0.558182       0.824704         0.576577    0.594595    0.679012   \n",
       "75%      0.693362       0.898801         0.648649    0.693694    0.787037   \n",
       "max      0.997687       1.025971         0.765766    0.855856    0.975309   \n",
       "\n",
       "       TMAX_Science Garden  TMAX_San Jose  TMIN_San Jose  TMAX_Tayabas  \\\n",
       "count           146.000000     146.000000     146.000000    146.000000   \n",
       "mean              0.543171       0.560176       0.810185      0.658213   \n",
       "std               0.164609       0.171825       0.092766      0.184081   \n",
       "min              -0.010101      -0.071429       0.583333      0.198198   \n",
       "25%               0.444444       0.466518       0.743056      0.531532   \n",
       "50%               0.575758       0.553571       0.796296      0.666667   \n",
       "75%               0.664141       0.707589       0.861111      0.790541   \n",
       "max               0.888889       0.875000       1.074074      1.117117   \n",
       "\n",
       "       TMIN_Tayabas   TMAX_CLSU   TMIN_CLSU  TMAX_Ambulong  TMAX_Casiguran  \\\n",
       "count    146.000000  146.000000  146.000000     146.000000      146.000000   \n",
       "mean       0.711840    0.639251    0.755886       0.563618        0.671501   \n",
       "std        0.135763    0.136037    0.111262       0.149362        0.188913   \n",
       "min        0.339286    0.190083    0.390625       0.180180        0.127451   \n",
       "25%        0.642857    0.595041    0.687500       0.450450        0.553922   \n",
       "50%        0.696429    0.661157    0.765625       0.576577        0.686275   \n",
       "75%        0.785714    0.727273    0.843750       0.682432        0.803922   \n",
       "max        1.107143    0.884298    1.000000       0.909910        1.029412   \n",
       "\n",
       "       TMIN_Casiguran  TMAX_Clark  TMIN_Clark  TMAX_Calapan  TMIN_Calapan  \\\n",
       "count      146.000000  146.000000  146.000000    146.000000    146.000000   \n",
       "mean         0.736791    0.604718    0.642680      0.694221      0.822111   \n",
       "std          0.120253    0.104543    0.098211      0.172207      0.145781   \n",
       "min          0.459184    0.274074    0.349398      0.080460      0.391304   \n",
       "25%          0.673469    0.548148    0.581325      0.563218      0.717391   \n",
       "50%          0.734694    0.622222    0.650602      0.689655      0.826087   \n",
       "75%          0.816327    0.674074    0.710843      0.833333      0.913043   \n",
       "max          1.061224    0.785185    0.903614      1.034483      1.173913   \n",
       "\n",
       "             GWAP        LWAP  TMIN_Cubi Point  TMIN_Science Garden  \\\n",
       "count  146.000000  146.000000       146.000000           146.000000   \n",
       "mean     0.425797    0.413879         0.605789             0.490719   \n",
       "std      0.129339    0.128884         0.141149             0.172058   \n",
       "min      0.126463    0.117123         0.284784             0.143431   \n",
       "25%      0.331399    0.318027         0.495312             0.377928   \n",
       "50%      0.413897    0.399998         0.609240             0.473266   \n",
       "75%      0.506575    0.495505         0.717940             0.583711   \n",
       "max      0.871075    0.863151         0.973942             1.051195   \n",
       "\n",
       "       TMIN_Ambulong  RESERVE_GWAP_Fr  RESERVE_GWAP_Ru  RESERVE_GWAP_Rd  \\\n",
       "count     146.000000       146.000000       146.000000       146.000000   \n",
       "mean        0.572939         0.248807         0.509619         0.020389   \n",
       "std         0.159695         0.155736         0.155523         0.065825   \n",
       "min         0.248106         0.000000         0.000000        -0.062443   \n",
       "25%         0.470994         0.140552         0.449355        -0.051848   \n",
       "50%         0.557184         0.248140         0.554680         0.021794   \n",
       "75%         0.652418         0.357595         0.605170         0.065220   \n",
       "max         1.198596         0.698703         0.853544         0.178230   \n",
       "\n",
       "       RESERVE_GWAP_Dr  RAINFALL_Cubi Point  RAINFALL_NAIA  \\\n",
       "count       146.000000           146.000000     146.000000   \n",
       "mean          0.126388             0.584337       0.227546   \n",
       "std           0.162759             0.238283       0.031679   \n",
       "min           0.000008             0.000000       0.213983   \n",
       "25%           0.000022             0.493149       0.215057   \n",
       "50%           0.000058             0.493149       0.215700   \n",
       "75%           0.286576             0.760694       0.220537   \n",
       "max           0.614714             0.993971       0.414274   \n",
       "\n",
       "       RAINFALL_Science Garden  RAINFALL_San Jose  RAINFALL_Tayabas  \\\n",
       "count               146.000000         146.000000        146.000000   \n",
       "mean                  0.484212           0.512489          0.409320   \n",
       "std                   0.289652           0.260219          0.247500   \n",
       "min                   0.000000           0.000000          0.000000   \n",
       "25%                   0.347275           0.417791          0.212722   \n",
       "50%                   0.465016           0.417791          0.403740   \n",
       "75%                   0.710276           0.729482          0.632636   \n",
       "max                   1.019252           0.988346          0.884560   \n",
       "\n",
       "       RAINFALL_CLSU  RAINFALL_Tanay  TMAX_Tanay  TMIN_Tanay  \\\n",
       "count     146.000000      146.000000  146.000000  146.000000   \n",
       "mean        0.574505        0.525290    0.560115    0.590763   \n",
       "std         0.192706        0.222899    0.180478    0.153060   \n",
       "min         0.000000        0.000000    0.168248    0.278156   \n",
       "25%         0.493466        0.335037    0.433551    0.490038   \n",
       "50%         0.493466        0.512369    0.565962    0.574931   \n",
       "75%         0.735342        0.717194    0.693058    0.667813   \n",
       "max         0.953031        0.920012    0.946032    1.523148   \n",
       "\n",
       "       RAINFALL_Ambulong  RAINFALL_Casiguran  RAINFALL_Clark  RAINFALL_Calapan  \n",
       "count         146.000000          146.000000      146.000000        146.000000  \n",
       "mean            0.569485            0.595334        0.482560          0.599255  \n",
       "std             0.170946            0.221186        0.259388          0.180647  \n",
       "min             0.413668            0.000000        0.000000          0.435920  \n",
       "25%             0.413668            0.383035        0.422884          0.435920  \n",
       "50%             0.503364            0.575704        0.422884          0.514458  \n",
       "75%             0.719330            0.812450        0.643084          0.769946  \n",
       "max             0.926266            0.986848        0.987053          1.000699  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_combined = np.hstack([train_data_minmax, train_data_bc, train_data_yj])\n",
    "val_data_combined = np.hstack([val_data_minmax, val_data_bc, val_data_yj])\n",
    "test_data_combined = np.hstack([test_data_minmax, test_data_bc, test_data_yj])\n",
    "\n",
    "# Create DataFrames for each combined dataset\n",
    "train_data_combined_df = pd.DataFrame(train_data_combined, columns=minmax_cols + boxcox_cols + yeojohnson_cols)\n",
    "val_data_combined_df = pd.DataFrame(val_data_combined, columns=minmax_cols + boxcox_cols + yeojohnson_cols)\n",
    "test_data_combined_df = pd.DataFrame(test_data_combined, columns=minmax_cols + boxcox_cols + yeojohnson_cols)\n",
    "\n",
    "# Ensure all columns are displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the summary statistics for each column in the combined validation data\n",
    "print(\"Summary statistics for each column in the combined validation data:\")\n",
    "display(train_data_combined_df.describe(), val_data_combined_df.describe(), test_data_combined_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for each column in the combined validation data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GWAP</th>\n",
       "      <th>LWAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.606551</td>\n",
       "      <td>0.601150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.170671</td>\n",
       "      <td>0.170680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.489789</td>\n",
       "      <td>0.486298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.579811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.746433</td>\n",
       "      <td>0.738104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             GWAP        LWAP\n",
       "count  438.000000  438.000000\n",
       "mean     0.606551    0.601150\n",
       "std      0.170671    0.170680\n",
       "min      0.000000    0.000000\n",
       "25%      0.489789    0.486298\n",
       "50%      0.584372    0.579811\n",
       "75%      0.746433    0.738104\n",
       "max      1.000000    1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GWAP</th>\n",
       "      <th>LWAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.547668</td>\n",
       "      <td>0.547750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.170823</td>\n",
       "      <td>0.174764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.204948</td>\n",
       "      <td>0.193783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.420445</td>\n",
       "      <td>0.423292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.533827</td>\n",
       "      <td>0.542242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.660125</td>\n",
       "      <td>0.660893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.872305</td>\n",
       "      <td>0.889567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             GWAP        LWAP\n",
       "count  146.000000  146.000000\n",
       "mean     0.547668    0.547750\n",
       "std      0.170823    0.174764\n",
       "min      0.204948    0.193783\n",
       "25%      0.420445    0.423292\n",
       "50%      0.533827    0.542242\n",
       "75%      0.660125    0.660893\n",
       "max      0.872305    0.889567"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GWAP</th>\n",
       "      <th>LWAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.425797</td>\n",
       "      <td>0.413879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.129339</td>\n",
       "      <td>0.128884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.126463</td>\n",
       "      <td>0.117123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.331399</td>\n",
       "      <td>0.318027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.413897</td>\n",
       "      <td>0.399998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.506575</td>\n",
       "      <td>0.495505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.871075</td>\n",
       "      <td>0.863151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             GWAP        LWAP\n",
       "count  146.000000  146.000000\n",
       "mean     0.425797    0.413879\n",
       "std      0.129339    0.128884\n",
       "min      0.126463    0.117123\n",
       "25%      0.331399    0.318027\n",
       "50%      0.413897    0.399998\n",
       "75%      0.506575    0.495505\n",
       "max      0.871075    0.863151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_labels_combined_df = pd.DataFrame(train_labels_transformed, columns=boxcox_colsy )\n",
    "val_labels_combined_df = pd.DataFrame(val_labels_transformed, columns=boxcox_colsy )\n",
    "test_labels_combined_df = pd.DataFrame(test_labels_transformed, columns=boxcox_colsy)\n",
    "\n",
    "# Ensure all columns are displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the summary statistics for each column in the combined validation data\n",
    "print(\"Summary statistics for each column in the combined validation data:\")\n",
    "display(train_labels_combined_df.describe(),val_labels_combined_df.describe(), test_labels_combined_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx:idx+self.seq_len], self.y[idx+self.seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(train_data_transformed, train_labels_transformed, seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TimeSeriesDataset(val_data_transformed, val_labels_transformed, seq_len)    \n",
    "val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False) \n",
    "\n",
    "test_dataset = TimeSeriesDataset(test_data_transformed, test_labels_transformed, seq_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCustomCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, activation_fn):\n",
    "        super(LSTMCustomCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "        # Combine all gate matrices into one large matrix for efficiency\n",
    "        self.W_ih = nn.Linear(input_size, 4 * hidden_size, bias=False)\n",
    "        self.W_hh = nn.Linear(hidden_size, 4 * hidden_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(4 * hidden_size))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        \n",
    "        # Optimized matrix multiplication and bias addition\n",
    "        gates = self.W_ih(x) + self.W_hh(h) + self.bias\n",
    "        \n",
    "        # Split into 4 gate vectors\n",
    "        i_gate, f_gate, o_gate, g_gate = torch.chunk(gates, 4, dim=1)\n",
    "        \n",
    "        # Sigmoid activations for gates\n",
    "        i_gate = torch.sigmoid(i_gate)\n",
    "        f_gate = torch.sigmoid(f_gate)\n",
    "        o_gate = torch.sigmoid(o_gate)\n",
    "        \n",
    "        # Apply the custom activation function for the cell gate\n",
    "        g_gate = self.activation_fn(g_gate)\n",
    "        \n",
    "        # Compute the new cell state\n",
    "        c_next = f_gate * c + i_gate * g_gate\n",
    "        \n",
    "        # Compute the new hidden state using the custom activation function\n",
    "        h_next = o_gate * self.activation_fn(c_next)\n",
    "        \n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCustom(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, activation_fn=torch.tanh, batch_first=False):\n",
    "        super(LSTMCustom, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.activation_fn = activation_fn\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        # Create a list of LSTM cells\n",
    "        self.cells = nn.ModuleList([LSTMCustomCell(input_size if i == 0 else hidden_size, hidden_size, activation_fn) for i in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Determine the correct input shape\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_len, _ = x.size()\n",
    "            x = x.transpose(0, 1)  # Convert to (seq_len, batch_size, input_size) for processing\n",
    "        else:\n",
    "            seq_len, batch_size, _ = x.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            # Initialize hidden and cell states with zeros\n",
    "            h = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
    "        else:\n",
    "            h, c = hidden\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        # Iterate over each time step\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t, :, :]  # Input at time step t\n",
    "            for i, cell in enumerate(self.cells):\n",
    "\n",
    "                h[i], c[i] = cell(x_t, (h[i], c[i]))\n",
    "                x_t = h[i]  # Pass hidden state to the next layer\n",
    "\n",
    "            outputs.append(h[-1].unsqueeze(0))  # Collect output from the last layer\n",
    "        \n",
    "        # Stack the outputs across time steps\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        \n",
    "        # Convert output back to (batch_size, seq_len, hidden_size) if batch_first is True\n",
    "        if self.batch_first:\n",
    "            outputs = outputs.transpose(0, 1)\n",
    "        \n",
    "        # Return outputs and the last hidden and cell states\n",
    "        return outputs, (torch.stack(h), torch.stack(c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers,activation_fn):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = LSTMCustom(input_size, hidden_size, num_layers, activation_fn, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = train_data.shape[1]  # Number of features\n",
    "hidden_size = 128\n",
    "output_size = train_labels.shape[1]  # Number of output features\n",
    "num_layers = 2\n",
    "activation_fn = torch.relu\n",
    "model = LSTMModel(input_size, hidden_size,output_size, num_layers,activation_fn).to(device)\n",
    "epoch=100\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Use MSE loss for regression tasks\n",
    "optimizer = optim.AdamW(list(model.parameters()), lr=1e-4, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "def train(model, dataloader, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0  # Initialize total loss to 0\n",
    "\n",
    "    \n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Check for NaN values in inputs\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).to(device)\n",
    "        \n",
    "        # Check for NaN values in outputs\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Return the average loss over all batches\n",
    "    return total_loss / len(train_dataloader.dataset)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0  # Initialize total loss\n",
    "    \n",
    "\n",
    "    for i, (inputs, target) in enumerate(dataloader):  # Use `test_dataloader`\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        predictions = outputs.to(device)  \n",
    "    \n",
    "        loss = criterion(predictions, target)\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Return the average loss over all batches\n",
    "    \n",
    "    return total_loss/len(dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(config,epoch):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = LSTMModel(input_size, config[\"hidden_size\"], output_size, config[\"num_layers\"],activation_fn).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(list(model.parameters()), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    \n",
    "    \n",
    "\n",
    "    train_dataloader, val_dataloader = load_data()\n",
    "    for e in range(epoch):  # Replace with your actual number of epochs\n",
    "        train_loss = train(model, train_dataloader, device, optimizer, criterion)\n",
    "        val_loss = evaluate(model, val_dataloader, device, criterion)\n",
    "        \n",
    "\n",
    "        # Report the results\n",
    "        session.report(  # Highlighted change\n",
    "            {\"loss\": val_loss} # Highlighted change\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"hidden_size\": tune.choice([50, 100, 200]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-3)\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"loss\", \"training_iteration\"]\n",
    ")\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    return f\"trial_{trial.trial_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 23:29:55,594\tINFO worker.py:1807 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d2b75a711240fc9524de1935f304e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.9.20</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.0.0.dev0</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.20', ray_version='3.0.0.dev0', ray_commit='e1c4e58016f79c9c1134da45cea23e3345f5354b')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()  # Shutdown any existing Ray instances\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 23:29:58,514\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-10-24 23:29:58 (running for 00:00:00.19)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "+------------------------+----------+-------+---------------+------------+--------------+----------------+\n",
      "| Trial name             | status   | loc   |   hidden_size |         lr |   num_layers |   weight_decay |\n",
      "|------------------------+----------+-------+---------------+------------+--------------+----------------|\n",
      "| train_lstm_d4002_00000 | PENDING  |       |           200 | 0.0184626  |            2 |    6.1431e-06  |\n",
      "| train_lstm_d4002_00001 | PENDING  |       |           200 | 0.0181971  |            3 |    1.92374e-05 |\n",
      "| train_lstm_d4002_00002 | PENDING  |       |           200 | 0.0158663  |            3 |    0.000331209 |\n",
      "| train_lstm_d4002_00003 | PENDING  |       |            50 | 0.0036777  |            3 |    6.96127e-06 |\n",
      "| train_lstm_d4002_00004 | PENDING  |       |            50 | 0.0304247  |            3 |    4.37131e-05 |\n",
      "| train_lstm_d4002_00005 | PENDING  |       |            50 | 0.00011321 |            2 |    8.17622e-05 |\n",
      "| train_lstm_d4002_00006 | PENDING  |       |           200 | 0.0543124  |            2 |    2.00467e-06 |\n",
      "| train_lstm_d4002_00007 | PENDING  |       |            50 | 0.00187894 |            3 |    1.46198e-06 |\n",
      "| train_lstm_d4002_00008 | PENDING  |       |            50 | 0.0131494  |            1 |    1.80784e-05 |\n",
      "| train_lstm_d4002_00009 | PENDING  |       |           200 | 0.0220781  |            3 |    4.28162e-05 |\n",
      "+------------------------+----------+-------+---------------+------------+--------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:03 (running for 00:00:05.19)\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 8.000: -5688448891.4280405 | Iter 4.000: -3251428374.252351 | Iter 2.000: -27195272.362719256 | Iter 1.000: -12023428.513999254\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (9 PENDING, 1 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |        loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------|\n",
      "| train_lstm_d4002_00001 | PENDING    |                 |           200 | 0.0181971  |            3 |    1.92374e-05 |             |                      |\n",
      "| train_lstm_d4002_00002 | PENDING    |                 |           200 | 0.0158663  |            3 |    0.000331209 |             |                      |\n",
      "| train_lstm_d4002_00003 | PENDING    |                 |            50 | 0.0036777  |            3 |    6.96127e-06 |             |                      |\n",
      "| train_lstm_d4002_00004 | PENDING    |                 |            50 | 0.0304247  |            3 |    4.37131e-05 |             |                      |\n",
      "| train_lstm_d4002_00005 | PENDING    |                 |            50 | 0.00011321 |            2 |    8.17622e-05 |             |                      |\n",
      "| train_lstm_d4002_00006 | PENDING    |                 |           200 | 0.0543124  |            2 |    2.00467e-06 |             |                      |\n",
      "| train_lstm_d4002_00007 | PENDING    |                 |            50 | 0.00187894 |            3 |    1.46198e-06 |             |                      |\n",
      "| train_lstm_d4002_00008 | PENDING    |                 |            50 | 0.0131494  |            1 |    1.80784e-05 |             |                      |\n",
      "| train_lstm_d4002_00009 | PENDING    |                 |           200 | 0.0220781  |            3 |    4.28162e-05 |             |                      |\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  | 3.09405e+09 |                   10 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:08 (running for 00:00:10.24)\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 8.000: -5688448891.4280405 | Iter 4.000: -3251428374.252351 | Iter 2.000: -27195272.362719256 | Iter 1.000: -56535109745.47936\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (8 PENDING, 2 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |        loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------|\n",
      "| train_lstm_d4002_00002 | PENDING    |                 |           200 | 0.0158663  |            3 |    0.000331209 |             |                      |\n",
      "| train_lstm_d4002_00003 | PENDING    |                 |            50 | 0.0036777  |            3 |    6.96127e-06 |             |                      |\n",
      "| train_lstm_d4002_00004 | PENDING    |                 |            50 | 0.0304247  |            3 |    4.37131e-05 |             |                      |\n",
      "| train_lstm_d4002_00005 | PENDING    |                 |            50 | 0.00011321 |            2 |    8.17622e-05 |             |                      |\n",
      "| train_lstm_d4002_00006 | PENDING    |                 |           200 | 0.0543124  |            2 |    2.00467e-06 |             |                      |\n",
      "| train_lstm_d4002_00007 | PENDING    |                 |            50 | 0.00187894 |            3 |    1.46198e-06 |             |                      |\n",
      "| train_lstm_d4002_00008 | PENDING    |                 |            50 | 0.0131494  |            1 |    1.80784e-05 |             |                      |\n",
      "| train_lstm_d4002_00009 | PENDING    |                 |           200 | 0.0220781  |            3 |    4.28162e-05 |             |                      |\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  | 3.09405e+09 |                   10 |\n",
      "| train_lstm_d4002_00001 | TERMINATED | 127.0.0.1:14692 |           200 | 0.0181971  |            3 |    1.92374e-05 | 1.13058e+11 |                    1 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:13 (running for 00:00:15.26)\n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 8.000: -5688448891.4280405 | Iter 4.000: -3251428374.252351 | Iter 2.000: -27195272.362719256 | Iter 1.000: -113058196062.44472\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (7 PENDING, 3 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |        loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------|\n",
      "| train_lstm_d4002_00003 | PENDING    |                 |            50 | 0.0036777  |            3 |    6.96127e-06 |             |                      |\n",
      "| train_lstm_d4002_00004 | PENDING    |                 |            50 | 0.0304247  |            3 |    4.37131e-05 |             |                      |\n",
      "| train_lstm_d4002_00005 | PENDING    |                 |            50 | 0.00011321 |            2 |    8.17622e-05 |             |                      |\n",
      "| train_lstm_d4002_00006 | PENDING    |                 |           200 | 0.0543124  |            2 |    2.00467e-06 |             |                      |\n",
      "| train_lstm_d4002_00007 | PENDING    |                 |            50 | 0.00187894 |            3 |    1.46198e-06 |             |                      |\n",
      "| train_lstm_d4002_00008 | PENDING    |                 |            50 | 0.0131494  |            1 |    1.80784e-05 |             |                      |\n",
      "| train_lstm_d4002_00009 | PENDING    |                 |           200 | 0.0220781  |            3 |    4.28162e-05 |             |                      |\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  | 3.09405e+09 |                   10 |\n",
      "| train_lstm_d4002_00001 | TERMINATED | 127.0.0.1:14692 |           200 | 0.0181971  |            3 |    1.92374e-05 | 1.13058e+11 |                    1 |\n",
      "| train_lstm_d4002_00002 | TERMINATED | 127.0.0.1:5236  |           200 | 0.0158663  |            3 |    0.000331209 | 2.49491e+12 |                    1 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:18 (running for 00:00:20.36)\n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 8.000: -2985228641.63646 | Iter 4.000: -1626421568.6410925 | Iter 2.000: -13713995.015881972 | Iter 1.000: -56547881075.28061\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (6 PENDING, 4 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |        loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------|\n",
      "| train_lstm_d4002_00004 | PENDING    |                 |            50 | 0.0304247  |            3 |    4.37131e-05 |             |                      |\n",
      "| train_lstm_d4002_00005 | PENDING    |                 |            50 | 0.00011321 |            2 |    8.17622e-05 |             |                      |\n",
      "| train_lstm_d4002_00006 | PENDING    |                 |           200 | 0.0543124  |            2 |    2.00467e-06 |             |                      |\n",
      "| train_lstm_d4002_00007 | PENDING    |                 |            50 | 0.00187894 |            3 |    1.46198e-06 |             |                      |\n",
      "| train_lstm_d4002_00008 | PENDING    |                 |            50 | 0.0131494  |            1 |    1.80784e-05 |             |                      |\n",
      "| train_lstm_d4002_00009 | PENDING    |                 |           200 | 0.0220781  |            3 |    4.28162e-05 |             |                      |\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  | 3.09405e+09 |                   10 |\n",
      "| train_lstm_d4002_00001 | TERMINATED | 127.0.0.1:14692 |           200 | 0.0181971  |            3 |    1.92374e-05 | 1.13058e+11 |                    1 |\n",
      "| train_lstm_d4002_00002 | TERMINATED | 127.0.0.1:5236  |           200 | 0.0158663  |            3 |    0.000331209 | 2.49491e+12 |                    1 |\n",
      "| train_lstm_d4002_00003 | TERMINATED | 127.0.0.1:40520 |            50 | 0.0036777  |            3 |    6.96127e-06 | 2.14421e+08 |                   10 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:23 (running for 00:00:25.37)\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 8.000: -282008391.8448797 | Iter 4.000: -888580936.1029772 | Iter 2.000: -9195765.205534132 | Iter 1.000: -32128379956.383858\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (5 PENDING, 5 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |        loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------|\n",
      "| train_lstm_d4002_00005 | PENDING    |                 |            50 | 0.00011321 |            2 |    8.17622e-05 |             |                      |\n",
      "| train_lstm_d4002_00006 | PENDING    |                 |           200 | 0.0543124  |            2 |    2.00467e-06 |             |                      |\n",
      "| train_lstm_d4002_00007 | PENDING    |                 |            50 | 0.00187894 |            3 |    1.46198e-06 |             |                      |\n",
      "| train_lstm_d4002_00008 | PENDING    |                 |            50 | 0.0131494  |            1 |    1.80784e-05 |             |                      |\n",
      "| train_lstm_d4002_00009 | PENDING    |                 |           200 | 0.0220781  |            3 |    4.28162e-05 |             |                      |\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  | 3.09405e+09 |                   10 |\n",
      "| train_lstm_d4002_00001 | TERMINATED | 127.0.0.1:14692 |           200 | 0.0181971  |            3 |    1.92374e-05 | 1.13058e+11 |                    1 |\n",
      "| train_lstm_d4002_00002 | TERMINATED | 127.0.0.1:5236  |           200 | 0.0158663  |            3 |    0.000331209 | 2.49491e+12 |                    1 |\n",
      "| train_lstm_d4002_00003 | TERMINATED | 127.0.0.1:40520 |            50 | 0.0036777  |            3 |    6.96127e-06 | 2.14421e+08 |                   10 |\n",
      "| train_lstm_d4002_00004 | TERMINATED | 127.0.0.1:33256 |            50 | 0.0304247  |            3 |    4.37131e-05 | 1.0225e+08  |                   10 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+-------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:28 (running for 00:00:30.43)\n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 8.000: -194987917.83261895 | Iter 4.000: -444997849.56640565 | Iter 2.000: -4714241.437289409 | Iter 1.000: -16082973022.250185\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (4 PENDING, 6 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |             loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------|\n",
      "| train_lstm_d4002_00006 | PENDING    |                 |           200 | 0.0543124  |            2 |    2.00467e-06 |                  |                      |\n",
      "| train_lstm_d4002_00007 | PENDING    |                 |            50 | 0.00187894 |            3 |    1.46198e-06 |                  |                      |\n",
      "| train_lstm_d4002_00008 | PENDING    |                 |            50 | 0.0131494  |            1 |    1.80784e-05 |                  |                      |\n",
      "| train_lstm_d4002_00009 | PENDING    |                 |           200 | 0.0220781  |            3 |    4.28162e-05 |                  |                      |\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  |      3.09405e+09 |                   10 |\n",
      "| train_lstm_d4002_00001 | TERMINATED | 127.0.0.1:14692 |           200 | 0.0181971  |            3 |    1.92374e-05 |      1.13058e+11 |                    1 |\n",
      "| train_lstm_d4002_00002 | TERMINATED | 127.0.0.1:5236  |           200 | 0.0158663  |            3 |    0.000331209 |      2.49491e+12 |                    1 |\n",
      "| train_lstm_d4002_00003 | TERMINATED | 127.0.0.1:40520 |            50 | 0.0036777  |            3 |    6.96127e-06 |      2.14421e+08 |                   10 |\n",
      "| train_lstm_d4002_00004 | TERMINATED | 127.0.0.1:33256 |            50 | 0.0304247  |            3 |    4.37131e-05 |      1.0225e+08  |                   10 |\n",
      "| train_lstm_d4002_00005 | TERMINATED | 127.0.0.1:6296  |            50 | 0.00011321 |            2 |    8.17622e-05 | 199380           |                   10 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:34 (running for 00:00:35.49)\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 8.000: -194987917.83261895 | Iter 4.000: -444997849.56640565 | Iter 2.000: -4714241.437289409 | Iter 1.000: -32128379956.383858\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (3 PENDING, 7 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |             loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------|\n",
      "| train_lstm_d4002_00007 | PENDING    |                 |            50 | 0.00187894 |            3 |    1.46198e-06 |                  |                      |\n",
      "| train_lstm_d4002_00008 | PENDING    |                 |            50 | 0.0131494  |            1 |    1.80784e-05 |                  |                      |\n",
      "| train_lstm_d4002_00009 | PENDING    |                 |           200 | 0.0220781  |            3 |    4.28162e-05 |                  |                      |\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  |      3.09405e+09 |                   10 |\n",
      "| train_lstm_d4002_00001 | TERMINATED | 127.0.0.1:14692 |           200 | 0.0181971  |            3 |    1.92374e-05 |      1.13058e+11 |                    1 |\n",
      "| train_lstm_d4002_00002 | TERMINATED | 127.0.0.1:5236  |           200 | 0.0158663  |            3 |    0.000331209 |      2.49491e+12 |                    1 |\n",
      "| train_lstm_d4002_00003 | TERMINATED | 127.0.0.1:40520 |            50 | 0.0036777  |            3 |    6.96127e-06 |      2.14421e+08 |                   10 |\n",
      "| train_lstm_d4002_00004 | TERMINATED | 127.0.0.1:33256 |            50 | 0.0304247  |            3 |    4.37131e-05 |      1.0225e+08  |                   10 |\n",
      "| train_lstm_d4002_00005 | TERMINATED | 127.0.0.1:6296  |            50 | 0.00011321 |            2 |    8.17622e-05 | 199380           |                   10 |\n",
      "| train_lstm_d4002_00006 | TERMINATED | 127.0.0.1:33720 |           200 | 0.0543124  |            2 |    2.00467e-06 |      4.45238e+13 |                    1 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:39 (running for 00:00:40.49)\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 8.000: -194987917.83261895 | Iter 4.000: -444997849.56640565 | Iter 2.000: -9195765.205534132 | Iter 1.000: -16082973022.250185\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (2 PENDING, 8 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |             loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------|\n",
      "| train_lstm_d4002_00008 | PENDING    |                 |            50 | 0.0131494  |            1 |    1.80784e-05 |                  |                      |\n",
      "| train_lstm_d4002_00009 | PENDING    |                 |           200 | 0.0220781  |            3 |    4.28162e-05 |                  |                      |\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  |      3.09405e+09 |                   10 |\n",
      "| train_lstm_d4002_00001 | TERMINATED | 127.0.0.1:14692 |           200 | 0.0181971  |            3 |    1.92374e-05 |      1.13058e+11 |                    1 |\n",
      "| train_lstm_d4002_00002 | TERMINATED | 127.0.0.1:5236  |           200 | 0.0158663  |            3 |    0.000331209 |      2.49491e+12 |                    1 |\n",
      "| train_lstm_d4002_00003 | TERMINATED | 127.0.0.1:40520 |            50 | 0.0036777  |            3 |    6.96127e-06 |      2.14421e+08 |                   10 |\n",
      "| train_lstm_d4002_00004 | TERMINATED | 127.0.0.1:33256 |            50 | 0.0304247  |            3 |    4.37131e-05 |      1.0225e+08  |                   10 |\n",
      "| train_lstm_d4002_00005 | TERMINATED | 127.0.0.1:6296  |            50 | 0.00011321 |            2 |    8.17622e-05 | 199380           |                   10 |\n",
      "| train_lstm_d4002_00006 | TERMINATED | 127.0.0.1:33720 |           200 | 0.0543124  |            2 |    2.00467e-06 |      4.45238e+13 |                    1 |\n",
      "| train_lstm_d4002_00007 | TERMINATED | 127.0.0.1:8004  |            50 | 0.00187894 |            3 |    1.46198e-06 |      4.64372e+07 |                    2 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:44 (running for 00:00:45.51)\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 8.000: -107967443.82035819 | Iter 4.000: -14467172.389174856 | Iter 2.000: -7134665.1244955715 | Iter 1.000: -37566088.11651013\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (1 PENDING, 9 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |             loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------|\n",
      "| train_lstm_d4002_00009 | PENDING    |                 |           200 | 0.0220781  |            3 |    4.28162e-05 |                  |                      |\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  |      3.09405e+09 |                   10 |\n",
      "| train_lstm_d4002_00001 | TERMINATED | 127.0.0.1:14692 |           200 | 0.0181971  |            3 |    1.92374e-05 |      1.13058e+11 |                    1 |\n",
      "| train_lstm_d4002_00002 | TERMINATED | 127.0.0.1:5236  |           200 | 0.0158663  |            3 |    0.000331209 |      2.49491e+12 |                    1 |\n",
      "| train_lstm_d4002_00003 | TERMINATED | 127.0.0.1:40520 |            50 | 0.0036777  |            3 |    6.96127e-06 |      2.14421e+08 |                   10 |\n",
      "| train_lstm_d4002_00004 | TERMINATED | 127.0.0.1:33256 |            50 | 0.0304247  |            3 |    4.37131e-05 |      1.0225e+08  |                   10 |\n",
      "| train_lstm_d4002_00005 | TERMINATED | 127.0.0.1:6296  |            50 | 0.00011321 |            2 |    8.17622e-05 | 199380           |                   10 |\n",
      "| train_lstm_d4002_00006 | TERMINATED | 127.0.0.1:33720 |           200 | 0.0543124  |            2 |    2.00467e-06 |      4.45238e+13 |                    1 |\n",
      "| train_lstm_d4002_00007 | TERMINATED | 127.0.0.1:8004  |            50 | 0.00187894 |            3 |    1.46198e-06 |      4.64372e+07 |                    2 |\n",
      "| train_lstm_d4002_00008 | TERMINATED | 127.0.0.1:5916  |            50 | 0.0131494  |            1 |    1.80784e-05 |      3.31342e+07 |                   10 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 23:30:46,163\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Paulo John Mercado/ray_results/train_lstm_2024-10-24_23-29-58' in 0.0124s.\n",
      "2024-10-24 23:30:46,167\tINFO tune.py:1041 -- Total run time: 47.65 seconds (47.60 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-10-24 23:30:46 (running for 00:00:47.62)\n",
      "Using AsyncHyperBand: num_stopped=10\n",
      "Bracket: Iter 8.000: -107967443.82035819 | Iter 4.000: -14467172.389174856 | Iter 2.000: -7134665.1244955715 | Iter 1.000: -400827366.1814739\n",
      "Logical resource usage: 4.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/PAULOJ~1/AppData/Local/Temp/ray/session_2024-10-24_23-29-53_287663_35820/artifacts/2024-10-24_23-29-58/train_lstm_2024-10-24_23-29-58/driver_artifacts\n",
      "Number of trials: 10/10 (10 TERMINATED)\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "| Trial name             | status     | loc             |   hidden_size |         lr |   num_layers |   weight_decay |             loss |   training_iteration |\n",
      "|------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------|\n",
      "| train_lstm_d4002_00000 | TERMINATED | 127.0.0.1:12060 |           200 | 0.0184626  |            2 |    6.1431e-06  |      3.09405e+09 |                   10 |\n",
      "| train_lstm_d4002_00001 | TERMINATED | 127.0.0.1:14692 |           200 | 0.0181971  |            3 |    1.92374e-05 |      1.13058e+11 |                    1 |\n",
      "| train_lstm_d4002_00002 | TERMINATED | 127.0.0.1:5236  |           200 | 0.0158663  |            3 |    0.000331209 |      2.49491e+12 |                    1 |\n",
      "| train_lstm_d4002_00003 | TERMINATED | 127.0.0.1:40520 |            50 | 0.0036777  |            3 |    6.96127e-06 |      2.14421e+08 |                   10 |\n",
      "| train_lstm_d4002_00004 | TERMINATED | 127.0.0.1:33256 |            50 | 0.0304247  |            3 |    4.37131e-05 |      1.0225e+08  |                   10 |\n",
      "| train_lstm_d4002_00005 | TERMINATED | 127.0.0.1:6296  |            50 | 0.00011321 |            2 |    8.17622e-05 | 199380           |                   10 |\n",
      "| train_lstm_d4002_00006 | TERMINATED | 127.0.0.1:33720 |           200 | 0.0543124  |            2 |    2.00467e-06 |      4.45238e+13 |                    1 |\n",
      "| train_lstm_d4002_00007 | TERMINATED | 127.0.0.1:8004  |            50 | 0.00187894 |            3 |    1.46198e-06 |      4.64372e+07 |                    2 |\n",
      "| train_lstm_d4002_00008 | TERMINATED | 127.0.0.1:5916  |            50 | 0.0131494  |            1 |    1.80784e-05 |      3.31342e+07 |                   10 |\n",
      "| train_lstm_d4002_00009 | TERMINATED | 127.0.0.1:33044 |           200 | 0.0220781  |            3 |    4.28162e-05 |      7.64089e+08 |                    1 |\n",
      "+------------------------+------------+-----------------+---------------+------------+--------------+----------------+------------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x212280c28e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\n",
    "    tune.with_parameters(train_lstm, epoch=epoch),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    verbose=1,\n",
    "    trial_dirname_creator=trial_dirname_creator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "def run(model, train_dataloader, val_dataloader, test_dataloader, device, epoch,optimizer):\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        \n",
    "        train_loss = train(model, train_dataloader, device, optimizer, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss=evaluate(model,val_dataloader,device,criterion)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "        \n",
    "\n",
    "        if train_loss < 1e-3 :\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.4221355153761994, Val Loss: 102719.73389890115\n",
      "Epoch 2, Training Loss: 0.40658862290692166, Val Loss: 182406.08396069356\n",
      "Epoch 3, Training Loss: 0.3907516410621856, Val Loss: 177085.00918466458\n",
      "Epoch 4, Training Loss: 0.3734323219190894, Val Loss: 372562.60939332086\n",
      "Epoch 5, Training Loss: 0.3524107794059125, Val Loss: 786204.4945901558\n",
      "Epoch 6, Training Loss: 0.3249097933210519, Val Loss: 1068055.410893077\n",
      "Epoch 7, Training Loss: 0.28749868944460166, Val Loss: 1350454.1183733488\n",
      "Epoch 8, Training Loss: 0.23544287854446888, Val Loss: 1640112.6716024575\n",
      "Epoch 9, Training Loss: 0.16261480088300329, Val Loss: 5567456.739530799\n",
      "Epoch 10, Training Loss: 0.07289002136502354, Val Loss: 17228113.05968508\n",
      "Epoch 11, Training Loss: 0.045143086929954675, Val Loss: 26662004.071861144\n",
      "Epoch 12, Training Loss: 0.045482916485198414, Val Loss: 23927137.63559626\n",
      "Epoch 13, Training Loss: 0.035034955827100096, Val Loss: 17553680.59482131\n",
      "Epoch 14, Training Loss: 0.03424135689772752, Val Loss: 22127140.854310922\n",
      "Epoch 15, Training Loss: 0.03167795893798848, Val Loss: 24508729.115776334\n",
      "Epoch 16, Training Loss: 0.03150917772903796, Val Loss: 22650288.828356422\n",
      "Epoch 17, Training Loss: 0.03078365287032592, Val Loss: 20353031.3873545\n",
      "Epoch 18, Training Loss: 0.02939843871304441, Val Loss: 24175336.076623958\n",
      "Epoch 19, Training Loss: 0.028309098451361578, Val Loss: 27043453.256134193\n",
      "Epoch 20, Training Loss: 0.027455368522217544, Val Loss: 41711104.01853983\n",
      "Epoch 21, Training Loss: 0.026748080951812095, Val Loss: 45465942.5790681\n",
      "Epoch 22, Training Loss: 0.02589796131299571, Val Loss: 44397542.23291738\n",
      "Epoch 23, Training Loss: 0.025000682243269172, Val Loss: 43179287.95886427\n",
      "Epoch 24, Training Loss: 0.02417163586723694, Val Loss: 54599241.684934296\n",
      "Epoch 25, Training Loss: 0.023425676006382967, Val Loss: 56039468.21675902\n",
      "Epoch 26, Training Loss: 0.022713328636059075, Val Loss: 56974593.85650535\n",
      "Epoch 27, Training Loss: 0.022040506876711226, Val Loss: 67245306.48912317\n",
      "Epoch 28, Training Loss: 0.021433767363463644, Val Loss: 58686342.459932506\n",
      "Epoch 29, Training Loss: 0.020890426716125482, Val Loss: 60713273.10704154\n",
      "Epoch 30, Training Loss: 0.020401597662619262, Val Loss: 96908995.23620978\n",
      "Epoch 31, Training Loss: 0.019970580036568254, Val Loss: 99215625.2215354\n",
      "Epoch 32, Training Loss: 0.019596729008372865, Val Loss: 115462784.93351951\n",
      "Epoch 33, Training Loss: 0.019272837275747218, Val Loss: 118008581.53762549\n",
      "Epoch 34, Training Loss: 0.018991458175699562, Val Loss: 164499839.091409\n",
      "Epoch 35, Training Loss: 0.018748825089681866, Val Loss: 167303389.01932827\n",
      "Epoch 36, Training Loss: 0.018533764968857412, Val Loss: 130664079.66669956\n",
      "Epoch 37, Training Loss: 0.01834081647498425, Val Loss: 120820522.37164484\n",
      "Epoch 38, Training Loss: 0.0181659305286691, Val Loss: 122529261.59459284\n",
      "Epoch 39, Training Loss: 0.01800288943241063, Val Loss: 120872856.87510534\n",
      "Epoch 40, Training Loss: 0.017848657480853753, Val Loss: 136838504.99015903\n",
      "Epoch 41, Training Loss: 0.017703811063273463, Val Loss: 110969156.15558147\n",
      "Epoch 42, Training Loss: 0.01756598554733112, Val Loss: 108835191.72388658\n",
      "Epoch 43, Training Loss: 0.01743359752042595, Val Loss: 108291462.45766568\n",
      "Epoch 44, Training Loss: 0.017306001370733824, Val Loss: 131158001.27777973\n",
      "Epoch 45, Training Loss: 0.017181973748790692, Val Loss: 92679985.73818508\n",
      "Epoch 46, Training Loss: 0.017059267250056055, Val Loss: 94546170.48636341\n",
      "Epoch 47, Training Loss: 0.016937594672798973, Val Loss: 54510945.623033985\n",
      "Epoch 48, Training Loss: 0.0168129401648819, Val Loss: 58621333.19136367\n",
      "Epoch 49, Training Loss: 0.016684317790628723, Val Loss: 57214810.25610612\n",
      "Epoch 50, Training Loss: 0.016553328573496326, Val Loss: 56804665.10502492\n",
      "Epoch 51, Training Loss: 0.016422166110917475, Val Loss: 46527930.02588203\n",
      "Epoch 52, Training Loss: 0.016294244787031287, Val Loss: 66848237.59421678\n",
      "Epoch 53, Training Loss: 0.016172117536364602, Val Loss: 70644448.70211418\n",
      "Epoch 54, Training Loss: 0.016047623978707327, Val Loss: 64944798.399945855\n",
      "Epoch 55, Training Loss: 0.015924332021647013, Val Loss: 47412360.29921641\n",
      "Epoch 56, Training Loss: 0.015800745142170006, Val Loss: 45137345.392726324\n",
      "Epoch 57, Training Loss: 0.015674724615245987, Val Loss: 28862266.946660314\n",
      "Epoch 58, Training Loss: 0.015551835670202902, Val Loss: 30732043.061743706\n",
      "Epoch 59, Training Loss: 0.015427837653945604, Val Loss: 32322169.565308455\n",
      "Epoch 60, Training Loss: 0.015301115975095059, Val Loss: 30877312.932183415\n",
      "Epoch 61, Training Loss: 0.015175190285332242, Val Loss: 27381041.73791704\n",
      "Epoch 62, Training Loss: 0.015049106154046313, Val Loss: 30410670.975307763\n",
      "Epoch 63, Training Loss: 0.014916131988150476, Val Loss: 21416492.212703772\n",
      "Epoch 64, Training Loss: 0.014786673104714628, Val Loss: 34262056.52923155\n",
      "Epoch 65, Training Loss: 0.014656392896327508, Val Loss: 29099687.60834393\n",
      "Epoch 66, Training Loss: 0.0145256608603587, Val Loss: 19937469.709029507\n",
      "Epoch 67, Training Loss: 0.014393378565576265, Val Loss: 17868181.19101102\n",
      "Epoch 68, Training Loss: 0.014261080816457277, Val Loss: 14111598.514726417\n",
      "Epoch 69, Training Loss: 0.014128190681355597, Val Loss: 18357325.36362581\n",
      "Epoch 70, Training Loss: 0.013994650604103945, Val Loss: 19586690.773677293\n",
      "Epoch 71, Training Loss: 0.013861101831627141, Val Loss: 20455475.579416346\n",
      "Epoch 72, Training Loss: 0.013726784743956374, Val Loss: 13666096.816813765\n",
      "Epoch 73, Training Loss: 0.013590390984133198, Val Loss: 15382788.615359293\n",
      "Epoch 74, Training Loss: 0.013452798130288893, Val Loss: 13860248.874336274\n",
      "Epoch 75, Training Loss: 0.0133117109001236, Val Loss: 16788472.644112904\n",
      "Epoch 76, Training Loss: 0.01317325912269044, Val Loss: 13609739.98224544\n",
      "Epoch 77, Training Loss: 0.013030815497905085, Val Loss: 12312192.931899179\n",
      "Epoch 78, Training Loss: 0.012886938597929174, Val Loss: 9823984.356378322\n",
      "Epoch 79, Training Loss: 0.01274762429262397, Val Loss: 22101561.104614943\n",
      "Epoch 80, Training Loss: 0.012605248081763632, Val Loss: 14189240.183790289\n",
      "Epoch 81, Training Loss: 0.012465629661823093, Val Loss: 9180557.824117867\n",
      "Epoch 82, Training Loss: 0.012325750927499164, Val Loss: 9671194.716271859\n",
      "Epoch 83, Training Loss: 0.012185110984988113, Val Loss: 9459362.083228057\n",
      "Epoch 84, Training Loss: 0.012043225872838166, Val Loss: 4694727.378213598\n",
      "Epoch 85, Training Loss: 0.011903391632396218, Val Loss: 5347820.673286503\n",
      "Epoch 86, Training Loss: 0.011766137326192413, Val Loss: 6430180.385652334\n",
      "Epoch 87, Training Loss: 0.011627052962486124, Val Loss: 2658661.3065482997\n",
      "Epoch 88, Training Loss: 0.011486523365493856, Val Loss: 3002825.680614052\n",
      "Epoch 89, Training Loss: 0.011359311995720987, Val Loss: 3281380.8462041426\n",
      "Epoch 90, Training Loss: 0.011221432339901367, Val Loss: 10728062.170432294\n",
      "Epoch 91, Training Loss: 0.011100124273872707, Val Loss: 7935833.336463287\n",
      "Epoch 92, Training Loss: 0.011120511034153523, Val Loss: 6797803.753050186\n",
      "Epoch 93, Training Loss: 0.01100820106326704, Val Loss: 7304376.183964813\n",
      "Epoch 94, Training Loss: 0.011982248737336311, Val Loss: 6386611.581381495\n",
      "Epoch 95, Training Loss: 0.011437134806438416, Val Loss: 6084005.310802895\n",
      "Epoch 96, Training Loss: 0.012924734766653408, Val Loss: 4597013.191087253\n",
      "Epoch 97, Training Loss: 0.011424741833071007, Val Loss: 5421169.279966505\n",
      "Epoch 98, Training Loss: 0.010749659512900767, Val Loss: 3909726.860214361\n",
      "Epoch 99, Training Loss: 0.010662004293273191, Val Loss: 3882366.1707602628\n",
      "Epoch 100, Training Loss: 0.010365693679775204, Val Loss: 4004530.659632863\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "run(model, train_dataloader,val_dataloader, test_dataloader, device, epoch,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAAHUCAYAAAAjh1kfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi41LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvSM8oowAAIABJREFUeJzs3XlYVGX7B/DvAWHYUTZBQBBFUVFURFFUJPfdzNTc09LM1PLlrfTnkmWaWy6Yr2UK7uG+7wsoZaYpmfsSboi5pCAg6zy/P6Y5MswAAwyi8P1c11wzc84zz3nObHDPc5/7SEIIASIiIiIiIjIYo9IeABERERERUVnDQIuIiIiIiMjAGGgREREREREZGAMtIiIiIiIiA2OgRUREREREZGAMtIiIiIiIiAyMgRYREREREZGBMdAiIiIiIiIyMAZaREREREREBsZAi8q1iIgISJKE06dPl/ZQSkR6ejq+++47BAcHw97eHiYmJrC3t0fr1q3x/fff49mzZwCA7OxsVKxYEZ06ddLqY/78+ZAkCe+8847Wuq+++gqSJOHcuXNa68aPHw9JktC1a1edY7t58yYkSZIvRkZGsLe3R+fOnXHixIlC7+vQoUM1+svrMnTo0EL3nZd+/frBx8enSI9dunQpJEnC/fv3DTaeooqMjIQkSYiIiMizzc6dOyFJEpYsWVKovhcvXgxJkvDo0SN5We/eveHr61vgY5OTkyFJEubOnVuobQKq99cXX3yBixcvaq0LDQ2FlZVVofssCxwcHPDRRx/lub537956fY7y66Oo5s6diw0bNujd3srKyqCf59dFWloa5s2bh8aNG8Pa2hoWFhaoV68evvjiCyQlJZX28LR89NFH+b6XkpOTS3V8mzZtgiRJiIqKKtVxUNlUobQHQEQl4+HDh+jYsSPOnz+PIUOGYOzYsXBycsLjx49x5MgRfPrpp4iJicHq1athbGyMli1bIioqCllZWahQ4cVXQ1RUFCwtLXH06FGtbURFRcHe3h716tXTWJ6ZmYk1a9YAAPbt24f4+Hi4urrqHOeYMWPQv39/ZGdn48KFC5g2bRpCQkJw4sQJNGzYUO/9nTx5Mj744AP5/pkzZzB69GjMmDEDISEh8nJHR0e9+yzI9OnTkZKSUqTH9urVCw0aNIC9vb3BxlNUPXv2hL29PVasWJHnP67h4eEwMzND//79i729mTNn4vnz58XuJz83b97EtGnT4Ovrizp16misGzt2LPr27Vui239dzZw5E6GhofL9n3/+GaGhoZg/fz4CAwPl5c7Ozgbf9ty5c9GiRQv06dPH4H2XFf/88w/at2+P8+fPY/To0fj6669RoUIFREdHY968eVi/fj0OHToEd3f30h6qlrx+QLOwsHjJIyF6eRhoEZVRAwcOxJ9//olDhw6hVatWGut69uyJqVOnYu/evfKykJAQ7Nq1C6dPn5b/oVIqlTh+/DhGjRqFuXPn4tKlS6hduzYAICMjAydOnEDnzp0hSZJG/9u3b8fDhw/RpUsX7N69GytXrsTEiRN1jrNq1ary9oKCglCjRg20adMGS5YswbJly/Te3+rVq6N69ery/bS0NACAt7e3xj+I+Xn+/DnMzc313maNGjX0bpubk5MTnJycivx4Q1IoFBgwYAAWLVqEa9euwdvbW2P9w4cPsWvXLvTp0wcVK1Ys9vZy9/+yVa1aFVWrVi3VMbyqvL29NV4f9Uykj4+P3p8jKjnvv/8+zp49i3379qFdu3by8jZt2qBHjx4ICgpC//79cfz48Zc6rtTU1AIDJr5/qDxi6iBRAdTphTdv3tRYHhUVpZFuoL6v6+Lp6Sk/TqlUYvbs2fDx8YFCoYCTkxMGDx6Mu3fvavTfunVr+Pr64tSpU2jZsiUsLCzg5eWFb775BkqlMt8xnzp1CgcOHMCIESO0giw1e3t7DBw4UL6vnvXJmT7xxx9/4MmTJxgxYgRcXFw0ZrVOnjyJ58+fa8wWqS1fvhympqYIDw+Hu7s7wsPDIYTId8xq6j/Gt27d0qt9UTk7O6N379746aef4OfnB4VCgVmzZgEAFixYgBYtWsDR0RFWVlbw8/PDt99+i6ysLI0+cqcOpqWlQZIkhIaGYsWKFahVqxYsLCzQqFEjHDhwQOOxulIHAwMD0bhxY5w4cQLNmzeHhYUFatSogXnz5mk9f3/88QfatGkDc3NzODk54eOPP8bWrVshSRJ+/fXXQj8fw4cPBwCd6YNr1qxBZmYmhg0bJi/btWsXunTpAldXV5ibm6NmzZoYM2YMnj59WuC2dKUO/vPPPxg6dCgqVaoEa2trdO/eXeszBwAXL17EoEGDUL16dZibm8Pd3R1vvvkmrly5ojE29fvy7bfflj+H6hREXamDWVlZ+Oqrr+Dt7Q1TU1M4Oztj+PDh+PvvvzXaNW7cGIGBgYiJiUFgYKD8Gi1YsKDA/c7Lzz//jN69e8PDwwNmZmaoVq0aBg8ejPj4eI126jTMX3/9FcOHD4ednR0cHBzQt29fPHz4UKNtWloaPv74Yzg5OcHCwgKtW7fGH3/8UeQx5mf58uXw9/eHubk5bG1t0aNHD1y+fFmjzfnz59GzZ09UrlwZCoUCLi4u6NixI65fvw5AlQb4999/Y/PmzfLr1bhx42KPTQiBhQsXom7dulAoFHBwcEC/fv203lsFjQ9Qpc82b94cFStWhIWFBapVq6aVUp2amoqJEyeiRo0acj+jR49GYmKiRjt9+srt8uXL2LJlC9555x2NIEvN398fo0ePRkxMDKKjowEALVq0yDMzwNvbG+3bt5fvZ2VlYfbs2ahbty7MzMzg4OCAgQMH4t69exqP8/X1RevWrbF7924EBATA3Nwcn376ab5j18fp06chSRKWLVuGCRMmoEqVKjAzM0NQUJDO77T9+/ejZcuWsLS0hJWVFUJCQuT9zunGjRsYPHgwXFxcoFAo4ObmhgEDBsip82rPnz+XPzOVKlVC165dtf4OxcTEoF27dnBwcICZmRnc3d3Rs2dPPHnypNj7T2UTAy0iA2nUqBFOnDihcVm1ahVMTExQt25dud2oUaPw2WefoV27dtixYwe++uor7Nu3D82bN9c4jgUA7t+/jwEDBmDgwIHYsWMHOnXqhAkTJshpeXk5ePAgAKB79+56j9/Pzw+VKlXSCKaOHj0KFxcXeHt7o1WrVhpBmLpd7kDr7t27OHDgAHr06AFHR0cMGTIE169fx7Fjx/Qah/ofm5wpfurjr3T9410cJ06cwOTJkzF+/Hjs378f3bp1AwD89ddfGDRoENasWYMdO3Zg8ODBmD59OsaOHatXv1u2bMGPP/6IGTNmYNOmTbCwsED37t1x586dAh97584dDB06FMOGDcOOHTsQEhKC0NBQbNy4UW5z+/ZttG7dGjdv3sQPP/yAiIgI/P333xg/fnzRnggA9evXR+PGjbFy5UpkZ2drrAsPD0e1atU0Xuvr16/Lx/rt27cPEyZMwOHDhxESElLgDwG5ZWdno3PnztiwYQP+7//+D1u2bIGvr6/8euR0584duLq6Yu7cudi/fz8WLFiA9PR0BAQEyP8UtWjRAt999x0AYMaMGfLnccCAAXmOYciQIZg6dSp69OiBXbt2YdKkSdi6dStatGihddzLzZs3MXz4cLz//vvYsWMHgoOD8cknn2DLli2F2m+1uLg41K9fH4sWLcKBAwcwY8YMXLt2DU2bNtX6ZxAABg0aBDs7O0RGRmL69OnYu3evHCirDRw4EIsXL8aIESOwfft2dO7cGd26dUNqamqRxpiX8ePHY+TIkQgKCsLWrVuxbNky3Lx5E0FBQbh9+zYAVSpx+/btERcXh4ULF+LgwYMICwuDt7e3/NwePXoUdnZ2ctrwiRMn8j1mUF+hoaH4+OOPERQUhO3bt2PWrFk4fvw4AgMD5QBCn/H98ccfePPNN+Hs7Iy1a9diz549mDZtmsZ7PT09HW3atMH//vc/jBgxAnv27MHkyZMRGRmJjh07yj/U6NOXLurv9Z49e+bZRr1O3XbYsGGIjY1FbGysRrvjx4/j+vXrePfddwGoAtK+ffti6tSp6NWrF3bu3Il58+YhJiYGLVu21PoMXL58GaNHj8aIESOwd+9ejR/t8pKVlaV1yf1dA6iO/Y2NjcXSpUsRERGBp0+fok2bNhrHW27btg2dO3eGsbExVq9ejZUrVyIjIwNt27bV+FHrypUrCAgIwJEjRzBp0iTs3bsXc+bMAQCtz8Lo0aORlpaG1atXY8GCBfjtt9800ljv37+Pjh07IjMzE8uWLcP+/fsxe/Zs2NnZyRkURFoEUTkWHh4uAIhTp04V2CYuLk5j+dGjRwUAcfToUZ2P+/vvv4WXl5eoW7euePLkiRBCiEuXLgkA4sMPP9Roe/LkSQFATJw4UV4WHBwsAIiTJ09qtK1Tp47o0KFDvvv1wQcfCADi8uXLGsuVSqXIzMyUL1lZWRrre/bsKSwtLUVmZqYQQohu3bqJfv36CSGEWLJkiXB0dBRKpVIIIURISIhwcnLS2vaXX34pAIh9+/YJIYT466+/hCRJYtCgQRrt4uLiBAAxa9YskZmZKdLS0sTvv/8uAgICBACxe/duue2wYcOEsbGxuHnzZr77nZP69dm4caPO9ZUrVxampqZar2tu2dnZIjMzU/zwww/CxMREJCcny+v69u0ratWqJd9//vy5ACDc3NxESkqKvPz27dsCgJg/f7687H//+58AIBISEuRlTZs2FZIkidjYWHmZUqkUNWrUED169JCXjRkzRhgbG4tr165pjFX9njlx4kS++5QX9Zj27NkjLzt16pQAIL766qs8H6d+X124cEEAEIcPH5bXhYWFCQDi4cOH8rK33npL1K1bV76/ceNGAUAsX75co98JEyYIAGLOnDl5bjsrK0ukp6eLKlWqiMmTJ8vL83v9//Of/whLS0v5/unTpwUA8emnn2q0O3z4sAAgZsyYIS/z9/cXRkZG4s8//5SXZWdnC09PT/HWW2/lOc7CyMzMFE+ePBEmJiYaz4n6ucw9zilTpggAIikpSQjx4jXL+XwIIcT3338vAIjRo0frPZadO3cKAGLv3r1a686dOycAiGnTpmksf/DggbCxsZG/5y5fviwAiDVr1uS7rcqVKxfqObS0tBRDhgzJc/2tW7eEkZGRGDx4sMbyP//8UxgZGYmxY8fqPb6lS5cKAOLu3bt5tlG/PtHR0RrL9+3bJwCIDRs26N2XLqGhoQKAOH36dJ5t7ty5IwDI37fPnj0TlpaW8r6qDRs2TFSsWFE8f/5cCPHidV65cqVGuwsXLggjIyMxe/ZseVndunW1vqfyM3r0aAFA56Vp06ZyO/X71sfHR+Nv0/3794W5ubn8t0gIIWrUqCGqV68uMjIy5GVpaWnC3d1d1K9fX17WvXt3YWlpKe7cuZPn+NTfP7nfJ0uWLBEA5O9Z9esYExOj134TCSEEZ7QKcOzYMXTr1g1VqlSBJEnYtm1bofvYv38/AgMDYW1tDUdHR7z11luIi4srgdHSqyIlJQVdunRBWloa9u7dKx/Xop4Fyl1woEmTJqhduzYOHz6ssdzZ2RlNmjTRWFa/fv0ip9Vt374dJiYm8sXW1lZjfUhICFJSUnDq1Cn5+KzWrVsDAIKDg/Hw4UNcuHAB6enp+PXXX7Vms4QQcrqgOrWlWrVqaN26NTZv3qyzItZnn30GExMTmJmZwd/fH7dv38b333+Pzp07y22WL1+OrKwseHh4FGm/8+Lv76+R1ql26tQpdO3aFXZ2djA2NoaJiQlGjBiBzMxMjVSivLRt21bjeAV3d3dUrFhRr9fNw8MDfn5+8n1JkuDr66vx2OjoaDRq1EjrGLGCUo8K8s4778Dc3BwrVqyQl4WHh8PIyAhDhgzRaHvv3j0MHz4crq6uqFChgsbM7aVLlwq13aNHj+qsbKmr8EZ6ejq++OIL+Pj4wMTEBBUqVIBCocC9e/cKvV21I0eOAND+XL7xxhtwd3fX+lx6eXlppD4aGRlpvUaFkZiYiE8++QTVqlWTn8tKlSohMzNT5z7lnqmuX78+AMgzSOrvmdwzeIYoZJLT7t27AQCDBw/WmKWoVKkSAgIC5BnwqlWronLlypgyZQq+++47nD9/Xu9U4uKIjo6GUqnUel19fX3h7+8vv676jK9x48aQJAkDBw7E+vXrdc5O79q1C15eXmjevLnG8xESEgKFQiE/H/r0VVTqcauPm7WyskLv3r2xdu1aZGRkAFDN5GzcuBHvvPMOzMzM5LGbmZnh7bff1hh7zZo14eXlpVWRr0aNGhrfU/o4deqU1kXXrGW/fv1gbGws369cuTLeeOMN+X19584dXL9+HQMGDICJiYncTqFQoF+/fjh37hwePXoEpVIpZ1e4ubkVOL68Plfqz7Wvry/Mzc3x0UcfYcWKFbh27Vqh9p/KJwZaBUhJSYGfnx8WL15cpMf/9ddf6NGjB9544w3ExsZi//79ePToEXr16mXgkdKrIisrC71798bVq1exZ88ejepPjx8/BgC4uLhoPa5KlSryejVdFekUCkWBFdvUB/rn/sevdevW8h84XWXX1YHT0aNHcfbsWTx9+hTBwcEAgDp16sDR0RFRUVH49ddfdR6fdeTIEcTFxeHtt99GUlISnj59iqdPn6JPnz5ITU3F+vXrtbY5btw4nDp1Cr///jtu3LiBhIQEjBgxIt/9MxRdr8ONGzfkoDIsLAwxMTE4deoUvv32WwDQq1peUV83fR/7+PFjVK5cWaudrmWFYWtri969e2PHjh14/Pgx0tPTsX79erRv317jfZyZmYmQkBDs27cP//d//4cjR47g1KlTcsBS2IqCjx8/hq2trVYhEl2V7UaNGoUZM2agX79+2LNnD06ePIlTp07B29u7yJUMX9bnMi9vvvkmli9fjo8++ggHDhzAb7/9hlOnTsHKykpnn7m3r1AoALx43tXjzf38WVlZwdLSskhj1EV9/Fq1atU0fsAxMTHB4cOH5VRoc3NzREdHo1mzZpg6dSrq1asHFxcXfP755yWacqXv66rP+Pz9/bFnzx6Ymppi2LBhqFq1Knx8fBAeHq7xfPz1119az4VCoUB6err8fOjTly7q7/X8fqhVp1fn/LwOGzYMjx8/xs6dOwGoypk/e/ZMThtUjz0tLQ0WFhZa479+/bpWWruu57QgjRs31rroOj2Grs+9s7Oz/HoV9Lqq2zx79gxpaWl6BVlAwZ8rV1dXREVFwdPTEx9//DFq1qwJT09PzJ49+6X8cECvJ1YdLECnTp10nltILSMjA5MmTcLatWvx9OlT+Pr6YtasWfIswJkzZ5CdnY3p06fDyEgV14aGhqJHjx7IzMzU+DWGXk3qX/zS09M1luf+w6M2YsQIHD58GHv27NH6xU/9RZ6QkKD15X/v3j04ODgYZMzt2rXDxIkTsWPHDo2DnStWrCgfYK7rn0VfX185mFIoFKhcubLGH8JWrVrh6NGj8h+63IHW8uXLAQDffvutHJjkXj9y5EiNZW5ubgY56L0ocldLBIDNmzfj+fPn2L59u8Yf/KIUmCgp9vb2WkUaABjknFzDhw/H6tWrsXbtWjg5OeHJkydax/+cOnUKV69exaZNm/DWW2/Jy3MfB6Ive3t7JCYmalV9zL0/SqUS69atw8iRI/HFF19orHv48CFq1qxZ5O0Dqs9l7qqK9+7dQ61atYrUrz7u3buHo0ePYu7cufjPf/4jL3/69GmRzy+k3p/79+9rzFonJycX+XQEuqi/rw4dOqQ1Ow5A4+9brVq1sGbNGgghcOHCBaxduxbffPMNKlSogOnTpxtsTDnlfF1z/0Of+/tWn/F17NhRPkbn5MmTmDNnDoYNGwZ3d3e0bdsWDg4OqF69On766Sed47Gzs5NvF9SXLurl27ZtQ+/evXW2UWfd5CyW0apVK1SvXh3h4eF46623EB4ejrp16yIgIEBu4+DgAAsLC53FJABoBei6vjsNRdf32P379+XXM+frmpv6uDt7e3tYW1vDzMxMq9BUcTRp0gRbt25FdnY2YmNjsWTJEnz22Wews7PDe++9Z7DtUNnBGa1ievfdd/Hzzz/jp59+wrlz5/D222+jY8eO8pRy48aNYWxsjPDwcGRnZyMxMRGrV69G+/btGWS9JtSpZblPyrtjxw6ttpMmTUJ4eDh+/PFHnX8s33jjDQDQKmZx6tQpXLp0CW3atDHImBs3boz27dtj2bJlhSrzK0kSgoOD8csvv+DgwYPybJZacHAwoqOjcfToUVSpUkXjH9snT55g69atCAoKwtGjR7UuAwYMwKlTp3D+/HmD7GNJUVc8U/+aCaiKNfz444+lOCpNwcHBOHPmjFYaY17/4BW27xo1amDFihUIDw+Hvb29VkqN+p+snM8RAHz//fdF2mZISAiEEFoznuvWrdNqm/u1AVQnXM5d7TD3r9H5UX/ucn8uo6KicOfOHYN9LnUx9HMJvPgBZO3atRrLdT2fxdGlSxcAqplzXbMVulLL1KmwM2fOhJubG86cOSOvK86soC6tW7eGsbGx1ut68eJF/P777zpf1/zGp2ZiYoIWLVrIPyap23Tt2hW3b9+GlZWVzufDy8tL7750qV27Nnr16oX169fLxS5y+v333/Hdd9+hRYsWWt/dQ4cOxb59+/DLL78gOjpao4KoeuypqalISkrSOXb1aT1ehsjISI0iGX///TeOHDki/4Dt7u4Ob29v/PTTTxrt0tPTERkZCT8/Pzg4OMDIyAjt27fHjh07tConFpexsTH8/f2xdOlSGBkZ5fu6UfnGGa1iuHHjBtavX4+7d+/K09WhoaHYt28fwsPDMWPGDHh6euLAgQN4++23MXLkSGRnZ6NZs2bYs2dPKY+ecjpy5IjOinadO3dGQEAAatWqhdDQUPn4g61btyImJkaj7caNG/H111+jd+/eqFmzpsYMiEKhQMOGDVGrVi2MGDECYWFhMDIyQqdOnXDz5k1MnjwZ7u7u+OSTTwy2T2vWrEGHDh3Qtm1bDB06FB06dICTkxOSkpJw7tw5HDp0CDY2NlqPCwkJwaZNm3DgwAGtlNng4GA8fvwYx44d0zreY+3atUhLS8PYsWPlP4g52dvbY+3atVi+fDnmz59fqH0ZPnw4Vq5ciRs3bhj8OK3cOnTogIkTJ6Jv374YP348UlJSsHjxYoNXayuO0NBQrFq1Ch06dMC0adNgb2+P1atXy+9h9ew5oDphdKdOnTBz5kx8/vnnevU/bNgwTJw4EZIkYdy4cTA1NdVY7+fnBzc3N4wfPx6pqamwtrbGli1b8vw1vCA9e/ZEQEAAxowZg3/++Qd+fn6IiorSCryMjIzQuXNnLF26FNWqVYOPjw9OnjyJBQsWaKUb1apVCyYmJli5ciU8PDxgYWEBd3d3nemVjRo1Qv/+/fHNN98gKysLbdu2xbVr1zB58mTUqFEDH374YZH2q3Hjxrh8+XK+M1MuLi5o1KgRpk+fDisrK7i6uuLQoUNYv359kU/k2rhxY/Ts2RMzZsyAEAKtWrXC2bNnERYWVqjzxBWkQYMGGD9+PEaPHo0///wTb7zxBqytrZGQkIATJ07A09MT48ePx9GjRzFz5kz06tULXl5eMDY2xq5du3D37l2NSpn16tXDzz//jG3btsHNzQ2WlpYF/oN/69YtbNq0SWt5rVq1UK9ePYwbNw7ffvstFAoFevTogfj4eEyZMgWOjo747LPPAECv8c2cOROXLl1C+/bt4ebmhmfPnmHJkiUwMjKSf0AbOXIkNmzYgJCQEHzyySdo0KABJEnCnTt3cOjQIbz//vsICQnRq6+8LFu2DLdu3UK3bt3w0UcfoUOHDjAyMsKxY8fw7bffwsPDQ2dAPXToUEydOhV9+/aFsbGxVpXA7t2746233kKvXr0wduxYNGvWDAqFAvHx8YiOjsYbb7xR7GP88soKqF+/vsZ7PTk5GV27dsWHH36I1NRUfPnllwBUP2SqzZkzB7169UL79u0xZswYKJVKzJ8/H/Hx8RrnX5w9ezaaNWuGwMBATJgwAT4+Pnj06BF27dqF2bNnFyrdetWqVdi2bRu6du0KT09PZGRkYM2aNVAqlTrL7RMBYNXBwgAgtm7dKt/fsGGDACAsLS01LhUqVBB9+vQRQgiRkJAgvL29xX//+19x5swZER0dLYKDg0WbNm3k6m1UetQVBfO6qCvSXb16VbRv317Y2NgIR0dHMWbMGLF7926NqoNTp07Nsx8PDw95m9nZ2WLWrFmiZs2awsTERDg4OIiBAwdqVUUKDg7WqMymNmTIEI3+8pOWlibCwsJEixYtRMWKFUWFChWEnZ2daNmypZg1a5Z4/Pix1mMuXrwoj/v8+fMa65RKpbCzsxMAxLJlyzTWNWjQQDg5OYn09PQ8xxMYGCgcHBxEenq6XHUwv4pyOfc55+uhD32qDuZV4WzLli2iXr16wszMTLi5uYkJEyaI7du3a1X0y6vq4H/+8x+d2xs5cqR8P6+qg/7+/lqPzb0dIYSIjY0VISEhwszMTNjb24uRI0eKZcuWCQDiypUrcjt1Ra2IiAid+6rLvXv3hLGxsQAgzp07p7ONevtWVlbCzs5ODBgwQFy7dk3rNdWn6qAQQjx69EgMGjRI2NraCktLS9G5c2e5ql3O/h4+fCgGDRokHBwchKWlpWjdurX47bffhL+/v+jSpYtGnytWrBDe3t6iQoUKGv3krjoohKrS35dffimqV68uTExMhJOTk3j33XfF/fv3Ndr5+/trVErLb59q1aolatSoofP5yykuLk50795d2NraChsbG9GtWzdx9epVYW9vr1EhUP1cXrp0SePx6opxOaunpqamijFjxggHBwdhbm4uWrZsKc6cOaPVZ0HyqzqotmbNGhEUFCSsrKyEmZmZ8PLyEv379xe//PKLEEJVeXTAgAHC29tbWFhYCBsbG+Hv7y++//57jb+DFy9eFMHBwcLS0lIA0PlZyEndTtdF/RlUKpViwYIFwsfHR5iYmAg7OzvRt29f8ddff8n96DO+gwcPiq5duwo3NzdhamoqHB0dRfv27TUqbAqh+g748ssvRZ06dYRCoRDW1ta+b5CfAAAgAElEQVSibt26YsyYMfJ3vL595eX58+dizpw5olGjRsLS0lKYm5sLX19fMXXqVJGYmJjn49q3by8AaFQwzSk7O1uEhYWJRo0aCXNzc2FpaSlq1qwp3nvvPXHhwgW5Xd26dUVwcLBeYxUi/6qDAMTZs2eFEC+qDn7//ffiv//9r3B2dhYKhUIEBgaKn3/+WavfvXv3iqCgIGFubi4sLCxEcHCwzirA165dE/379xeOjo7CxMREuLm5iYEDB4pnz54JIV58R+Z+rHo8O3fuFEII8fvvv4vevXsLT09PYWZmJipVqiRatGiR598YIiGEkITgEXz6kiQJW7dulc9TERkZiQEDBuDChQsaFXIA1UHHzs7OmDx5Mvbu3YvTp0/L6+7evQt3d3ecOHGCZ0onIoMZPHgwdu7ciUePHsnfSWPHjsWuXbtw5coVpiu/RA8ePEDlypURERGhVbGRiLSdPn0aAQEBCA8P16oUSfS6YupgMTRs2BDZ2dl48OABWrZsqbNNamqqVhCmvl/Yk3oSEalNmTIFHh4e8PLyQlJSErZt24bVq1fj66+/1vjOOXr0KL744gsGWS/Z0aNH4ePjo9eJXImIqGxioFWA5ORkjQPO4+LiEBsbCzs7O9SsWRMDBgzA4MGDMW/ePDRs2BCPHj3CkSNHUK9ePXTu3BldunTB/Pnz8eWXX+Kdd97Bs2fPMHHiRHh4eKBhw4aluGdE9DozNjbGN998g/j4eCiVStSsWROLFy/G6NGjNdr9+eefpTTC8q1v377o27dvaQ+DiIhKEVMHCxAVFaVVwhoAhgwZgoiICGRmZmL69OlYtWoV4uPjYW9vj2bNmmHatGmoV68eAFUlsNmzZ+Pq1auwsLBAs2bNMGvWLJ3njyAiIiIiotcfAy0iIiIiIiIDK9XzaB07dgzdunVDlSpVIEmSfKK9vAwdOlQ+x03OS926deU2EREROtuU5NnniYiIiIiIcirVQCslJQV+fn5a5+rJy8KFC5GQkCBf7ty5Azs7O7z99tsa7WxsbDTaJSQkwMzMrCR2gYiIiIiISEupFsPo1KkTOnXqpHd7W1tb2Nrayve3bduGJ0+e4N1339VoJ0mS1skrC0OpVOLevXuwtraGJElF7oeIiIiIiF5vQgg8e/YMVapUgZGR/vNUr3XVweXLl6Nt27bw8PDQWJ6cnAwPDw9kZ2ejQYMG+Oqrr/Kt8Jeeno709HT5fnx8POrUqVNi4yYiIiIiotfLnTt34Obmpnf71zbQSkhIwN69e7Fu3TqN5T4+PoiIiEC9evWQlJSEhQsXIigoCH/88Qe8vb119jVz5kxMmzZNa/mdO3dgY2NTIuMnIiIiIqJXX1JSEtzd3WFtbV2ox70yVQclScLWrVvRs2dPvdrPnDkT8+bNw71792BqappnO6VSiUaNGqFVq1ZYtGiRzja5Z7TUT2ZiYiIDLSIiIiKiciwpKQm2traFjg1eyxktIQRWrFiBQYMG5RtkAYCRkRECAgJw7dq1PNsoFAooFApDD5OIiIiIiMqpUq06WFTR0dG4fv06hg8fXmBbIQRiY2Ph4uLyEkZGRERERERUyjNaycnJuH79unw/Li4OsbGxsLOzQ9WqVTFhwgTEx8dj1apVGo9bvnw5mjZtCl9fX60+p02bhsDAQHh7eyMpKQmLFi1CbGwsvvvuuxLfHyIiIiIiIqCUA63Tp08jJCREvj9+/HgAwJAhQxAREYGEhATcvn1b4zGJiYnYvHkzFi5cqLPPp0+fYsSIEbh//z5sbW3RsGFDHDt2DE2aNCm5HSEiIiKiEiOEQFZWFrKzs0t7KFRGmZiYwNjY2KB9vjLFMF4lRT3gjYiIiIgMKyMjAwkJCUhNTS3toVAZJkkS3NzcYGVlpbWuXBXDICIiIqKyT6lUIi4uDsbGxqhSpQpMTU0hSVJpD4vKGCEEHj58iLt378Lb29tgM1sMtIiIiIjolZSRkQGlUgl3d3dYWFiU9nCoDHN0dMTNmzeRmZlpsEDrtaw6SERERETlh5ER/2WlklUSM6V81xIRERERERkYAy0iIiIiIiIDY6BFRERERPQaCQwMxOeff653+8uXL0OSJFy+fLkER0W5MdAiIiIiIjIgSZLyvQwdOrRY/e/ZsweTJk3Su723tzcSEhLg7e1drO0WhAGdJlYdJCIiIiIyoISEBPl2ZGQkpkyZgitXrsjLzM3NdT4uMzMTJiYmBfZvZ2dXqPEYGxvD2dm5UI+h4uOMFhFRSbu4HVjVE/j7QmmPhIjotSeEQGpGVqlchBB6jdHZ2Vm+2NraQpIkrWXq2Z8tW7agZcuWUCgU2LRpE/7++2/06dMHrq6usLCwgJ+fHzZv3qzRf+7UQWdnZ8ydOxeDBw+GlZUVPD09ERERIa/PPdO0b98+SJKE6OhoNGzYEJaWlmjVqhVu3Lih8TxPmTIFDg4OsLW1xQcffIDx48cjMDCwGK8esGjRIlSrVg2mpqaoXbs2IiMjNbb5f//3f3B3d4dCoYCbmxtCQ0Pl9QsWLED16tWhUChQuXJl9O/fv1hjKWmc0SIiKilKJRA1Ezg2W3X/j5+A9l+V7piIiF5zzzOzUWfK/lLZ9sUvO8DC1LD/Pn/22WeYO3cu6tevD3Nzczx//hzNmzfHxIkTYW1tje3bt6Nv3744ffo0GjRokGc/s2bNwowZMzBlyhSsW7cO77//PoKDg1GtWrU8HzNp0iSEhYWhUqVKGD58OEaMGIHDhw8DAFasWIF58+Zh6dKlaNq0KVatWoWwsDDUrl27yPu6fv16fPrppwgLC0NwcDC2bNmC/v37o2rVqmjWrBnWrl2L//3vf/jpp5/g4+ODhIQEXLig+pEyJiYGn376KdauXYsmTZrg8ePH+OWXX4o8lpeBgRYRUUlITwa2jgQu73qxLPlB6Y2HiIheSaGhoejRo4fGso8//li+PX78eOzevRubNm3KN9Dq2bMn3n//fQCqAOrbb79FdHR0voHWN998g6CgIADAp59+ij59+iA7OxvGxsYICwvDqFGjMGjQIADA9OnTsW/fviLvJwDMnTsXI0aMkMf5+eef45dffsHcuXOxefNm3L59G66urmjTpg2MjY1RtWpVNG3aFABw+/Zt2NjYoEuXLrCwsICHhwcaNWpUrPGUNAZaRESG9vQ2sP4d4O/zgLEpUKMdcGU3kPx3aY+MiOi1Z25ijItfdii1bRta48aNNe5nZWVhxowZ2LhxI+Lj45GRkYH09HS4urrm20/9+vXl20ZGRqhcuTIePMj/B76cj3FxcUF2djYeP34MJycnXL16FRMnTtRo36RJE5w5c0bfXdNy+fJlfPrppxrLgoKCsHLlSgBAv3798N1338HLywsdO3ZEly5d0KVLFxgbG6Nz58746quvUK1aNXTs2BEdO3bEm2++CTMzsyKPp6Qx0CIiMqRbJ4DIgUDqI8DSCei3FkhP+jfQ4owWEVFxSZJk8PS90mRpaalxf8aMGfjuu++wYMEC1KlTB5aWlhg1ahQyMjLy7Sd3EQ1JkqBUKvV+jCRJAAClUikfi6ZepqbvMWq65NenepmXlxeuXbuGAwcO4NChQ3j//fdRu3ZtHD58GBUrVsS5c+dw5MgRHDx4EBMnTsRXX32FkydPwtrausjjKkkshkFEZChnVgEru6mCLOf6wIijgHsTwKqyaj1ntIiIqADHjx9H79698c4778DPzw+enp64du3aSx2DJEmoWbMmfvvtN43lp0+fLlafPj4+iImJ0Vj+yy+/aBz3ZWFhgZ49e2Lx4sU4cOAAoqOj5YqNJiYm6NChA+bOnYuzZ8/i8uXLOH78eJHHVNLKzs8BRESl6fcIYOc41e06PYGeSwDTf3+lVAdaqY+B7CzAmF+9RESkW40aNbBv3z55pmbWrFl48uTJSx/HmDFjMG7cODRo0AABAQFYs2YNrl69ijp16hT42MuXLyMtLU1jma+vL/773/9i6NChqF+/vlwMY/fu3XLw9eOPP6JChQoICAiAubk51q5dCysrK7i7u2PLli1ISEhAixYtYGtri23btsHIyKjEzw1WHPxrT0RkCBe3q66bjAA6zQZypkZY2AOSESCUqtkua57LhIiIdPvyyy9x584dtGnTBtbW1vjwww/RqVOnlz6OYcOG4ebNmxg7diwyMzPRv39/9O/fX6+TEb/55ptayxISEtCvXz88ePAAX3/9NT788ENUr14da9euRbNmzQAAtra2mDNnDi5fvgwhBOrXr4/du3fD2toalSpVwoIFCzB58mSkpaWhVq1a2Lhx4ysdaEmiOMmWZVRSUhJsbW2RmJgIGxub0h4OEb0Ovg8GEmKB/huAmjoO0p5bU5U6OPIY4OL38sdHRPQaSktLQ1xcHKpVq/ZKFz0oL1q2bAkfHx8sW7astIdicPm914oaG3BGi4jIEJ7/m9Zhbqd7vaWTKtBiQQwiInoNJCYmYuXKlWjXrh0AYNWqVYiJicGMGTNKeWSvDxbDICIyBDnQqqR7vZWT6pqBFhERvQYkScK2bdsQFBSEgIAAHDx4EDt27EDLli1Le2ivDc5oEREVV3amqoQ7AFjkMaPFyoNERPQasbGxwZEjR0p7GK81zmgRERWXejYLEmBmq7sNZ7SIiIjKFQZaRETFpQ60zGwBI2PdbeRAizNaRERE5QEDLSKi4kr9R3WdV9ogkCN1kDNaRERE5QEDLSKi4nr+b6CVV8VB4MWMVgoDLSIiovKAgRYRUXEVakaLqYNERETlAQMtIqLiKqi0O/BiRistEchMK/kxERERUalioEVEVFz6pA6aVQSMTVW3mT5IRER6GjhwIHr37i3fb9GiBUJDQ/N9jJubGxYvXlzsbRuqn/KKgRYRUXHpkzooSYCluvLgw5IfExERlZpu3bqhbdu2OtedOHECkiThzJkzRep7x44dmDp1anGGp+XHH3+Eg4OD1vKzZ89i2LBhBt1WbocOHYIkSUhOTi7R7ZQGBlpERMUlz2jlkzoIsMQ7EVE5MXz4cBw5cgS3bt3SWrdixQo0aNAAjRo1KlLfdnZ2sLa2Lu4Q9eLo6AgLC4uXsq2yiIEWEVFxPX+qui4w0GJBDCKiYhMCyEgpnYsQeg2xa9eucHJyQkREhMby1NRUREZGYvjw4QCAzMxMDBs2DJ6enjA3N0etWrUQFhaWb9+5Uwfv37+Prl27wtzcHF5eXvjpp5+0HjNnzhz4+vrCwsIC7u7u+Oijj5CSkgJANaP0/vvv4/Hjx5AkCZIkYfr06QC0Uwdv3ryJ7t27w9LSEra2tujXrx8ePnyRpTFp0iQ0btwYK1euhIeHBypWrIgBAwYUa7ZKqVRi6tSpcHV1hUKhQKNGjXDw4EF5fXp6OkaNGgUXFxeYmZnB09MTs2fPBgAIITB58mRUrVoVCoUCrq6u+OSTT4o8lsKq8NK2RERUVumTOggAVo6qa55Li4io6DJTgRlVSmfbE+8BppYFNqtQoQIGDx6MiIgITJkyBZIkAQA2btyIjIwMDBgwAACQnZ2NqlWrYtOmTbC3t0dMTAxGjhwJV1dX9OrVS68hDR48GA8ePEBUVBSMjIwwduxYPH78WGs8ixcvhqenJ27cuIFRo0bByMgIixYtQqtWrTBv3jx8/fXXuHDhAgDonDFTKpXo3r077OzscPz4cWRkZGDUqFF45513cOjQIbndlStXsHv3buzevRuPHz9Gnz59MGfOHEybNk2v/clt3rx5WLhwIX744Qf4+flh2bJl6Nq1Ky5dugQvLy/Mnz8fe/fuxcaNG+Hu7o7bt28jPj4eABAZGYmwsDBERkaidu3aSEhIwPnz54s0jqJgoEVEVFz6FMMAXsxosRgGEVGZN2zYMMyZMwdRUVEICQkBoEob7NWrFypVUmVAmJmZ4YsvvpAfU61aNcTExGDDhg16BVoXL17EwYMHcfr0afj7+wMAli1bhnr16mm0yzmL4+npiWnTpuGTTz7BokWLYGpqChsbG0iSBGdn5zy3tX//fly6dAk3b96Eq6srAGDlypXw8/PD2bNn0bBhQ7lteHg4LC1VAemAAQNw+PDhIgdac+fOxcSJE9GnTx/5/pEjR7Bw4UIsXLgQt2/fRs2aNREUFARJkuDh4SE/9vbt26hSpQratGmDChUqoGrVqmjatGmRxlEUDLSIiIpLn/LuAFMHiYgMwcRCNbNUWtvWk4+PD5o3b44VK1YgJCQEN27cwPHjx3HgwAGNdkuWLMGKFStw69YtPH/+HBkZGWjcuLFe27h06RJMTU01jvfy9fXVmpE6dOgQZs6cicuXLyMxMRHZ2dlIS0tDeno6FAqF3tvy9PSUgywAqF+/PqysrHDp0iU50PLy8pKDLABwcXHBgwdF+4Hxn3/+wYMHDxAUFKSxPCgoCJcuXQIAvPvuu2jfvj18fHzQsWNHjUIkffv2xaJFi+Dl5YWOHTuic+fO6NatG4yNjYs0nsLiMVpERMWRkQpk/XterAJTB9XFMDijRURUZJKkSt8rjcu/KYD6Gj58ODZv3oykpCSEh4fDw8MDbdq0kdevW7cOoaGheO+993DgwAHExsZi8ODByMjI0Kt/IYSclph7uVpcXBy6du2KBg0aYMuWLThz5gwWLVoEQHWMmL7y2hYAjeUmJiZa65RKpd7byb3N3P3nHktAQABu3ryJadOmISUlBW+99Rb69esHAPDw8MC1a9cQFhYGhUKBDz74AK1bt0ZWVlaRxlNYDLSIiIpDnTZoZAKYWuXfljNaRETlSp8+fWBsbIx169Zh5cqVePfddzWChuPHj6Nly5b44IMP0LBhQ9SoUQPXr1/Xu/86deogPT0dZ8+elZdduHBBo/jEb7/9BkB1rFPTpk1Rs2ZN+RgmNVNTU2RnZxe4rbi4ONy792I28dy5c0hOTkbt2rX1HnNh2Nvbw8nJCTExMRrLf/nlF41tqgtz/Pjjj1i3bh0iIyORlJQEADA3N0ePHj0QFhaGw4cPIyYmBhcvXiyR8ebG1EEiouLIWQijoF86LVkMg4ioPLGyskLfvn0xceJEJCYmYujQoRrra9SogfXr1+PgwYPw8PBAREQEzp49C29vb736r1OnDtq2bYv33nsPS5cuhZGREcaNGwczMzONbaSnp2Px4sXo3Lkzjh8/jh9++EGjH09PTyQmJiIqKgq+vr6wtLSEubm5RpsOHTqgdu3aGDBgAL799lukp6fjww8/RJs2bdCgQYOiPUE5/PnnnxrblCQJfn5++O9//4vp06ejWrVqqF+/Pn788UdcuHABmzZtAqA6Zsvd3R0NGjSAJEnYtGkTXF1dYW1tjRUrVkCSJDRp0gTm5uZYs2YNLCwsULVq1WKPVx+c0SIiKg59j88CXsxoZaYC6WXvxIxERKRt+PDhePLkCdq2bav1D/7o0aPRvXt3vP322wgMDERSUhJGjhxZqP5XrVoFZ2dntGrVCr1798bo0aNhb28vr/f398ecOXPw9ddfw9fXF5GRkZg5c6ZGHy1btsR7772H3r17w9HREfPmzdPajpGREXbs2AErKyu0aNECHTp0QM2aNbF+/fpCjTcvzZs3R8OGDeWLurjH+PHjMW7cOHz88ceoV68eDh8+jJ07d8LLywuAKpidMWMG/P39ERAQgLt372L37t2QJAm2trZYunQpmjdvDj8/P0RHR2PXrl2oWLGiQcZcEEkIPU8IUI4kJSXB1tYWiYmJsLGxKe3hENGr7MJWYONQoGpzYNjegtt/XQXITAHGnAHsq5f48IiIXmdpaWmIi4tDtWrVNGZpiAwtv/daUWMDzmgRERWHvufQUmNBDCIionKBgRYRUXHI59DSI3UQyBFosSAGERFRWcZAi4ioOJ4/VV0XNtBKeVgy4yEiIqJXAgMtIqLiKHTqIEu8ExERlQelGmgdO3YM3bp1Q5UqVSBJErZt25Zv+6ioKEiSpHW5fPmyRrvNmzejTp06UCgUqFOnDrZu3VqSu0FE5ZmcOshAi4iopLB2G5W0kniPlWqglZKSAj8/PyxevLhQj7ty5QoSEhLkS85zDZw4cQJ9+/bFoEGD8Mcff2DQoEHo06cPTp48aejhExG9mNEq9DFaLIZBRFQQExMTAEBqamopj4TKuoyMDACAsbGxwfos1RMWd+rUCZ06dSr045ycnPKsf79gwQK0a9cOEyZMAABMmDAB0dHRWLBgQZ51/tPT05Geni7fV59JmoioQOrzaOmbOmjJQIuISF/GxsaoWLEiHjxQfWdaWFhAKujk8ESFpFQq8fDhQ1hYWKBCBcOFR6UaaBVVw4YNkZaWhjp16mDSpEkICQmR1504cQKffPKJRvsOHTpgwYIFefY3c+ZMTJs2rcTGS0RlWJFTBxloERHpw9nZGQDkYIuoJBgZGaFq1aoGDeRfq0DLxcUFP/zwA/z9/ZGeno7Vq1ejTZs2iIqKQqtWrQAA9+/fR+XKlTUeV7lyZdy/fz/PfidMmIDx48fL95OSkuDu7l4yO0FEZYdSWfgZrZzl3YUA+MssEVG+JEmCi4sLnJyckJmZWdrDoTLK1NQURkaGParqtQq0atWqhVq1asn3mzVrhjt37mDu3LlyoAVAKxIVQuQbnSoUCigUCsMPmIjKtvQkQChVtwt7jJYyUxWk6RugERGVc8bGxgY9foaopL325d0DAwNx7do1+b6zs7PW7NWDBw+0ZrmIiIpNnTZoYglU0PPHmgoKwMxWdZvpg0RERGXWax9onT17Fi4uLvL9Zs2a4eDBgxptDhw4gObNm7/soRFRWZdayLRBNfVxWikMtIiIiMqqUk0dTE5OxvXr1+X7cXFxiI2NhZ2dHapWrYoJEyYgPj4eq1atAqCqKOjp6Ym6desiIyMDa9aswebNm7F582a5j3HjxqFVq1aYNWsWevToge3bt+PQoUOIiYl56ftHRGXc80KWdlezqgw8usoZLSIiojKsVAOt06dPa1QMVBekGDJkCCIiIpCQkIDbt2/L6zMyMhAaGor4+HiYm5ujbt262L17Nzp37iy3ad68OX766SdMmjQJkydPRvXq1REZGYmmTZu+vB0jovJBXQij0IFWjoIYREREVCZJgqfa1pKUlARbW1skJibCxsamtIdDRK+qX5cC+z4D6r4JvB2h/+P2TQB+XQIEjQPafVliwyMiIqLiK2ps8Nofo0VEVGoKew4tNUtH1XXyQ8OOh4iIiF4ZDLSIiIoqtRjHaAFMHSQiIirDGGgRERVVYU9WrCYHWiyGQUREVFYx0CIiKqqipg6yGAYREVGZx0CLiKio1KmDhZ7R+jfQSn0EKLMNOyYiIiJ6JTDQIiIqqqKWd7dwACABQgmkPjb4sIiIiKj0MdAiIioqOdAq5IyWcQXA0kF1m+mDREREZRIDLSKiosjOBNKTVLcLmzoIsPIgERFRGcdAi4ioKNSzWZAAM9vCP14uiMHKg0RERGURAy0ioqJQB1pmtoCRceEfb8lAi4iIqCxjoEVEVBRFrTioxhktIiKiMo2BFhFRURT1HFpqPEaLiIioTGOgRURUFOoZrcKWdldjoEVERFSmMdAiIioK9TFaRU4ddFRdpzw0zHiIiIjolcJAi4ioKJg6SERERPlgoEVEVBTFLobxb6D1/AmQlW6YMREREdErg4EWEVFRPC/mMVpmFQEjE9Vtpg8SERGVOQy0iIiK4vlT1XVRAy0jI5Z4JyIiKsMYaBERFUVxUwcBwPLfghgMtIiIiMocBlpEREVR3GIYQOkUxBACCO8CLGkGZGW8vO0SERGVMwy0iIiKQl3evaipg0DppA4+vQXcigEeXAQeX3t52yUiIipnGGgRERVWRiqQlaa6XZzUwdKY0bp7+sXtJ7de3naJiIjKGQZaRESFpU4bNDIBTK2K3o96RivlJc5oxZ95cfvJzZe3XSIionKGgRYRUWGl5ijtLklF76c0Ugfjc85o3Xx52yUiIipnGGgRERWW+vis4qQNAi8/dTA7E0j448X9p0wdJCIiKikMtIiICssQFQeBHIHWS5rR+vvCi2PLAM5oERERlSAGWkREhWWIc2gBL1IHM5KBjJTi9aUPddpgpWqq6ye3VOXeiYiIyOAYaBERFZY8o1WxeP2YWgEVzFW3X8aslroQRp0egGQEZD3nyZKJiIhKCAMtIqLCev5UdV3c1EFJerkFMdSl3as2A2xcVbeZPkhERFQiGGgRERWWoVIHgZdXECMtEXh0VXXb1R+o5Km6zUCLiIioRDDQIiIqLEMVwwBe3rm07p0FIICKVQErR6CSh2r5y6g8KASQGM/jwYiIqFxhoEVEVFg5z6NVXC8rdTD+d9W1a2PVdUVP1fXLmNE6uxqYXwf4bVnJb4uIiOgVwUCLiKiwDHUeLeBF6mBSfPH7ys9ddaDlr7p+mamDN2NU1/fOlvy2iIiIXhEMtIiICsuQqYMufqrra4cAZXbx+9NFiBel3d3+ndGSA62XkDr4+LrquqTTI4mIiF4hDLSIiApDqXwxo2WI1MHqbwBmtkDyfeDWL8XvT5ekeFWxDckYcK6vWqY+RispHshKL5ntAqogTw60HpbcdoiIiF4xDLSIiAojPQkQStVtQ6QOVlAAtburbp/fVPz+dFGXda9cFzC1UN22dARMLAAI4OmdktkuAKQ+VlU8BIBkBlpERFR+MNAiIioMddqgiaUqSDIE37dU1xe3A9mZhukzJ3UhDHXaIKA6h5c6ffDpTcNvU009mwWoZrRYeZCIiMoJBlpERIWRasBCGGrVWgGWTqqUxBtHDdevWnyuQhhqFf9NHyzJghg5Ay1lJpD2tOS2RURE9AphoEVEVBhyIYyKhuvTyBio21N1+/xmw/ULANlZL6r9uTbWXPcyKg/mDLQApg8SEVG5wUCLiKgw5EIYBpzRAgDf3qrry7uAzOeG6/fhZSAzFTC1Bhy8Nde9jMqDuQMtFsQgIqJygoEWEVFhqE9WbMjUQQBwC718AocAACAASURBVABs3YGMZODaAcP1K6cNNlTNnOVU6WWkDt7494akumKJdyIiKidKNdA6duwYunXrhipVqkCSJGzbti3f9lu2bEG7du3g6OgIGxsbNGvWDPv379doExERAUmStC5paWkluStEVF4Y8hxaORkZAb69VLf/NGD1QfX5s3IfnwWU/IyWUvki0HL2VV0zdZCIiMqJUg20UlJS4Ofnh8WLF+vV/tixY2jXrh327NmD33//HSEhIejWrRvOnj2r0c7GxgYJCQkaFzMzs5LYBSIqb9QzWoY4h1Zu6uqD1w4AaUmG6TP+jOo69/FZwItiGOmJL1IiDSnpLpCdDhiZvAj0mDpIRETlRIXS3HinTp3QqVMnvdsvWLBA4/6MGTOwfft27Ny5Ew0bNpSXS5IEZ2dng42TiEj2vASqDqo51wfsvYHH14ArewC/fsXrLz0ZeHBRdVvXjJapharaYcoDVfqgoYPHR9dU13ZegLWL6jZTB4mIqJx4rY/RUiqVePbsGezsNP/hSU5OhoeHB9zc3NC1a1etGa/c0tPTkZSUpHEhItKppFIHAdW5rdSzWoaoPpgQqzq5so0rYOOiu01JVh5Upw3a1wAsHVS3Ux4ZfjtERESvoNc60Jo3bx5SUlLQp08feZmPjw8iIiKwY8cOrF+/HmZmZggKCsK1a9fy7GfmzJmwtbWVL+7u7i9j+ET0OirJ1EHgRaB148iLbRWVXAijUd5tSvI4LXXFQfvqqpkzAEjmjBYREZUPr22gtX79enzxxReIjIyEk5OTvDwwMBADBw6En58fWrZsiQ0bNqBmzZoICwvLs68JEyYgMTFRvty5c+dl7AIRvY6el1DVQTXHmoBzPUCZBVzcXry+7qoLYeg4PkutJCsPyoFWDcDq3+9ppg4SEVE58VoGWpGRkRg+fDg2bNiAtm3b5tvWyMgIAQEB+c5oKRQK2NjYaFyIiHR6/lR1XRKpg2qGSh+UC2HoOD5LrURTB3MEWpaOqttMHSQionLitQu01q9fj6FDh2LdunXo0qVLge2FEIiNjYWLSx7HJxAR6Ss7E0j/9xjOkprRAoC6/5Z5vxkDJCXk3S4zDcjO0r3u2X1V1T/JCKjSUHcb4EWg9dTAqYNZ6cDT26rbOQOtjGQgI9Ww2yIiInoFlWqglZycjNjYWMTGxgIA4uLiEBsbi9u3VX+cJ0yYgMGDB8vt169fj8GDB2PevHkIDAzE/fv3cf/+fSQmJsptpk2bhv379+Ovv/5CbGwshg8fjtjYWHzwwQcvd+eIqOyRS6BLgJltyW2nkgfg1gSAAC5s1V7/TxywfTQwowrwbW3g50WqCoM5qY/PcvQBFFZ5b0td4v3pbUCZbZDhy2OEAEytVWmDCmugwr+n2WD6IBERlQOlGmidPn0aDRs2lEuzjx8/Hg0bNsSUKVMAAAkJCXLQBQDff/89srKyMHr0aLi4uMiXcePGyW2ePn2KESNGoHbt2mjfvj3i4+Nx7NgxNGnS5OXuHBGVPepAy8wWMDIu2W3V6626zpk++OQmsP0jYHFj4OwaQGSrgpaDk4EF9YBjc4C0f394upvPiYpzsqmiOs+VMgtIijfc+HMWwpAk1YXpg0REVI6U6nm0WrduDSFEnusjIiI07kdFRRXY5/z58zF//vxijoyISIfUEi6EkVOdnsC+z4H4/2fvvsOjKrc9jn8nvUBCDwFC7yBFQNoBBQQEC1ixYQMVu2I5B8u56vHIsaOCKCpipQkCIohgARVE6UU6SBASQigJCaTP/ePNpJCeTLKn/D7PM8/emezZs3LulWTNWu9618H+lbDtS9j0hUmIAFpeDP0fNwnNz6/Bif3wwwuw+m3oOc60HULJiZaPL9RoDCf2mcmDNRo7J/6867McQutCwiFNHhQREa9gaaIlIuJWKnMPrXNVj4Cm/eDASvjkitznWwyCi/4FUdlV+sa9oNP1sH0+rHoV4nfBypdyr29UzMRBh5pNshOtv6BZP+fEX1SiBWodFBERr+B2wzBERCxT2XtonatT7h6BNB8Ad3wHo+fnJlkOvn7m2nt/g2s/hojzzPOh9aBuu5LfpzImDzo2K67TKve5ao5E65jz3kdERMRFqaIlIlJajjVaVdE6CND5RjM1sFYLaNyz5Ot9fKDDSGg/AqLXmAqSbyn+ma+MyYN512g55GxarERLREQ8nxItEZHSqsrWQTCJU5cby/46mw2a9Cn99TWcvGlxSkJue2CtvImWKloiIuI91DooIlJaVd06WFWc3TroqGZVi4CgPBvAV8uuaCnREhERL6BES0SktM5W4dTBquRItJKPQVpyxe/nWJ+VdxAG5Fa0NHVQRES8gBItEZHSsNvh9FFz7mkVreAauRswn3TCOq3C1meBWgdFRMSrKNESESlJSiJ8eQf8/bv5+txKjSdwZvtgYaPdIbd18OwJyEyv+PuIiIi4MCVaIiLFidkM0y40+1T5+MHQF6FBF6ujcj5nTh4sKtEKrmmmKAIkx1f8fURERFyYpg6KiBTGbod1H8K3EyAzDcKj4JqPIKqH1ZFVDmdNHrTbi16j5eMLIXXMRMLkYxAWWbH3EhERcWFKtEREzpWSAIsehD8XmK/bDIcRUzxvCEZezmodTDoKaUmmcuW4Z17V6mUnWhqIISIink2JlohIXjFbYM4tcPKAaRUc/Dz0utfsTeXJnJVoOdoGazQGv8CC3w+tY47atFhERDycEi0REQe7HeaMNslGeGO49iNo1N3qqKpGTqJ10PzvUN7EMmd9VqvCvx+qvbRERMQ7KNESEXFI+NskWT5+cPdKz24VPFd4FGCDjLNmn6vqEeW7T1GDMBxyNi1W66CIiHg2TR0UEXE4vN4cIzp4V5IF4BcA4Y3MeUXaB3MGYbQo/PuO1kFNHRQREQ+nREtExMGRaDXsZm0cVnFMHqzIiPeSKlqO1sEkVbRERMSzKdESEXE4stEcG5xvbRxWqehAjMwMOHHAnBeZaNU1R7UOioiIh1OiJSICkJWZm2h5a0WroonWqYOQlQ5+QRDWsPBrqjkSLbUOioiIZ1OiJSICEL/H7P/kHwp121gdjTVqOjYtLmfroGN9Vq0W4FPEr5e8Uwezssr3PiIiIm5AiZaICOSuz2rQBXx8rY3FKhWtaOWszypiEAbkDsPIyoCUU+V7HxERETegREtEBPIMwvDS9VmQm2glHoaM1LK/vqRBGGA2MQ4KN+faS0tERDyYEi0REYAjG8zRWwdhgBlU4R8C2GHbfLNxcVmUJtECTR4UERGvoERLRCQ9BWK3mXNvHYQBYLNBq8HmfME4mHkDnDpU+tfn7KFVUqLlGIihipaIiHguJVoiIke3mWl5IbWhRmOro7HWldOg/+Pg4w+7l8KUnrBmihndXpy0M5D4tzmv06r4a6sp0RIREc+nREtE5HB222DDbqaq4838g2Dg0zDuF4jqBenJsOxJ+GBg7vj7wpzYb47BNSGkVvHv4ahoqXVQREQ8mBItEZGcQRhe3DZ4rnpt4falcPmbZnhFzGZ4fyAsHg/bF0DcDshIy72+tOuzIP+IdxEREQ/lZ3UAIiKW0yCMwvn4QLfboM1w+HYCbPsS1n1oHgA2X6jVDOq0yR3VXppES62DIiLiBZRoiYh3S0mA+N3m3JtHuxenWj245kPoehNsmQvxu+DYbkg7bSpZjmoWlLKipdZBERHxfEq0RMS7OdYd1Wicu5muFK7FQPMAM/r9dAwc22US1WO7IP0MnH9LyfdR66CIiHgBJVoi4t3yDsKQ0rPZIKyBebQYULbXqnVQRES8gIZhiIh30yCMqudoHUw/A6lJ1sYiIiJSSZRoiYh3c7QOahBG1QmoBn7B5lxVLRER8VBKtETEeyXGQOJhsPlAZGero/EeNltuVUuJloiIeCglWiLivRxj3eu2hcBq1sbibbROS0REPJwSLRHxXjmDMNQ2WOUckwc14l1ERDyUEi0R8V4ahGEdxyh9VbRERMRDKdESEe9kt+e2DmoQRtWrpr20RETEsynREhHvdGI/pCSAbyBEdLA6Gu+j1kEREfFwSrRExDs52gYjO4Gvv7WxeCO1DoqIiIdToiUi3ilnEIbWZ1lCrYMiIuLhlGiJiHfSIAxrOfbRUuugiIh4KEsTrVWrVnH55ZfToEEDbDYbCxYsKPE1K1eupFu3bgQFBdG8eXPefffdAtfMmzeP9u3bExgYSPv27fnqq68qI3wRcVeZ6RC7xZxrEIY1HGu0Uk5BRpq1sYiIiFQCSxOt5ORkOnfuzOTJk0t1/YEDBxg+fDj9+vVj48aNPPnkkzz44IPMmzcv55o1a9YwatQoRo8ezebNmxk9ejTXXXcda9eurawfQ0TcTdyfkJECQeFQq7nV0Xin4Jpg8zXnZ+KtjUVERKQS2Ox2u93qIABsNhtfffUVI0eOLPKaf/7znyxatIgdO3bkPDdu3Dg2b97MmjVrABg1ahSJiYksXbo055pLLrmEmjVrMnPmzFLFkpiYSHh4OAkJCYSFhZXzJxIRl7VuOix+BJpfBLcstDoa7/Vqa0g6CnethAZdrI5GRESkUOXNDdxqjdaaNWsYMmRIvueGDh3KunXrSE9PL/aa1atXF3nf1NRUEhMT8z1ExINpEIZrcLQPJquiJSIinsetEq3Y2FgiIiLyPRcREUFGRgbx8fHFXhMbG1vkfSdOnEh4eHjOIyoqyvnBi4jrUKLlGqplD8RI1kAMERHxPG6VaIFpMczL0fmY9/nCrjn3ubwmTJhAQkJCzuPQoUNOjFhEXEZqEnzzKMRtN19rEIa1NHlQREQ8mJ/VAZRF/fr1C1Sm4uLi8PPzo3bt2sVec26VK6/AwEACAwOdH7CIuI6Da2DBPXDygPm636MQFmltTN7OkWhpLy0REfFAblXR6t27N8uXL8/33HfffUf37t3x9/cv9po+ffpUWZwi4kLSz8Kyp+CjYSbJCmsEoxfAoH9bHZko0RIREQ9maUUrKSmJvXv35nx94MABNm3aRK1atWjcuDETJkzg8OHDfPLJJ4CZMDh58mTGjx/PnXfeyZo1a/jwww/zTRN86KGH6N+/Py+99BIjRoxg4cKFrFixgl9++aXKfz4Rsdjf62HBOIjfbb7ucjNc8qIZ6y7Wq5Y9DEOtgyIi4oEsTbTWrVvHgAEDcr4eP348ALfeeiszZswgJiaG6OjonO83a9aMJUuW8MgjjzBlyhQaNGjAW2+9xdVXX51zTZ8+fZg1axZPP/00zzzzDC1atGD27Nn07Nmz6n4wEbHez6/BD/8FeyZUi4DL34I2l1gdleSlqYMiIuLBXGYfLVeifbRE3NzJv+DNzua849Uw/FUIqWVpSFKII5tg2oUmEX5st9XRiIiIFKq8uYFbDcMQESmVU9mV8Nqt4Jrp1sYiRauWp6KVlQU+brVsWEREpFj6rSYinsex5qd6fWvjkOKF1DFHeyacPWltLCIiIk6mREtEPE/SUXOsVvS2DuIC/AIgqIY516bFIiLiYZRoiYjnUaLlPnLaBwsZ8Z5wGH54weyBJiIi4ma0RktEPM9pR6JVz9o4pGShdc34/bwj3lMS4JdJ8Ns7kJEC+36EO7+3LkYREZFyUKIlIp5HFS33kXfT4ow0WP8RrHwJzhzPveZ0jDWxiYiIVIASLRHxPI7qiCpars/xf6NdS+D3aXBiv/m6TmvoeTd886hJwux2sNmsi1NERKSMtEZLRDyPKlruw1HROrDKJFmh9eCyN+CeNdDlJvO9zDRIPW1djCIiIuWgipaIeJbM9Ny2MyVarq92C3P0D4E+D0Kf+yGwunnO1w8CqkFakqlqBWkDeRERcR9KtETEsyTHA3aw+UJILaujkZK0Hwk3hUP986B6IYlxaJ3sRCs+NykTERFxA2odFBHP4mgbDK0LPr7WxiIl8/GFVhcXnmRB/mEZIiIibkSJloh4Fg3C8CyOROtMvLVxiIiIlJESLRHxLBqE4VlCapujKloiIuJmlGiJiGdRouVZcloHVdESERH3okRLRDyLo3WwqDU/4l60RktERNyUEi0R8SyqaHkWVbRERMRNlSvR+vbbb/nll19yvp4yZQpdunThxhtv5OTJk04LTkSkzDQMw7OEOtZoKdESERH3Uq5E6/HHHycxMRGArVu38uijjzJ8+HD279/P+PHjnRqgiEiZqKLlWdQ6KCIibqpcGxYfOHCA9u3bAzBv3jwuu+wyXnzxRTZs2MDw4cOdGqCISJko0fIsOePdj0NWFvio411ERNxDuX5jBQQEcObMGQBWrFjBkCFDAKhVq1ZOpUtEpMqlJkFakjlX66BncIx3t2dCyilrYxERESmDclW0/vGPfzB+/Hj69u3L77//zuzZswHYvXs3jRo1cmqAIiKllpy9Pss/BAKqWRuLOIevPwTVMElW8jEIqWV1RCIiIqVSrorW5MmT8fPz48svv2Tq1Kk0bNgQgKVLl3LJJZc4NUARkVLLOwjDZrM2FnEerdMSERE3VK6KVuPGjVm8eHGB5994440KByQiUm5an+WZQuvC8T1KtERExK2Uq6K1YcMGtm7dmvP1woULGTlyJE8++SRpaWlOC05EpEw02t0zhdYxR414FxERN1KuROvuu+9m9+7dAOzfv5/rr7+ekJAQ5s6dyxNPPOHUAEVESk0VLc+kREtERNxQuRKt3bt306VLFwDmzp1L//79+eKLL5gxYwbz5s1zaoAiIqWmRMszaY2WiIi4oXIlWna7naysLMCMd3fsnRUVFUV8vD5xFBGLqHXQMynREhERN1SuRKt79+688MILfPrpp6xcuZJLL70UMBsZR0Tok2QRsYgqWp7J0Tp45ri1cYiIiJRBuRKtSZMmsWHDBu6//36eeuopWrZsCcCXX35Jnz59nBqgiEipqaLlmUIca7RU0RIREfdRrvHunTp1yjd10OGVV17B19e3wkGJiJRZVlaeREsVLY+i1kEREXFD5Uq0HNavX8+OHTuw2Wy0a9eO888/31lxiYiUTcopyEo356GqaHkUR6J19iRkpoOvv7XxiIiIlEK5Eq24uDhGjRrFypUrqVGjBna7nYSEBAYMGMCsWbOoW7eus+MUESmeY31WcC3wC7A2FnGu4Jpg8wF7Fpw5AdVVsRQREddXrjVaDzzwAKdPn2b79u2cOHGCkydPsm3bNhITE3nwwQedHaOISMk0CMNz+fhASG1zrvZBERFxE+WqaH377besWLGCdu3a5TzXvn17pkyZwpAhQ5wWnIhIqZ12JFpqG/RIoXVNkqVES0RE3ES5KlpZWVn4+xfskff398/ZX0tEpEqpouXZHCPek7VXo4iIuIdyJVoDBw7koYce4siRIznPHT58mEceeYSBAwc6LTgRkVJLUkXLozkGYpxRoiUiIu6hXInW5MmTOX36NE2bNqVFixa0bNmSZs2akZSUxOTJk50do4hIyTTa3bNpLy0REXEz5VqjFRUVxYYNG1i+fDk7d+7EbrfTvn17Wrduzb///W+mT5/u7DhFRIqn1kHPpr20RETEzVRoH63BgwczePDgnK83b97Mxx9/rERLRKpeTkVLrYMeSWu0RETEzZSrdVBExOWoouXZVNESERE3o0RLRNxfRhqcPWHOlWh5JlW0RETEzVieaL3zzjs0a9aMoKAgunXrxs8//1zktbfddhs2m63Ao0OHDjnXzJgxo9BrUlJSquLHERErOKocPn4QXNPaWKRy5FS0lGiJiIh7KNMarauuuqrY7586dapMbz579mwefvhh3nnnHfr27ct7773HsGHD+PPPP2ncuHGB6998803+97//5XydkZFB586dufbaa/NdFxYWxq5du/I9FxQUVKbYRMSNONoGQ+uBj+WfH0llcFS00k5D+lnwD7Y2HhERkRKUKdEKDw8v8fu33HJLqe/3+uuvM2bMGMaOHQvApEmTWLZsGVOnTmXixImF3j9vDAsWLODkyZPcfvvt+a6z2WzUr1+/1HGIiJvTIAzPFxgGvgGQmWaqWjWirI5IRESkWGVKtD766COnvXFaWhrr16/nX//6V77nhwwZwurVq0t1jw8//JCLL76YJk2a5Hs+KSmJJk2akJmZSZcuXfjPf/5D165di7xPamoqqampOV8nJiaW4ScREctpEIbns9nMXlqnj5hNi5VoiYiIi7OsxyY+Pp7MzEwiIvL/YRQREUFsbGyJr4+JiWHp0qU51TCHtm3bMmPGDBYtWsTMmTMJCgqib9++7Nmzp8h7TZw4MadaFh4eTlSUfoGLuBVVtLyDBmKIiIgbsXwxg81my/e13W4v8FxhZsyYQY0aNRg5cmS+53v16sXNN99M586d6devH3PmzKF169a8/fbbRd5rwoQJJCQk5DwOHTpUvh9GRKyhipZ30Ih3ERFxIxXasLgi6tSpg6+vb4HqVVxcXIEq17nsdjvTp09n9OjRBAQEFHutj48PPXr0KLaiFRgYSGBgYOmDFxHXkpT974gSLc+mREtERNyIZRWtgIAAunXrxvLly/M9v3z5cvr06VPsa1euXMnevXsZM2ZMie9jt9vZtGkTkZGRFYpXRFyYo3WwuhItj6bWQRERcSOWVbQAxo8fz+jRo+nevTu9e/dm2rRpREdHM27cOMC09B0+fJhPPvkk3+s+/PBDevbsSceOHQvc87nnnqNXr160atWKxMRE3nrrLTZt2sSUKVOq5GcSEQuoddA7KNESERE3YmmiNWrUKI4fP87zzz9PTEwMHTt2ZMmSJTlTBGNiYoiOjs73moSEBObNm8ebb75Z6D1PnTrFXXfdRWxsLOHh4XTt2pVVq1ZxwQUXVPrPIyIWsNs1DMNbqHVQRETciM1ut9utDsLVJCYmEh4eTkJCAmFhYVaHIyLFST0NExuZ8wmHIbCatfFI5dm9DL64DiK7wN0rrY5GRES8RHlzA8unDoqIVIijmhVQTUmWpwvJbh08c9zaOEREREpBiZaIuLec9VlqG/R4OWu0jpmWUXeRGAPL/w/i91odiYiIVCFL12iJiFSYBmF4D0eilZECaUkQWN3aeErj7En4dCQc2wl7v4e7fgJf/eoVEfEGqmiJiHvTIAzvERAK/qHm3B0GYqSnwMwbTZIFcHQrrJtubUwiIlJllGiJiHtTRcu7hNY2x2QXX6eVlQnzx0L0aggMh55m2xJ+fAGS3CBJFBGRClOiJSLuTWu0vIs7jHi322HpE7Dja/ANgOs/h6EvQv3zICUBvn/W6ghFRKQKKNESEfeW0zqoipZXcIdE6+fX4I8PABtcNQ2a9QMfXxj+mvn+xs/g0B+WhigiIpVPiZaIuDe1DnqXvJMHXdHGz+CH/5jzYS9Bhytzv9e4J3S+0ZwvedS0F4qIiMdSoiUi1ko7YwYEnD1VvtdrGIZ3ceW9tHZ/B4seNOd9H4aedxe8ZvBzZs1WzGbY8HHVxiciIlVKiZaIWGvl/2DxI7D0n2V/bVamWge9jau2DsZshrm3gj0TOt8AFz9b+HXV6sGAJ83598/DmRNVFaGIiFQxJVoiYh27HbbNN+fbvyr7H51nTpg/bCH3D3DxbK6aaK19D9LPQPMBcMXbYLMVfW2PsRDR0eyx9f1zVRejiIhUKSVaImKdwxsg4ZA5z0yFLbPL9nrH+qyQ2uDr79zYxDXlrNGKtzaOcx3dbo7d7yj5/xd9/WD4K+Z8/cfmvwMREfE4SrRExDp/fmWOgWHmuH6GqXKVlgZheB9XTLSysiB+tzmv1650r2nSB867DrDDksfMPURExKMo0RIRa9jtsH2hOR/6IvgFw7GdcOj30t9DgzC8j6N18Ey86yQnCdGmbdA3AGo2K/3rhvwHAqrD4fWw6bPKi09ERCyhREtErHFkg/kD1T8EOl5tHmCqWqWVU9Gq7/TwxEU5pg5mZUBKOSdVOlvcTnOs09q0BZZW9fpw4ePmfKMSLRERT6NES0Ss8Wd2Nav1UAgIgW63ma+3zzdDAkpDFS3v4xcAQeHm3FXaB4/tMMe6bcv+2tbDzDF2q/bVEhHxMEq0RKTq2e2wfYE5bz/SHBt1h3rtISMFtswt3X20Rss75eyl5SKJlqOiVZ5Eq3YL8A81rYfH9zo3LhERsZQSLRGpejGb4NRBsy6r1WDznM2WW9Uq7VAMJVreydVGvDsqWvXKkWj5+EL988x5zGbnxSQiIpZToiUiVc9RzWo9BAJCc5/vdB34BUHcdjMgoCRqHfROOZMHXSDRysqCY9kTB+uWcuLguSI7maMSLRERj6JES0Sqlt0Of57TNugQXBM6XGnO139U8r1U0fJOORUtF2gdPHUQMs6CbyDUKsPEwbwiO5ujEi0REY+iREtEqlbsFjj5V3bb4JCC33e0D26bDykJRd8nIzV36pwqWt7FlSpax/JMHPTxLd89HIlW7Jay7SMnIiIuTYmWiFQtR9tgq4shsFrB70f1hDptzHCArcUMxXC0Dfr4m0qYeA9XqmjFVWB9lkPdtmYPrpQEUyETERGPoERLRKpOcW2DDqUdinForTlWizCvEe+RU9FygUTrWAUmDjr4+puJm6D2QRERD6JES0SqTuxWOLHfDLxofUnR13W+3qx5id0KRzbm/17CYfjyDpg3xnxdkUqCuCdXmjoYV4E9tPLSOi0REY+jREtEqo6jmtWyiLZBh5Ba0H6EOV8/wxwz0uCXSTC5B2ybBzYf6DEWrv6gUkMWFxTiImu0sjIhPnviYL1yThx0yJk8uKVi9xEREZfhZ3UAIuIlCtukuDjdboOtc2Drl2avrRXPwfE95nuNLoBLX82tAoh3cVS0zp6EzAzwtehX2amDZoNtvyCo2bRi94rsYo4xm8x/K2qHFRFxe0q0RKRqHN0OJ/aZlsA2xbQNOjTpA7VbmeRq9s3mudB6MPh56DQKfFSQ91ohtQAbYIezJwqfOnlgFaQmmSTd179y4ohzTBxsVf6Jgw4RHcDma6p0p2MhLLLi8YmIUtkzywAAIABJREFUiKX0l4qIVI18bYPVS77eZoPut2ef+0Kve+GBddDlBiVZ3s7HF0Jqm/PC2gd3fgMfXwGzboA3OsKPEyHxiPPjOOZYn1XBtkEA/2AzIh7MmHcREXF7qmiJSOXL2zbYoRRtgw49x0FwLWjQpeJrYMSzhNaBM/EFE63YbTDvTsBu9mpLioWV/4NVr0Db4WZdX7MLndOa56hoOWsgS2Rnk7zFbIbWQ51zTxERsYwSLRGpfHF/mhZA34Dipw2ey8fXVLBEzhVa14xWzzviPTkeZt4A6ckmmbphFuxeCn98CAd/hR1fm0ftVnDeNVCrOYRHQY0oqB5Z9vY/Z1a0wCRaW2Zp8qCIiIdQoiUilSMjzex1te8H2LnYPNdiEASFWRuXeIZz99LKSIPZoyEh2iRQ186AgBDoeLV5HP0T1n0Im2ebpP+nifnvZ/OFsIYm6YrsDBc/B34BRb9/VibEZw9nqdvGOT+TJg+KiHgUJVoi4hx2OxzfZxKrfd/DgZ9NZcHB5pu75kqkovLupWW3wzfjIXo1BIaZSlZIrfzXR7SHS1+Di5+FrXPh0B+QcAhORUPiYcjKMElaQrSpfjU4HzpdW/T7n/zLeRMHHeqfZ44J0XDmRMGfQURE3IoSLRGpmPSzZgT779MKLuIPrQstBppH8wFQPcKaGMXz5N1La+27sPFTs7faNdOLrzAFVofud5iHQ1YmJB2FU4dg/UeweaYZ3lJconXMMXGwdcUnDjoEhZtq3In9pn2wxQDn3FdERCyhREtEyufkQfjjA/MH7tmT5jnfAGjcy7QIthgIER01IVAqh6N1cN+P5v8HAYa8YMa5l5WPL4Q1MI+AEJNo7VkOqaeLnpAZl70+y9lDWup3UqIlIuIhlGiJSOnZ7bD/R/j9fdi1FLCb52s0NtPcuo5Wu5NUDUfrYEK0OXa92WwBUFERHaFWC7Pn2+5lZmhGYRwVrbpOmjjoENnZVNM04l1ExO0p0RKR0vv6QdjwSe7XzQfABXeZUdTOap8SKQ1HogXQuDdc+rpzRrbbbGYLgp9fMwlPUYlWzmh3J1e0IjuboyYPioi4PSVaIlI6Z07Axs/NeY+xcMHdULe1tTGJ96rT2uyTVT0CrvsU/AKdd+/22YnWnuWQmgSB1fJ/PysT4neb88qoaAEc31t866KIiLg8LZ4QkdLZ+z3YM82eQZe+piRLrBVaGx7eAvesgWp1S76+LOqfBzWbmamCe5YV/P6JA5CZahK9Gk2c+96hdcyYeTCbL4uIiNtSoiUipbP7W3NsPdTaOEQcqtUzwyuczdE+CLB9QcHv52xU3Lpyhr2ofVBExCMo0RKRkmVmwN7l5rzNMGtjEakK7bMTrT3LIS05//dyBmE4eX2WQ33HxsVKtERE3JkSLREp2aG1kJIAwTWhUQ+roxGpfJGdzUbEGWdhz3f5v5czCMPJ67Pyvjdo8qCIiJtToiUiJdu91BxbDdF0QfEONltuVevc9sHKrmg5Eq24HZCeUjnvISIilc7yROudd96hWbNmBAUF0a1bN37++ecir/3pp5+w2WwFHjt37sx33bx582jfvj2BgYG0b9+er776qrJ/DBHPtjt7IIDWZ4k3aT/CHPd8B2lnzHlmRu7EwcqqaIU1gJDaZvhM3PbKeQ8REal0liZas2fP5uGHH+app55i48aN9OvXj2HDhhEdHV3s63bt2kVMTEzOo1WrVjnfW7NmDaNGjWL06NFs3ryZ0aNHc91117F27drK/nFEPNPxfeYPSx8/aDHI6mhEqk6DrmYz7vQzuWsUTx6AzDTwD4HwxpXzvjZbnoEYah8UEXFXliZar7/+OmPGjGHs2LG0a9eOSZMmERUVxdSpU4t9Xb169ahfv37Ow9c3t5Vp0qRJDB48mAkTJtC2bVsmTJjAoEGDmDRpUmX/OCKeybE+pXFvCK5hbSwiVamw9sG47ImDdSpp4qCDJg+KiLg9yxKttLQ01q9fz5AhQ/I9P2TIEFavXl3sa7t27UpkZCSDBg3ixx9/zPe9NWvWFLjn0KFDi71namoqiYmJ+R4ikm1X9vqs1pdYG4eIFRxj3ncvg/Szueuz6lXS+iwHJVoiIm7PskQrPj6ezMxMIiIi8j0fERFBbGxsoa+JjIxk2rRpzJs3j/nz59OmTRsGDRrEqlWrcq6JjY0t0z0BJk6cSHh4eM4jKiqqAj+ZiAdJSYSDv5pzJVrijRqcb1oE05PNqPecQRiVtD7LwTHi/eh2yEyv3PcSEZFK4Wd1ADabLd/Xdru9wHMObdq0oU2bNjlf9+7dm0OHDvHqq6/Sv3//ct0TYMKECYwfPz7n68TERCVbIgD7foCsDKjdEuq0tDoakapns0H7K2DNZPhzYZ7R7pVc0arZDALDIDXRrJGM6FC57yciIk5nWUWrTp06+Pr6Fqg0xcXFFahIFadXr17s2bMn5+v69euX+Z6BgYGEhYXle4gIeaYNqpolXqzDlea4+1s4nv37prIrWj4+2rhYRMTNWZZoBQQE0K1bN5YvX57v+eXLl9OnT59S32fjxo1ERkbmfN27d+8C9/zuu+/KdE8RAbIycwdhaKy7eLOG3SCsEaQlZU8cDIXwKuh6iHQkWpo8KCLijixtHRw/fjyjR4+me/fu9O7dm2nTphEdHc24ceMA09J3+PBhPvnkE8BMFGzatCkdOnQgLS2Nzz77jHnz5jFv3rycez700EP079+fl156iREjRrBw4UJWrFjBL7/8YsnPKOK2Dq+HM/EQGG4mDop4K5vN7Kn12xTzdd1Knjjo4BiIsWcZ9LwbajWr/PcUERGnsTTRGjVqFMePH+f5558nJiaGjh07smTJEpo0aQJATExMvj210tLSeOyxxzh8+DDBwcF06NCBb775huHDh+dc06dPH2bNmsXTTz/NM888Q4sWLZg9ezY9e/as8p9PxK3t/tYcWw4CX39rYxGxWoeReRKtSl6f5dB8AASFw4n9MLUvDH4Ouo+pmiRPREQqzGa32+1WB+FqEhMTCQ8PJyEhQeu1xHtN7QtHt8GV06DzKKujEbFWVhZM6giJh2Hw89D3oap53xP7YeH9udM/m/aDEZOhZtOqeX8RESl3bqCPxUSkoFOHTJJl84FWg62ORsR6Pj4w8GkzoKLDVVX3vrWaw62LYdjL4B8Cf/0M7/SBPz4wyZ+IiLgsJVoinujkQXivP3xwMfzyBsTvLdvrHW2DUT0hpJbz4xNxR11uhHE/Q40q3v7Dx8es0brnV2jcx+zp9c2j8OkIOBVd8utFRMQSSrREPE1yPHx2lRkJ/fcfsOJZmNwNJl8AK54zQy5K6hjOGeuuaYMiLqNWc7jtG7jkJfALhgOr4NOrSv7vWURELKFES8STpCbB59fC8b1m/PSwl6HFIPDxh/hd8Mvr8P5AeL09LHkC/i4k6UpLNn/AgfbPEnE1Pj7Qa5ypbvkGmH29Th20OioRESmEpVMHRcSJMtJgzmg4sgGCa8Hor6BOK9NydPYU7F0BOxfDnuVw+gj8/p551GoOnUbBeddC7RawfyVkpkKNxpW/KauIlE/tFhDR0fz3fniDhmOIiLggJVoiniArCxbeB/t+MAvmb5prkiyH4Bpw3jXmkZ4CB1bC1i9N4nViP/w00Twadjd7BgG0HpZ7LiKup+H52YnWeuhYhQM6RESkVJRoibg7ux2+exq2zgEfP7juU2jUvejr/YPM2qvWQ02r4c5vYMts2P8jHF6Xe53WZ4m4tobdzPTBIxutjkRERAqhREvE3a1+K3cj1RFToNXFpX9tYDWzR1bnUXD6KGyfD9vmmdbDpv0qJ14RcY4G55vjkU2QlQk+vtbGIyIi+SjREnFnm76A5f8250NegM7Xl/9e1SOg1z3mISKur04rCKgOaafh2C6IaG91RCIikoemDoq4q6N/wsL7zXmfB8xDRLyHjy806GLOD6+3NhYRESlAiZaIu9q1BOyZ0OxCuPh5q6MRESs0dLQPbrA2DhERKUCJloi7OrTWHNsMM3vriIj3cazTUkVLRMTl6K8zEXeUlZWbaEX1tDYWEbFOw27meHS72bpBRERchhItEXd0bCekJIB/KNTvZHU0ImKV8EYQWheyMiB2q9XRiIhIHkq0RNzRod/MsVE38NXwUBGvZbPlVrW0TktExKUo0RJxR9HZiVbj3tbGISLW0zotERGXpERLxB05Ei2tzxIRR0XrsCpaIiKuRImWiLtJjIFTB8HmA416WB2NiFitQVdzPL4Hzp6yNhYREcmhREvE3TjWZ0V0gKAwa2MREeuF1oaaTc15zCZLQxERkVxKtETcTbRjrHsva+MQEdehdVoiIi5HiZaIu4leY46NlWiJSDat0xIRcTlKtETcSWpS7l45SrRExKGho6KlREtExFUo0RJxJ4fXgz0TwhqZjUpFRAAiO5sBOaePmIE5IiJiOSVaIu4kZ/8sVbNEJI+AUKjbzpxr42IREZegREvEnRxSoiUiRWjogQMx/lwERzZaHYWISLko0RJxF1mZcOgPc66NikXkXJ62TuvIRpgzGmZcBqeirY5GRKTMlGiJuIuj2yHtNARUN3toiYjk5Zg8eGQD2O3WxuIMe1eYY1oSLHrQM34mEfEqSrRE3MUhx/5ZPcDH19pYRMT11GsPfkGQkgAn9lsdTcXtX5nn/EfY+Kl1sYiIlIMSLRF3kbN/Vm9r4xAR1+TrD/U7mfPyrtOK3QrbvzLJmpXSknM/XOp2uzkuewoSDlsXk4hIGSnREnEX0Y6KltZniUgRyrNOy2431aNPr4R3/wFzb4PX2sE3j8KxXZUSZomi10BmGoRHwaWvmbbI1ERY/LBaCEXEbSjREnEHpw5B4t9g84VG3a2ORkRclWOdVmkqWlmZZqrf+wPhkytg3w9mL64ajSE9Gf74AKZcAB9fATu/MddXlf0/mWPzC02r9Ih3wDcA9nwHW2ZXXRwiIhWgREvEHThaaCI7mf1yREQK0yC7ohW7BTLTC78mIxU2fGKSqDmjzfAMvyDocSc8uBEe2gK3LIK2l5nE68BKmHUjvNkFfn3LvL6yOdZnNbvIHOu1hQv/ac6X/hNOx1Z+DCIiFaRES8QdODYqjtL+WSJSjFrNISgcMlIg7s/838vKgi1z4O3usOgBOL7XXNv/cXh4G1z6KtRsCjabqSRd/zk8uAn6PgTBNSEhGpY/A3NugYy00seUfha2zIWkY6W7Pvm4SRTBxOHQ9yGI7Awpp0xbo1oIRcTFKdEScQfaqFhESsPHBxp0Ned512ntXwnvXwTz7zQJU7X6MOQFeGQ7DHwaqtUt/H41m8Dg52H8Drhskql87f4W5t1RdMUsrzMn4JMRMH+sWV9VGgeyq1n1OkC1ernP+/qbFkIff9i5GLbPL939REQsokRLxNWlJJo9tECJloiULO86rbgd8Pm1Zg1WzGazD9/AZ0yLYJ8HILB66e7pHwzdbzdVLt8A2PE1fHV38eu2TkXD9KG5rc+7lkJSXMnvlbM+66KC36vfEfo/Zs6XPA7J8aWLX0TEAkq0RFzd33+APQtqNIHq9a2ORkRcnWOd1rb5MLWPGSDh4wcX3AUPbTKJSkBI+e7d8mK47hNzv23zYOF9piXxXDFb4IPBEL8bwhpCnTZgzyzdIIviEi2Af4yHiI5w5rhJtkREXJQSLRFX51ifpf2zRKQ0HBWt9GTzIU27y+HetTD8FQitU/H7txkG13xkpqBungmLH8qfbO3/CT4aDkmxZhPlMcuh1zjzvY2fF7+26sQBOHXQJHJN+hR+jV8AjJhiBnVsnw/H91X8ZxIRqQRKtERcXc76LO2fJSKlEBYJPe+BVkPhjmUw6jOo09K579H+Crhqmkl2NnwCSx83CdSWufDZNZB2Gpr2g9uXQnhD6Hi1Wd91bIeZclgUx/qsRj0gsFrR1zXoAi0GmvPNs5z3c4mIOJESLRFXlpkOf2fvh6OKloiU1rD/wU1zKndd53nXmOEU2MyeWx9fboZeZKVDhyvh5nkQXMNcGxRuKmtgqlpFKaltMK/ON5jj5lmFty+KiFhMiZaIK4vdatp/gsLNGgcREVfS5Qa4/E1z/tfP5tjrPrh6OvgFnnPtjea47UtITyl4r6ys3P2zml9U8nu3vRQCw80UxYO/lid6EZFKpURL3FfaGfjhv3Dod6sjqRzJ8fDtBHMe1cuMbRYRcTXdbjWj38MawdCJcMmLhf971exCc01KAuz6puD3j26FsycgoFruOrPi+AdDh5HmfPPMiv0MIiKVwPK/3N555x2aNWtGUFAQ3bp14+effy7y2vnz5zN48GDq1q1LWFgYvXv3ZtmyZfmumTFjBjabrcAjJaWQT8/Evf3xPqx6GT4aBmunedbmlXE74f2BZn1WYBhc+ITVEYmIFK377TB+O/S+t+hrfHxNBQwKbx90VLOa9DV7ZpWGo0r250JISy59vCIiVcDSRGv27Nk8/PDDPPXUU2zcuJF+/foxbNgwoqOjC71+1apVDB48mCVLlrB+/XoGDBjA5ZdfzsaNG/NdFxYWRkxMTL5HUFBQVfxIUpU2Z48JzsowC7EX3l94O4q72bsCPhxsJm/VbGomdjXqbnVUIiIV50iM9v0ACYfzf68s67MconpCreaQlmT29hIRcSGWJlqvv/46Y8aMYezYsbRr145JkyYRFRXF1KlTC71+0qRJPPHEE/To0YNWrVrx4osv0qpVK77+Ov8/rjabjfr16+d7iIeJ3QZx283GmRc9aSZfbfoMZgwv+Mvbnfz+Pnx+HaQmQuM+MPYHqNfW6qhERJyjVnNTscIOW/JMC8xIhYOrzXnzi0p/P5stdyjGpi+cFKSIiHNYlmilpaWxfv16hgwZku/5IUOGsHr16lLdIysri9OnT1OrVq18zyclJdGkSRMaNWrEZZddVqDida7U1FQSExPzPcTFOTa9bD0ULvpn9nSrmnB4PUy7KHfvKWfLyjJ/EDhbZgYseQKWPGY29ex8I9yyAEJrO/+9RESs5Khq5d1T69DvkHEWQutBvXZlu1+nUeZ4YBWcOuS8OEVEKsiyRCs+Pp7MzEwiIiLyPR8REUFsbGyp7vHaa6+RnJzMddddl/Nc27ZtmTFjBosWLWLmzJkEBQXRt29f9uzZU+R9Jk6cSHh4eM4jKiqqfD+UVI2sTNg615w7fsG2GAh3/gj1OkByHMy4DNZNd+77HtkI7/SCl5vDT/+D1CTn3DclEWaOgt/fM19f/CyMfKfgxC4REU/QfiT4h8KJfXBorXkup23wQlOlKouaTcyeXedWyURELGb5MAzbOf+g2u32As8VZubMmTz77LPMnj2bevXq5Tzfq1cvbr75Zjp37ky/fv2YM2cOrVu35u233y7yXhMmTCAhISHnceiQPhFzaX/9DKdjIKgGtMpTEa3VDMYuN7/Es9Jh8SPw/fMVf7+sTFj1CnxwMcTvMmsBfpoIb3U1yVxmRvnvnZ4CM28w67L8Q8zGov94pOx/aIiIuIvAarnTAjd+Zo4HyjDWvTA57YMzPWswkoi4NcsSrTp16uDr61ugehUXF1egynWu2bNnM2bMGObMmcPFF19c7LU+Pj706NGj2IpWYGAgYWFh+R7iwrbMMccOVxas+gSEwrUzYND/ma9/eQOO7Sr/e504YKYa/vCCGbrRfgRcOQ1qNjOVs8WPmCrXzm/K/ss9Kwu+uhsO/mImC972Te6GniIinqzLTea4/StIjDFt32BGwJdH+yvMh1Un9sHffzgnRhGRCrIs0QoICKBbt24sX7483/PLly+nT58+Rb5u5syZ3HbbbXzxxRdceumlJb6P3W5n06ZNREZGVjhmcQFpZ8wYX8htGzyXzQb9xkPby8CeVb6qlt1uPml99x+mtSWgOox8F679GDqPgvt+h2EvQ0htOL4HZt0I0y8p/Z5edjssexL+XAA+/qaS1fD8sscpIuKOmvQxH1ilJcHSJ8y/1bVbQo1ytu4HVod2V5hzDcUQERdhaevg+PHj+eCDD5g+fTo7duzgkUceITo6mnHjxgGmpe+WW27JuX7mzJnccsstvPbaa/Tq1YvY2FhiY2NJSEjIuea5555j2bJl7N+/n02bNjFmzBg2bdqUc09xc7uWmF/MNRqbsb7FGfiMmUa4czEcKsMnnMnHYfbNsPA+816N+8A9v5r9XxwtfX4B0PNueHAT9HsM/ILNnlcfDoZvHi15/daaybA2e7rmle+adQkiIt7CZsutau1YZI7NL6rYPR17dG2f7xlbfYiI27M00Ro1ahSTJk3i+eefp0uXLqxatYolS5bQpEkTAGJiYvLtqfXee++RkZHBfffdR2RkZM7joYceyrnm1KlT3HXXXbRr144hQ4Zw+PBhVq1axQUXXFDlP59UAkfbYKdR4FPC//vWa2um9wGseLZ0rX1nTsD7F5nkzMffDKa4bbFZbF2YoDAY9Aw8uAG63Gye++MDeLdv7qjic239Er572pwPeQHOu6bkuEREPE3n64E861HL2zbo0LQ/hDWClATzoZxUnejf4L+RsPY9qyMRcSk2u12rRs+VmJhIeHg4CQkJWq/lSpKOwWttzPjz+/6Auq1Lfs2pQ/B2N8hMhZvmQati1vTZ7fDFKNizDGo0gVGfQmTnssW470ezcXLi34ANet1rEjH/YPP9/Svhs6vNsI5e98LQFzX4QkS81ycjsicO2uCfB8w2HRXx/fPw82tmUNJNc50RoZTGV/fA5i8guBaM/zP3d56IhyhvbmD51EGRUts+3yRZDbqWLskC0+9/wZ3mfMWzZgBFUdZMNkmWb6BZM1XWJAugxQC4dzV0vRmww29T4N1+8Pc6s8ny7JtNktV+JAz5r5IsEfFu599qjlE9K55kQe70wb3fw+mjFb+flMxuh30/mPOzJ3K3XxERJVriRhybFHe6vmyv6/eomep3dKtJ1grz9zqTiAFc8iJEdip3mASFw4gpcOMcqFbfDMv4cDDMuBRSE6FJX7jyvZJbH0VEPF2HK80HW1e/75z71WkFjXqYD+W2znHOPSvi9FHTLr7sKTiyyepoKsfR7ZCUZ4L02vc0Yl8km/7SE/cQv8eM/7X5QseryvbakFrQ50Fz/sN/ICMt//fPnoS5t2ePbx8J3cc4J+bWQ+HeNWY9mT0LUk5B3XZw/efgH+Sc9xARcWc2m9nWokZj593TUdXa+DlkpjvvvqWRfBy2LzBDkSb3gNdaw7wxpmNi7m0V23fRVe373hyjepoR+0e3wcFfrY1JxEUo0RL34BiC0WIgVKtX/LWF6X0vhNaDk3/Bho9zn7fbzZqqhGio2RSueMu57XwhteCqaXD9TOgxFkbPd057jIiIFK7jVeAXBMd2mNbtvyr5j/6Tf8HKV2BqX3ilOcy91QxFit8N2EwbelA4nDzgGlU2Z9ubnWh1uCp325W171oXj4gLUaIlrs9uz20b7FzGtkGHgFC48AlzvvLl3PHrv7+fO2Hwmo/ML8PK0HY4XPoahDWonPuLiIgRXBOu/sDsc3hsB8wYDl+Ng6Q4573H2ZOw7iOzf+KbneHHF0wlB6BeB+g5DkZ9bgZ83L0K/jHefG/ly55V1UpLhug15rzlILPtCcDOb+BUdNGvE/ESSrTE9R1aC6cOQkA1aDO8/Pc5/1ZTtUqOg9+mmn75754y3xvyH20YLCLiKdpdDvevg263AzbYPBPe7m4+XMvKLN89s7Jgx2Iz1OjV1rD44ewkw2b2ABvxDjy+zwxEGvYStLsst4Ohx1iT+J08kPvBoSf461fITIPwxmbD6XrtzJh+e5ap6ol4OSVa4vocv5TaXQ4BIeW/j1+A2cQY4Nc3s/vl06DNpebTRxER8RwhteDySTD2e9O+l5oASx6D9weaNb9lYbfDgnEw+ybY8bX53RHREQY/b8aZ37IQut4EoXUKf31gNeibvefnqlc8p6rlWJ/VcmBu273j9+n6jyHtjDVxibgIJVri2jLSYFv2pMBO11X8fh2ugvrnQdpp88lieBSMmKwx6yIinqpRN7jzRxj+KgSGQ8wm+GCwqU6V1prJ5kM/my/0vh/G/Qr3/GqSp9K2hHtiVcuxPqvFoNznWg81e1GmnPLMNWkiZaBES1zb3uXmH+tq9U07QkX5+MCgZ825zReumW4+9RQREc/l42v2VHxgnemOsGeaaYAHV5f82n0/wvJ/m/NLJsLQ/0L9jmWPISDUs6pap6LN9iU2X2ie5/ezjy9ccJc516h38XJKtMS17fjaHDteZf7xdoaWg+Cq9+GmuRB1gXPuKSIirq9aPbhmhlnvm5ECM683+0AV5eRf8OXtZs1R5xtzE4jy6jEWQupkV7VmVexeRUlJhC/vgGkD4Pi+ynkPyK1mNepRcJBU15vNqPe4P+GvnysvBhEXp0RLXFdWFuxZbs7bDHPefW0204bYclDJ14qIiGfx9TPdDI17Q0oCfHZ14RPy0pJh1k1mwmCD8+GyNyreZl6gquXkfb4S/oaPhsG2eXBkg5mKGLvVue/hsO8Hc2wxsOD3gmvk7me29r3KeX8RN6BES1zXkY1wJh4CqkNUL6ujERERT+EfDDfMNJvIn46BT68ymw07OPZYPLoNQuvCqM+ct9F8jzHZVa2/nLtW68gmeH+QiblahBkznxwHMy6F6LXOex8wbY/7V5rzoj60dIx637XE/KwiXkiJlriuPcvMscUAMzFQRETEWYJrmk3kw6PMWqMvrjVVLIDVb8H2+eDjB9d9AuENnfe+palqpafA2mnwdjd49x+w4RPzXFF2LYWPhkNSLNRrD2NXwO1LzIeUKQnw6cjcVj9nOLzeTHEMrgkNuhZ+Td02ptqlUe/ixZRoieva8505th5qbRwiIuKZwhrAzfNNwnB4Pcy5BXZ/ByueNd+/5H/QpI/z37fHGFMpO/kXbM6zVisj1ez19VZXWPo4HN9rWv8WPQCTOsJPL0FyfP57/fYuzLoR0pOh+QAdk2DxAAAcLklEQVS441uo0di0742ebyYCpp+BL0bB9gXOid8x1r35RcWvn3aMet/wSW4SK+JFlGiJazp91LQOArQcbG0sIiLiueq2hhvngl8w7F1hKlv2LOhysxleURnOrWqlJcO66fDW+Wavr9NHIKyhGUk/5AUIawTJx+CnF+GNDvD1QxC3A5Y8Ad/+08R7/q1myFPewRQBoXDDLGg/ErLSzWCPDZ9WPP7CxroXpuVgqNnMVNU2V9LwDxEXZrPbNXfzXImJiYSHh5OQkEBYWJjV4XinjZ/BwvsgsgvcvdLqaERExNPt/s5MIbRnQsNucNsS563LKkxaMrzZ2SRQgeGmFQ+geiT0exTOvwX8As1zmenw50Kzn5fjQ8i8Ln7OJG5FDevIyjTJ2cbsJGvwf8x+YD7l+Lz9zAl4pYVJ7h75s+S2yt+mwrf/guoN4N41ptIm4mbKmxuooiWuaXf2+iy1DYqISFVoPQSu/8KMJh/1eeUmWZBd1XrYnKcmmAEWl7wED24ye345kiwAX3847xqz8fLtS6HNpYANfAPh2hnwj4eLn4jo4wtXvA19HjBfL38G3uwEP/wXThwoW9z7fzJJVt12pVu7dv6tUKuFqdJ9+6+yvZeIm/OzOgCRAjLTzQaRAK2GWBuLiIh4jzaXmEdVueBOOHvCJFnn32KmIRbHZjNrxpr0yR5Jb4MaUaV7L5vNVLKqR8LKlyHhEKx62Tya9jMJZrsrICCk+Ps41meVdouUgBAYORU+ugQ2z4S2l0G7y0r3WhE3p9bBQqh10GIHVsHHl5vxt4/tKV9rg4iIiBQuPQV2fWPa9Pf9CGT/KRhQ3VTOLpoA1SMKvs5uh9fbm+rUzfPLth/l8n/Dr2+aISD3/gahdZzyo4hUBbUOiudwtA22GqwkS0RExNn8g6Dj1TD6K3h4Kwx4Gmo2hbTTsP4jeLdv4ePgj+00SZZfUNmnMV70pGk3TD4Gix8xSZuIh9NfseJ69iw3R7UNioiIVK4aUXDh4/DARrh1cfZGx8fgs6tMFSrvPl+O5KtJ35LbHM/lHwRXTjV7k+1YBNvmOe9nEHFRSrTEtZz8C+J3gc3XbHQoIiIilc/HB5r1gzu/zx1r/+ubMH1o7sAMx/qs8v5+btAV+j9uzr95FBJjKhazJ0g8AqmnrY5CKokSLXEtu7M3KW7cSyNgRUREqpp/MFz6Goz6zOzJdXg9vNcfNn0BB1eba8qyNutc/R6FyM6Qcgq+ftC7WwiP74PJPeCNjmaao3gcJVriWvZkJ1pqGxQREbFOu8th3K8Q1QtSE2HBPZCRYvbDqtu2/Pf19Ycr3wPfAPM7f2MxGyhnZkBGWvnfy9WtfhvSkkzS+elVsPY97048PZDGu4vrSDsDf/1szpVoiYiIWKtGFNz2Dax8CVa9Atih5cDi9+wqjXrtYODTZg3YtxOgThvTPndiH5zYbyo9J/aZEfZZGeAXbKpr5z6a9TP7dFU0HiskHTNVQoBm/c3E5aVPwNHtMPxV8AuwNj5xCiVa4joOrDKfloVHmX+ERURExFq+fjDwKZMMbPoc+j7inPv2vh92fgOH1sL0Ej5czTgLSWchKTb/89u+NMdutzknpqr0+zTITIWG3eCWRaa6tfzfsOFjiN8Doz7VCHwPoERLXEfetkF3/HRKRETEUzXrZx7O4uNrNjL+cIipZtVqDrVb5Dm2MEf/YEhJyPNINMe/f4f1M0xFrElfqNPKebFVtrRk+ON9c97nQfM3T98HTUvmvDEQvRqmDYAbvoD651kbq1SIEi1xDXZ7bqLVeqi1sYiIiEjlq90CHtsN2IrfNzO4ZsHnOt9gpiEeWGmSkzEr3KfdbuPncPYk1Gxm1sI5tB4CY7+Hmdeb1skPh5g2wo5Xm/H44nY0DENcQ9wOSDhkNkFs6sRPzERERMR1+fgWn2QV+TofuPJdk4TFbIYfX3B+bJUhKxPWTDbnve8zP39edVubEfvNB0D6GVh4L7zSEubdaVot01NKfo+MNEhNcn7sUmaqaIlrcFSzmvaDgBBrYxERERHXF9YArpgMs2+CX9+CFoOg+YVWR1W8HYvg1EEIqQ1dbir8muCacNOX8MvrsP5jSPwbts4xj4Dq0GYYdBgJER3h5AE4vtcMEDm+1zxOHgR7lhmj32KASdoa9wK/wKr9WQWb3a45kudKTEwkPDychIQEwsLCrA7HO3w0HA7+akrkF9xpdTQiIiLiLr5+yKzXqh4J96yGkFrlu09Kgpl6GNmlctaK2+3w/kA4sgEu/BcMmFDya7KyzF5m27+CPxdA4uHyvbdfMDTpYxKvpv+AsEbmf6dzK2pSqPLmBkq0CqFEq4qdPQkvtwB7Jjy0GWo2tToiERERcRdpyfDehXB8D7S9zGy2XNpEKSnOtOTt+NpMP85KhwvugmEvOz/Z+usXmHGpWSbxyPayTxXMyoLD67KTroWQdNT8zVS7lVnvVrtl7gPM+rV9P8L+H82157L5QEgdCK0L1epCaD0Ii4QOV0GDLhX+cT2JEi0nUqJVxbbNgy/vMPto3P+71dGIiIiIuzmyCT642CRKl79Z/Mj3k3/BjsWwczFE/wYU8qfwwGeg/2POjfHz62DPMuh+B1z2RsXuZbeb9sDSVKTsdrMWfv+PJvE6sgHOHC/+NU36Qq97oM3wkt/DbjeJXEgdsx2ABypvbuCZ/2uI+0hJgFWvmvPW2qRYREREyqFBFxj0TO4myE36mspO4hGI3QIxW3KPCdHnvPZ8M/2v3eWw7wezcfAP/4FqEXD+aOfEF7fTJFnYzB5iFWWzga2UbX82G0S0N4/e95nnMjPgTLyp6CUfM4+kOIjZZKplB381jxqNoec46Hqz2SQaTGXt2A44uDr7utUm0QpraKqB3W4tfFKkF1JFqxCqaFWRzHT4/FrzCUu1CLjrJ7OwVURERKSssrLg0xGmBbBaBGRlFF65sfmYRKzd5dD2UghvlP/7K56FX94wicz1X0CbSyoe24L7YNNn5j1HfVbx+1WmxCPwx4ewbjqcPWGeC6gGHa+C5HiTWKWcKvr1/qEmMes1zuyL5gHUOuhESrSqgN0Oix6AjZ+a/yBvX6J+YBEREamYxCMwtY9Z/w0mWarbBup3gshO5lj/PAiuUfQ97HZYeB9s+twMkbh1EURdUIGYYmDSeaatccwKiOpR/ntVpfSzsGUO/DbVVLDy8g81/5s06WuGbNTvaNox10yBuO3ZF9lMItv7Pmjcu/g1b1lZkJFiHulns49nzDj7Bl0tb0lUouVESrSqwKpXTVne5gPXz3TOp0UiIiIicTvN0Ih67aBee/APLvs9MtNh1o1m+5ngmnDHMpOwlcfy/4NfJ5lk445vy3cPK9ntsP8nMzSkRmOTXEV2Al//oq/97Z3crXscbD7ZD9885z4mAc0oZn+wx/eVfXCIkynRciIlWpVs65dmF3fQOHcRERFxTWnJ8PEVJmkLawRjvoPwhgWvy8o0LXXJx8y6pzPHIfm4OU+Oh61zITXRfLDcdnjV/xxWObbLJFybZxWfSJ3Lx98kx35B4B8EY5ZD9fqVF2cpKNFyIiValejgavhkBGSmmcWgQ/9rdUQiIiIihUs+DtOHmtHxdduZdUqnY+B0bO4j6ajZoqY4ddvCPWvAx6dq4nYl6Wch9bSZknjuIyvTVMb8gk1S5Rf8/+3df0xV9/3H8dfhApcLBUUoXLDqsGvFH9NV6Jxif62Toq2bzrXO+atb0oZUHch+YFtJUdsy26xrOgotjTNrtMO4TccSa0s7w4al0ZFhnbJ22fodlh+hVCsXVFD4fP9A77wCiuSWc5XnIzm593zO517eJ3nnhnc+n/P52D5NsC8UWn5EofUFafmXtGVOz7zpifOlB18fnj84AADg2vF5nbQlvafA6pclhcf0THELj5UiYs6/nj+fkNEz7Q7XJJZ3R2DqOtszbO5p6Nkr6/QJaXSqtLCEIgsAAAS+kWOl5bulis2S8wYpMqFnKtsN7p7XyISeTX8DcCQG9rI9I4qKivT888+rsbFRkydP1osvvqg77rij3/4VFRXKycnRkSNHlJiYqJ/97GfKzMz06fP73/9eeXl5+ve//62bb75ZzzzzjBYuXPhF38rw0HV+qdQL85DbW/533t7yv7YL55cu/zlynLSkVAoNtyd+AACAqxWXLD241e4ocI2xtdDasWOHsrOzVVRUpLS0NL366quaO3eujh49qrFjew+vfvzxx5o3b54eeeQRbdu2Tfv379djjz2mG2+8UYsWLZIkVVVVafHixdq0aZMWLlyoXbt26aGHHlJlZaVmzJgx1Ld47TjTKp08Jn1+rOfV03RR0XRRAXW5fRP6Yzl6hs5jb+3ZCf2GG/0fPwAAABBAbH1Ga8aMGZo+fbqKi4u9bRMnTtSCBQtUUFDQq39ubq7KyspUW/u/tfwzMzN16NAhVVVVSZIWL16s1tZWvfnmm94+GRkZio6O1m9/+9sBxRVIz2iZrnM6feZ0zxS8rk5Z3Z3e997XS13Yp8AYWWdPSx2tsjpaZXV4et53eqQzrbLaGhV08hNZJ4/JuooCypyfh2zCY2TCY88fPXORTXiMTETc+ddYmfAbe/aqsJgmCAAAgKvjCnHIutweXEPgmntGq7OzU9XV1Vq3bp1Pe3p6ut57770+P1NVVaX09HSftvvuu09btmzR2bNnFRISoqqqKq1du7ZXnxdffLHfWDo6OtTR0eE9b21tvdrb+cKceytP4QeKhuRvnTA3qN7Eqt7EqslE67iJ0meK0nETqc/MCH2mSB03UfpcN6j7TJB0/HLf1nb++L8hiR0AAADXn6Mb71N4qO1POw2KbVG3tLSoq6tL8fHxPu3x8fFqamrq8zNNTU199j937pxaWlqUkJDQb5/+vlOSCgoKtGHDhkHeyRfsks3gOo1DZxV80eHoGWGSZMmcf5X3/JRxyqNwtRmXPAqX58KrXPrUjFS9idUn5kY1mBi1axAb+gEAAADoxfby8NKhQGPMZYcH++p/afvVfufjjz+unJwc73lra6vGjBlz5eCHQPA963QqLUdyhPZs4GZZsiSFnj+uJEqSvVu8AQAAAIPjCnHYHcKg2VZoxcbGyuFw9Bppam5u7jUidYHb7e6zf3BwsGJiYi7bp7/vlCSn0ymn0zmY2/jCWaHhCmeFPgAAAOCaYtsKBaGhoUpJSVF5eblPe3l5uWbNmtXnZ2bOnNmr/9tvv63U1FSFhIRctk9/3wkAAAAA/mbr1MGcnBwtX75cqampmjlzpkpKSlRXV+fdF+vxxx9XfX29Xn/9dUk9KwwWFhYqJydHjzzyiKqqqrRlyxaf1QSzsrJ05513avPmzfr2t7+tP/7xj3rnnXdUWVlpyz0CAAAAGH5sLbQWL16szz77TBs3blRjY6OmTJmiPXv2aNy4cZKkxsZG1dXVefsnJSVpz549Wrt2rV5++WUlJibqpZde8u6hJUmzZs1SaWmp1q9fr7y8PN18883asWMHe2gBAAAAGDK27qMVqAJpHy0AAAAA9hlsbcAusgAAAADgZxRaAAAAAOBnFFoAAAAA4GcUWgAAAADgZxRaAAAAAOBnFFoAAAAA4GcUWgAAAADgZxRaAAAAAOBnFFoAAAAA4GcUWgAAAADgZ8F2BxCIjDGSpNbWVpsjAQAAAGCnCzXBhRphoCi0+uDxeCRJY8aMsTkSAAAAAIHA4/FoxIgRA+5vmastzYaB7u5uNTQ0KDIyUpZl2R2OWltbNWbMGB07dkxRUVF2h4NrBHmDwSBvMFjkDgaDvMFgDHXeGGPk8XiUmJiooKCBP3nFiFYfgoKCdNNNN9kdRi9RUVH8COGqkTcYDPIGg0XuYDDIGwzGUObN1YxkXcBiGAAAAADgZxRaAAAAAOBnjvz8/Hy7g8CVORwO3X333QoOZrYnBo68wWCQNxgscgeDQd5gMK6FvGExDAAAAADwM6YOAgAAAICfUWgBAAAAgJ9RaAEAAACAn1FoAQAAAICfUWgFuKKiIiUlJSksLEwpKSn661//andICCAFBQW6/fbbFRkZqbi4OC1YsEAffvihTx9jjPLz85WYmCiXy6W7775bR44csSliBKKCggJZlqXs7GxvG3mD/tTX12vZsmWKiYlReHi4vvrVr6q6utp7ndzBpc6dO6f169crKSlJLpdL48eP18aNG9Xd3e3tQ97gL3/5i+bPn6/ExERZlqXdu3f7XB9IjnR0dGjNmjWKjY1VRESEvvWtb+mTTz4ZytvwQaEVwHbs2KHs7Gw9+eST+vvf/6477rhDc+fOVV1dnd2hIUBUVFRo1apVev/991VeXq5z584pPT1d7e3t3j7PPfecXnjhBRUWFurgwYNyu92aM2eOPB6PjZEjUBw8eFAlJSWaOnWqTzt5g76cOHFCaWlpCgkJ0ZtvvqmjR4/qF7/4hUaOHOntQ+7gUps3b9Yrr7yiwsJC1dbW6rnnntPzzz+vX/3qV94+5A3a29s1bdo0FRYW9nl9IDmSnZ2tXbt2qbS0VJWVlWpra9MDDzygrq6uoboNXwYB62tf+5rJzMz0aUtOTjbr1q2zKSIEuubmZiPJVFRUGGOM6e7uNm632/z85z/39jlz5owZMWKEeeWVV+wKEwHC4/GYW265xZSXl5u77rrLZGVlGWPIG/QvNzfXzJ49u9/r5A76cv/995sf/vCHPm3f+c53zLJly4wx5A16k2R27drlPR9Ijnz++ecmJCTElJaWevvU19eboKAgs3fv3qEL/iKMaAWozs5OVVdXKz093ac9PT1d7733nk1RIdCdPHlSkjRq1ChJ0scff6ympiafPHI6nbrrrrvII2jVqlW6//779c1vftOnnbxBf8rKypSamqoHH3xQcXFxuu222/Taa695r5M76Mvs2bP17rvv6qOPPpIkHTp0SJWVlZo3b54k8gZXNpAcqa6u1tmzZ336JCYmasqUKbblUeBupTzMtbS0qKurS/Hx8T7t8fHxampqsikqBDJjjHJycjR79mxNmTJFkry50lce/fe//x3yGBE4SktLVV1drb/97W+9rpE36M9//vMfFRcXKycnR0888YQOHDigH/3oR3I6nVqxYgW5gz7l5ubq5MmTSk5OlsPhUFdXl5555hktWbJEEr85uLKB5EhTU5NCQ0MVHR3dq49d/ztTaAU4y7J8zo0xvdoASVq9erU++OADVVZW9rpGHuFix44dU1ZWlt5++22FhYX124+8waW6u7uVmpqqZ599VpJ022236ciRIyouLtaKFSu8/cgdXGzHjh3atm2b3njjDU2ePFk1NTXKzs5WYmKiVq5c6e1H3uBKBpMjduYRUwcDVGxsrBwOR68KvLm5uVc1D6xZs0ZlZWXat2+fbrrpJm+72+2WJPIIPqqrq9Xc3KyUlBQFBwcrODhYFRUVeumllxQcHOzNDfIGl0pISNCkSZN82iZOnOhdpInfHPTlpz/9qdatW6fvfe97+spXvqLly5dr7dq1KigokETe4MoGkiNut1udnZ06ceJEv32GGoVWgAoNDVVKSorKy8t92svLyzVr1iybokKgMcZo9erV+sMf/qA///nPSkpK8rmelJQkt9vtk0ednZ2qqKggj4axe++9V4cPH1ZNTY33SE1N1dKlS1VTU6Px48eTN+hTWlpary0kPvroI40bN04Svzno26lTpxQU5Psvp8Ph8C7vTt7gSgaSIykpKQoJCfHp09jYqH/84x+25ZEjPz8/35a/jCuKiopSXl6eRo8erbCwMD377LPat2+ftm7d6rOULoavVatWafv27frd736nxMREtbW1qa2tTQ6HQyEhIbIsS11dXSooKNCECRPU1dWlH//4x6qvr1dJSYmcTqfdtwAbOJ1OxcXF+RxvvPGGxo8frxUrVpA36NfYsWO1YcMGBQcHKyEhQXv37lV+fr42bdqkqVOnkjvoU21trX7zm99owoQJCg0N1b59+/TEE0/o+9//vubMmUPeQJLU1tamo0ePqqmpSa+++qpmzJghl8ulzs5OjRw58oo5EhYWpoaGBhUWFmratGk6efKkMjMzFRkZqc2bN/cq9oeELWsdYsBefvllM27cOBMaGmqmT5/uXbYbMKZn+dO+jq1bt3r7dHd3m6eeesq43W7jdDrNnXfeaQ4fPmxf0AhIFy/vbgx5g/796U9/MlOmTDFOp9MkJyebkpISn+vkDi7V2tpqsrKyzNixY01YWJgZP368efLJJ01HR4e3D3mDffv29fk/zcqVK40xA8uR06dPm9WrV5tRo0YZl8tlHnjgAVNXV2fD3fSwjDFm6Ms7AAAAALh+8YwWAAAAAPgZhRYAAAAA+BmFFgAAAAD4GYUWAAAAAPgZhRYAAAAA+BmFFgAAAAD4GYUWAAAAAPgZhRYAAAAA+BmFFgAAfmZZlnbv3m13GAAAG1FoAQCuKw8//LAsy+p1ZGRk2B0aAGAYCbY7AAAA/C0jI0Nbt271aXM6nTZFAwAYjhjRAgBcd5xOp9xut88RHR0tqWdaX3FxsebOnSuXy6WkpCTt3LnT5/OHDx/WN77xDblcLsXExOjRRx9VW1ubT59f//rXmjx5spxOpxISErR69Wqf6y0tLVq4cKHCw8N1yy23qKys7Iu9aQBAQKHQAgAMO3l5eVq0aJEOHTqkZcuWacmSJaqtrZUknTp1ShkZGYqOjtbBgwe1c+dOvfPOOz6FVHFxsVatWqVHH31Uhw8fVllZmb785S/7/I0NGzbooYce0gcffKB58+Zp6dKlOn78+JDeJwDAPpYxxtgdBAAA/vLwww9r27ZtCgsL82nPzc1VXl6eLMtSZmamiouLvde+/vWva/r06SoqKtJrr72m3NxcHTt2TBEREZKkPXv2aP78+WpoaFB8fLxGjx6tH/zgB3r66af7jMGyLK1fv16bNm2SJLW3tysyMlJ79uzhWTEAGCZ4RgsAcN255557fAopSRo1apT3/cyZM32uzZw5UzU1NZKk2tpaTZs2zVtkSVJaWpq6u7v14YcfyrIsNTQ06N57771sDFOnTvW+j4iIUGRkpJqbmwd9TwCAawuFFgDguhMREdFrKt+VWJYlSTLGeN/31cflcg3o+0JCQnp9tru7+6piAgBcu3hGCwAw7Lz//vu9zpOTkyVJkyZNUk1Njdrb273X9+/fr6CgIN16662KjIzUl770Jb377rtDGjMA4NrCiBYA4LrT0dGhpqYmn7bg4GDFxsZKknbu3KnU1FTNnj1b27dv14EDB7RlyxZJ0tKlS/XUU09p5cqVys/P16effqo1a9Zo+fLlio+PlyTl5+crMzNTcXFxmjt3rjwej/bv3681a9YM7Y0CAAIWhRYA4Lqzd+9eJSQk+LRNmDBB//znPyX1rAhYWlqqxx57TG63W9u3b9ekSZMkSeHh4XrrrbeUlZWl22+/XeHh4Vq0aJFeeOEF73etXLlSZ86c0S9/+Uv95Cc/UWxsrL773e8O3Q0CAAIeqw4CAIYVy7K0a9cuLViwwO5QAADXMZ7RAgAAAAA/o9ACAAAAAD/jGS0AwLDCjHkAwFBgRAsAAAAA/IxCCwAAAAD8jEILAAAAAPyMQgsAAAAA/IxCCwAAAAD8jEILAAAAAPyMQgsAAAAA/IxCCwAAAAD87P8Bu626sp9mZpQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Luzon GWAP: Training, Validation, and Test Losses Over Epochs')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
