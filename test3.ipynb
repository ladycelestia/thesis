{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file = r'D:\\School\\ADMU\\4Y\\SEM 1\\MATH 199.11\\input_gwap.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "data = data[['GWAP', 'FLOW_LUZ','RGWAP_DR','RGWAP_FR','RGWAP_RD','RGWAP_RU']]  # Select the relevant time series column\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data.values)\n",
    "data_scaler = MinMaxScaler()\n",
    "columns = ['GWAP', 'FLOW_LUZ','RGWAP_DR','RGWAP_FR','RGWAP_RD','RGWAP_RU']\n",
    "        \n",
    "X  = data[columns].values\n",
    "y = data['GWAP'].values.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len - 1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx:idx+self.seq_len-1], self.y[idx+self.seq_len-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_tmp, y_test, y_tmp = train_test_split(X, y, test_size=.8)\n",
    "\n",
    "X_val, X_train, y_val, y_train = train_test_split(X_tmp, y_tmp, test_size=.75)\n",
    "seq_len=1440\n",
    "batch_size=64\n",
    "\n",
    "train_dataset = Dataset(X_train, y_train, seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = Dataset(X_val, y_val, seq_len)    \n",
    "val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False) \n",
    "\n",
    "test_dataset = Dataset(X_test, y_test, seq_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers,  x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "    \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "hidden_size = 64\n",
    "output_size = y_train.shape[1]  # Number of output features\n",
    "num_layers = 2\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "criterion=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_dataloader,device,optimizer):\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for i, (inputs, target) in enumerate(train_dataloader):\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, target) in enumerate(train_dataloader):\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "    \n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(model, train_dataloader, test_dataloader, device, epoch):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "        loss = train(model, train_dataloader, device, optimizer)\n",
    "        loss = evaluate(model, train_dataloader, device)\n",
    "        test_loss = evaluate(model, test_dataloader, device)\n",
    "        print(epoch, loss, test_loss)\n",
    "        if (epoch + 1) == 100 or (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch {epoch+1:04d} | loss: {loss:.4f} '\n",
    "                f'test_loss: {test_loss:.4f} ')\n",
    "    \n",
    "        if loss < 1e-3 and test_loss < 1e-3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 745987200.0 227505136.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[127], line 5\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(model, train_dataloader, test_dataloader, device, epoch)\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[1;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m evaluate(model, train_dataloader, device)\n\u001b[0;32m      7\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_dataloader, device)\n",
      "Cell \u001b[1;32mIn[125], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, device, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m total_loss, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_x, batch_y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m----> 6\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "run(model, train_dataloader, test_dataloader, device, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
